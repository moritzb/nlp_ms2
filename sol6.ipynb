{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment <span style=\"color:red\">option Four</span> - News Categorization  using PyTorch \n",
    "Download the dataset from https://www.kaggle.com/uciml/news-aggregator-dataset and develop a news classification or categorization model. The dataset contain only titles of a news item and some metadata. The categories of the news items include one of: –<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health. \n",
    "\n",
    "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n",
    "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models.\n",
    "3. Adapt the Text Categorization PyTorch code (see above) and evaluate the performance of the system for these task\n",
    "4. Use a pre-trained embeddings and compare your result. When you use pre-trained embeddings, you have to average the word embeddings of each tokens in ach document to get the unique representation of the document. DOC_EMBEDDING = (TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the <span  style=\"color:red\">spacy/FLAIR </span>document embedding methods\n",
    "6. Report the recall, precision, and F1 scores for both binary and multi-class classification.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.11/site-packages (2.1.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.1-cp311-cp311-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.local/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.local/lib/python3.11/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.local/lib/python3.11/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten:  337935\n",
      "Testdaten:  84484\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Daten einlesen\n",
    "# data = pd.read_csv(\"./data/uci-news-aggregator.csv\")\n",
    "data = pd.read_csv(\"./uci-news-aggregator.csv\")\n",
    "# Daten in Features (X) und Labels (y) aufteilen: Wenn mehr features, dann die weiteren hinzufügen\n",
    "X = data[['TITLE', 'PUBLISHER', 'CATEGORY']]\n",
    "X = data[['TITLE', 'CATEGORY']]\n",
    "y = data['CATEGORY']\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten. Der Parameter stratify führt dazu, dass das feature \"Category\" gleich aufgeteilt sind\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42, stratify=data['CATEGORY'])\n",
    "\n",
    "print(\"Trainingsdaten: \", len(X_train))\n",
    "print(\"Testdaten: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "\n",
    "# hier chrashed der kernel bei mir\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Daten einlesen\n",
    "data = pd.read_csv(\"./uci-news-aggregator.csv\")\n",
    "\n",
    "# Daten vorbereiten\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(data['TITLE'])\n",
    "\n",
    "# Aufteilung in Trainings- und Testdaten\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, data['CATEGORY'], test_size=0.2, random_state=42, stratify=data['CATEGORY'])\n",
    "\n",
    "# PyTorch Dataset und Dataloader erstellen\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.toarray(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = MyDataset(X_train_text, y_train)\n",
    "test_dataset = MyDataset(X_test_text, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Modell erstellen\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleClassifier(input_size=X_train_text.shape[1], output_size=len(data['CATEGORY'].unique()))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Modell trainieren\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Modell auswerten\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test data: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geht iwie nicht :(\n",
    "    \n",
    "from itertools import combinations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "# Parameters\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 150\n",
    "DISPLAY_STEP = 1\n",
    "\n",
    "# Network Parameters\n",
    "HIDDEN_SIZE = 100  # 1st layer and 2nd layer number of features\n",
    "INPUT_SIZE = total_words  # versteh nicht ganz was hier rein muss\n",
    "NUM_CLASSES = 2  # b, t, e, m (only 2 of them each)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Liste der Kategorien\n",
    "categories = ['b', 't', 'e', 'm']\n",
    "\n",
    "# Alle möglichen Paarkombinationen\n",
    "pairs = list(combinations(categories, 2))\n",
    "\n",
    "# NN erstellen\n",
    "class News_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(News_Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "model = News_Net(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()  # Verlustfunktion für Klassifikationsprobleme\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam-Optimierer\n",
    "\n",
    "loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=16)\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test, y_test)), batch_size=64, shuffle=False)\n",
    "\n",
    "# # Trainings- und Test-Datasets erstellen\n",
    "# train_dataset = Dataset(X_train, y_train)\n",
    "# test_dataset = Dataset(X_test, y_test)\n",
    "\n",
    "# # DataLoader erstellen\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

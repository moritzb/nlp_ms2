{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment <span style=\"color:red\">option Four</span> - News Categorization  using PyTorch \n",
    "Download the dataset from https://www.kaggle.com/uciml/news-aggregator-dataset and develop a news classification or categorization model. The dataset contain only titles of a news item and some metadata. The categories of the news items include one of: –<span  style=\"color:red\"> b</span> : business – <span  style=\"color:red\">t</span> : science and technology – <span  style=\"color:red\">e</span> : entertainment and –<span  style=\"color:red\">m</span> : health. \n",
    "\n",
    "1. Prepare training and test dataset: Split the data into training and test set (80% train and 20% test). Make sure they are balanced, otherwise if all <span  style=\"color:red\">b</span> files are on training, your model fails to predict <span  style=\"color:red\">t</span> files in test.\n",
    "2. Binary classification: produce training data for each two categories, such as <span  style=\"color:red\">b </span> and <span  style=\"color:red\"> t</span>, <span  style=\"color:red\">b</span> and <span  style=\"color:red\"> m</span>, <span  style=\"color:red\">e</span> and <span  style=\"color:red\">t</span> and so on. Evaluate the performance and report which categories are easier for the models.\n",
    "3. Adapt the Text Categorization PyTorch code (see above) and evaluate the performance of the system for these task\n",
    "4. Use a pre-trained embeddings and compare your result. When you use pre-trained embeddings, you have to average the word embeddings of each tokens in each document to get the unique representation of the document. DOC_EMBEDDING = (TOKEN1_EMBEDDING + ... + TOKENn_EMBEDDING). You can also use some of the <span  style=\"color:red\">spacy/FLAIR </span>document embedding methods\n",
    "6. Report the recall, precision, and F1 scores for both binary and multi-class classification.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all relevant packages\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Assuming you've downloaded the dataset and stored it locally as 'uci-news-aggregator.csv'\n",
    "df = pd.read_csv(\"uci-news-aggregator.csv\")\n",
    "# Keep only 'TITLE' and 'CATEGORY' columns\n",
    "df = df[['TITLE', 'CATEGORY']]\n",
    "# Drop rows with NaN values in the 'TITLE' column\n",
    "df = df.dropna(subset=['TITLE'])\n",
    "\n",
    "# Map category labels to numerical values\n",
    "label_mapping = {label: i for i, label in enumerate(df['CATEGORY'].unique())}\n",
    "\n",
    "# Split the data into training and test sets maintaining nearly the same percentage as inside the input file(80% train and 20% test)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['CATEGORY'], random_state=42)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Write your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classification (b vs t):\n",
      "Accuracy: 0.96\n",
      "Precision: 0.94\n",
      "Recall: 0.89\n",
      "F1 Score: 0.91\n",
      "Binary Classification (b vs e):\n",
      "Accuracy: 0.98\n",
      "Precision: 0.98\n",
      "Recall: 0.97\n",
      "F1 Score: 0.97\n",
      "Binary Classification (b vs m):\n",
      "Accuracy: 0.98\n",
      "Precision: 0.97\n",
      "Recall: 0.85\n",
      "F1 Score: 0.91\n",
      "Binary Classification (t vs e):\n",
      "Accuracy: 0.98\n",
      "Precision: 0.98\n",
      "Recall: 0.97\n",
      "F1 Score: 0.97\n",
      "Binary Classification (t vs m):\n",
      "Accuracy: 0.98\n",
      "Precision: 0.97\n",
      "Recall: 0.85\n",
      "F1 Score: 0.91\n",
      "Binary Classification (e vs m):\n",
      "Accuracy: 0.98\n",
      "Precision: 0.97\n",
      "Recall: 0.85\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "def binary_classification(train_df, test_df, label1, label2, model=None):\n",
    "    # Combine label mapping and filtering\n",
    "    label_mapping = {label1: 0, label2: 1}\n",
    "    train_df['CATEGORY_BINARY'] = train_df['CATEGORY'].map(label_mapping)\n",
    "    test_df['CATEGORY_BINARY'] = test_df['CATEGORY'].map(label_mapping)\n",
    "\n",
    "    # Handle missing values\n",
    "    train_df['CATEGORY_BINARY'].fillna(0, inplace=True)\n",
    "    test_df['CATEGORY_BINARY'].fillna(0, inplace=True)\n",
    "\n",
    "    # Feature: news titles, Label: binary category\n",
    "    X_train, y_train = train_df['TITLE'], train_df['CATEGORY_BINARY']\n",
    "    X_test, y_test = test_df['TITLE'], test_df['CATEGORY_BINARY']\n",
    "\n",
    "    # Use a pipeline for preprocessing\n",
    "    if model is None:\n",
    "        model = make_pipeline(\n",
    "            TfidfVectorizer(),  # You can use other vectorizers based on your needs\n",
    "            # Impute missing values with the mean\n",
    "            SimpleImputer(strategy='mean'),\n",
    "            LogisticRegression(max_iter=1000)\n",
    "        )\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate and report metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    print(f\"Binary Classification ({label1} vs {label2}):\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.2f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Get all unique pairs of labels\n",
    "label_pairs = list(combinations(label_mapping.keys(), 2))\n",
    "\n",
    "# Iterate over each pair\n",
    "for label1, label2 in label_pairs:\n",
    "    binary_classification(train_df, test_df, label1, label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total texts in train: 337935\n",
      "total texts in test: 84484\n",
      "135402\n",
      "135402\n",
      "Using device: mps\n",
      "Epoch [1/10], Step [4/1126], Loss: 1.1653\n",
      "Epoch [1/10], Step [8/1126], Loss: 0.8322\n",
      "Epoch [1/10], Step [12/1126], Loss: 0.7836\n",
      "Epoch [1/10], Step [16/1126], Loss: 0.5668\n",
      "Epoch [1/10], Step [20/1126], Loss: 0.4230\n",
      "Epoch [1/10], Step [24/1126], Loss: 0.4636\n",
      "Epoch [1/10], Step [28/1126], Loss: 0.6043\n",
      "Epoch [1/10], Step [32/1126], Loss: 0.4303\n",
      "Epoch [1/10], Step [36/1126], Loss: 0.5576\n",
      "Epoch [1/10], Step [40/1126], Loss: 0.4463\n",
      "Epoch [1/10], Step [44/1126], Loss: 0.4204\n",
      "Epoch [1/10], Step [48/1126], Loss: 0.3331\n",
      "Epoch [1/10], Step [52/1126], Loss: 0.4707\n",
      "Epoch [1/10], Step [56/1126], Loss: 0.3779\n",
      "Epoch [1/10], Step [60/1126], Loss: 0.3750\n",
      "Epoch [1/10], Step [64/1126], Loss: 0.2290\n",
      "Epoch [1/10], Step [68/1126], Loss: 0.3324\n",
      "Epoch [1/10], Step [72/1126], Loss: 0.4068\n",
      "Epoch [1/10], Step [76/1126], Loss: 0.3198\n",
      "Epoch [1/10], Step [80/1126], Loss: 0.3980\n",
      "Epoch [1/10], Step [84/1126], Loss: 0.3886\n",
      "Epoch [1/10], Step [88/1126], Loss: 0.3126\n",
      "Epoch [1/10], Step [92/1126], Loss: 0.3018\n",
      "Epoch [1/10], Step [96/1126], Loss: 0.2951\n",
      "Epoch [1/10], Step [100/1126], Loss: 0.3613\n",
      "Epoch [1/10], Step [104/1126], Loss: 0.2801\n",
      "Epoch [1/10], Step [108/1126], Loss: 0.3559\n",
      "Epoch [1/10], Step [112/1126], Loss: 0.2505\n",
      "Epoch [1/10], Step [116/1126], Loss: 0.3162\n",
      "Epoch [1/10], Step [120/1126], Loss: 0.2440\n",
      "Epoch [1/10], Step [124/1126], Loss: 0.3124\n",
      "Epoch [1/10], Step [128/1126], Loss: 0.2415\n",
      "Epoch [1/10], Step [132/1126], Loss: 0.2701\n",
      "Epoch [1/10], Step [136/1126], Loss: 0.3440\n",
      "Epoch [1/10], Step [140/1126], Loss: 0.3492\n",
      "Epoch [1/10], Step [144/1126], Loss: 0.2439\n",
      "Epoch [1/10], Step [148/1126], Loss: 0.3651\n",
      "Epoch [1/10], Step [152/1126], Loss: 0.3151\n",
      "Epoch [1/10], Step [156/1126], Loss: 0.2276\n",
      "Epoch [1/10], Step [160/1126], Loss: 0.2389\n",
      "Epoch [1/10], Step [164/1126], Loss: 0.3003\n",
      "Epoch [1/10], Step [168/1126], Loss: 0.2620\n",
      "Epoch [1/10], Step [172/1126], Loss: 0.2498\n",
      "Epoch [1/10], Step [176/1126], Loss: 0.2025\n",
      "Epoch [1/10], Step [180/1126], Loss: 0.2373\n",
      "Epoch [1/10], Step [184/1126], Loss: 0.3050\n",
      "Epoch [1/10], Step [188/1126], Loss: 0.2615\n",
      "Epoch [1/10], Step [192/1126], Loss: 0.2382\n",
      "Epoch [1/10], Step [196/1126], Loss: 0.2197\n",
      "Epoch [1/10], Step [200/1126], Loss: 0.1945\n",
      "Epoch [1/10], Step [204/1126], Loss: 0.2383\n",
      "Epoch [1/10], Step [208/1126], Loss: 0.3249\n",
      "Epoch [1/10], Step [212/1126], Loss: 0.2222\n",
      "Epoch [1/10], Step [216/1126], Loss: 0.2635\n",
      "Epoch [1/10], Step [220/1126], Loss: 0.2906\n",
      "Epoch [1/10], Step [224/1126], Loss: 0.3699\n",
      "Epoch [1/10], Step [228/1126], Loss: 0.2204\n",
      "Epoch [1/10], Step [232/1126], Loss: 0.3094\n",
      "Epoch [1/10], Step [236/1126], Loss: 0.2361\n",
      "Epoch [1/10], Step [240/1126], Loss: 0.2205\n",
      "Epoch [1/10], Step [244/1126], Loss: 0.2636\n",
      "Epoch [1/10], Step [248/1126], Loss: 0.2253\n",
      "Epoch [1/10], Step [252/1126], Loss: 0.2350\n",
      "Epoch [1/10], Step [256/1126], Loss: 0.2873\n",
      "Epoch [1/10], Step [260/1126], Loss: 0.3660\n",
      "Epoch [1/10], Step [264/1126], Loss: 0.2126\n",
      "Epoch [1/10], Step [268/1126], Loss: 0.3006\n",
      "Epoch [1/10], Step [272/1126], Loss: 0.1427\n",
      "Epoch [1/10], Step [276/1126], Loss: 0.2671\n",
      "Epoch [1/10], Step [280/1126], Loss: 0.3006\n",
      "Epoch [1/10], Step [284/1126], Loss: 0.2800\n",
      "Epoch [1/10], Step [288/1126], Loss: 0.2718\n",
      "Epoch [1/10], Step [292/1126], Loss: 0.2235\n",
      "Epoch [1/10], Step [296/1126], Loss: 0.1981\n",
      "Epoch [1/10], Step [300/1126], Loss: 0.2231\n",
      "Epoch [1/10], Step [304/1126], Loss: 0.2559\n",
      "Epoch [1/10], Step [308/1126], Loss: 0.2933\n",
      "Epoch [1/10], Step [312/1126], Loss: 0.2395\n",
      "Epoch [1/10], Step [316/1126], Loss: 0.2147\n",
      "Epoch [1/10], Step [320/1126], Loss: 0.2312\n",
      "Epoch [1/10], Step [324/1126], Loss: 0.2518\n",
      "Epoch [1/10], Step [328/1126], Loss: 0.2074\n",
      "Epoch [1/10], Step [332/1126], Loss: 0.2493\n",
      "Epoch [1/10], Step [336/1126], Loss: 0.1884\n",
      "Epoch [1/10], Step [340/1126], Loss: 0.2962\n",
      "Epoch [1/10], Step [344/1126], Loss: 0.2540\n",
      "Epoch [1/10], Step [348/1126], Loss: 0.2425\n",
      "Epoch [1/10], Step [352/1126], Loss: 0.2419\n",
      "Epoch [1/10], Step [356/1126], Loss: 0.1463\n",
      "Epoch [1/10], Step [360/1126], Loss: 0.1913\n",
      "Epoch [1/10], Step [364/1126], Loss: 0.1510\n",
      "Epoch [1/10], Step [368/1126], Loss: 0.2298\n",
      "Epoch [1/10], Step [372/1126], Loss: 0.2368\n",
      "Epoch [1/10], Step [376/1126], Loss: 0.2706\n",
      "Epoch [1/10], Step [380/1126], Loss: 0.2136\n",
      "Epoch [1/10], Step [384/1126], Loss: 0.2733\n",
      "Epoch [1/10], Step [388/1126], Loss: 0.2044\n",
      "Epoch [1/10], Step [392/1126], Loss: 0.2106\n",
      "Epoch [1/10], Step [396/1126], Loss: 0.2101\n",
      "Epoch [1/10], Step [400/1126], Loss: 0.1713\n",
      "Epoch [1/10], Step [404/1126], Loss: 0.2665\n",
      "Epoch [1/10], Step [408/1126], Loss: 0.3482\n",
      "Epoch [1/10], Step [412/1126], Loss: 0.2204\n",
      "Epoch [1/10], Step [416/1126], Loss: 0.2884\n",
      "Epoch [1/10], Step [420/1126], Loss: 0.3080\n",
      "Epoch [1/10], Step [424/1126], Loss: 0.2279\n",
      "Epoch [1/10], Step [428/1126], Loss: 0.2378\n",
      "Epoch [1/10], Step [432/1126], Loss: 0.3001\n",
      "Epoch [1/10], Step [436/1126], Loss: 0.2040\n",
      "Epoch [1/10], Step [440/1126], Loss: 0.2985\n",
      "Epoch [1/10], Step [444/1126], Loss: 0.2036\n",
      "Epoch [1/10], Step [448/1126], Loss: 0.1721\n",
      "Epoch [1/10], Step [452/1126], Loss: 0.1356\n",
      "Epoch [1/10], Step [456/1126], Loss: 0.1940\n",
      "Epoch [1/10], Step [460/1126], Loss: 0.2790\n",
      "Epoch [1/10], Step [464/1126], Loss: 0.2239\n",
      "Epoch [1/10], Step [468/1126], Loss: 0.2068\n",
      "Epoch [1/10], Step [472/1126], Loss: 0.2105\n",
      "Epoch [1/10], Step [476/1126], Loss: 0.3350\n",
      "Epoch [1/10], Step [480/1126], Loss: 0.2816\n",
      "Epoch [1/10], Step [484/1126], Loss: 0.2599\n",
      "Epoch [1/10], Step [488/1126], Loss: 0.1919\n",
      "Epoch [1/10], Step [492/1126], Loss: 0.1329\n",
      "Epoch [1/10], Step [496/1126], Loss: 0.3055\n",
      "Epoch [1/10], Step [500/1126], Loss: 0.2449\n",
      "Epoch [1/10], Step [504/1126], Loss: 0.2177\n",
      "Epoch [1/10], Step [508/1126], Loss: 0.1949\n",
      "Epoch [1/10], Step [512/1126], Loss: 0.2879\n",
      "Epoch [1/10], Step [516/1126], Loss: 0.1782\n",
      "Epoch [1/10], Step [520/1126], Loss: 0.2224\n",
      "Epoch [1/10], Step [524/1126], Loss: 0.1976\n",
      "Epoch [1/10], Step [528/1126], Loss: 0.2002\n",
      "Epoch [1/10], Step [532/1126], Loss: 0.2605\n",
      "Epoch [1/10], Step [536/1126], Loss: 0.2404\n",
      "Epoch [1/10], Step [540/1126], Loss: 0.1573\n",
      "Epoch [1/10], Step [544/1126], Loss: 0.2513\n",
      "Epoch [1/10], Step [548/1126], Loss: 0.2063\n",
      "Epoch [1/10], Step [552/1126], Loss: 0.2321\n",
      "Epoch [1/10], Step [556/1126], Loss: 0.2407\n",
      "Epoch [1/10], Step [560/1126], Loss: 0.2023\n",
      "Epoch [1/10], Step [564/1126], Loss: 0.2844\n",
      "Epoch [1/10], Step [568/1126], Loss: 0.2113\n",
      "Epoch [1/10], Step [572/1126], Loss: 0.2263\n",
      "Epoch [1/10], Step [576/1126], Loss: 0.2140\n",
      "Epoch [1/10], Step [580/1126], Loss: 0.2482\n",
      "Epoch [1/10], Step [584/1126], Loss: 0.2250\n",
      "Epoch [1/10], Step [588/1126], Loss: 0.2830\n",
      "Epoch [1/10], Step [592/1126], Loss: 0.2865\n",
      "Epoch [1/10], Step [596/1126], Loss: 0.1834\n",
      "Epoch [1/10], Step [600/1126], Loss: 0.2050\n",
      "Epoch [1/10], Step [604/1126], Loss: 0.1810\n",
      "Epoch [1/10], Step [608/1126], Loss: 0.2448\n",
      "Epoch [1/10], Step [612/1126], Loss: 0.2135\n",
      "Epoch [1/10], Step [616/1126], Loss: 0.1815\n",
      "Epoch [1/10], Step [620/1126], Loss: 0.1804\n",
      "Epoch [1/10], Step [624/1126], Loss: 0.2513\n",
      "Epoch [1/10], Step [628/1126], Loss: 0.1257\n",
      "Epoch [1/10], Step [632/1126], Loss: 0.1830\n",
      "Epoch [1/10], Step [636/1126], Loss: 0.1944\n",
      "Epoch [1/10], Step [640/1126], Loss: 0.1380\n",
      "Epoch [1/10], Step [644/1126], Loss: 0.2679\n",
      "Epoch [1/10], Step [648/1126], Loss: 0.1888\n",
      "Epoch [1/10], Step [652/1126], Loss: 0.1794\n",
      "Epoch [1/10], Step [656/1126], Loss: 0.1890\n",
      "Epoch [1/10], Step [660/1126], Loss: 0.2993\n",
      "Epoch [1/10], Step [664/1126], Loss: 0.1640\n",
      "Epoch [1/10], Step [668/1126], Loss: 0.2196\n",
      "Epoch [1/10], Step [672/1126], Loss: 0.1553\n",
      "Epoch [1/10], Step [676/1126], Loss: 0.1763\n",
      "Epoch [1/10], Step [680/1126], Loss: 0.1681\n",
      "Epoch [1/10], Step [684/1126], Loss: 0.2351\n",
      "Epoch [1/10], Step [688/1126], Loss: 0.1820\n",
      "Epoch [1/10], Step [692/1126], Loss: 0.1490\n",
      "Epoch [1/10], Step [696/1126], Loss: 0.1657\n",
      "Epoch [1/10], Step [700/1126], Loss: 0.2149\n",
      "Epoch [1/10], Step [704/1126], Loss: 0.1522\n",
      "Epoch [1/10], Step [708/1126], Loss: 0.2996\n",
      "Epoch [1/10], Step [712/1126], Loss: 0.1735\n",
      "Epoch [1/10], Step [716/1126], Loss: 0.1953\n",
      "Epoch [1/10], Step [720/1126], Loss: 0.1962\n",
      "Epoch [1/10], Step [724/1126], Loss: 0.2016\n",
      "Epoch [1/10], Step [728/1126], Loss: 0.1999\n",
      "Epoch [1/10], Step [732/1126], Loss: 0.2287\n",
      "Epoch [1/10], Step [736/1126], Loss: 0.2811\n",
      "Epoch [1/10], Step [740/1126], Loss: 0.2541\n",
      "Epoch [1/10], Step [744/1126], Loss: 0.2382\n",
      "Epoch [1/10], Step [748/1126], Loss: 0.1529\n",
      "Epoch [1/10], Step [752/1126], Loss: 0.2504\n",
      "Epoch [1/10], Step [756/1126], Loss: 0.2520\n",
      "Epoch [1/10], Step [760/1126], Loss: 0.2212\n",
      "Epoch [1/10], Step [764/1126], Loss: 0.3166\n",
      "Epoch [1/10], Step [768/1126], Loss: 0.1686\n",
      "Epoch [1/10], Step [772/1126], Loss: 0.1877\n",
      "Epoch [1/10], Step [776/1126], Loss: 0.2172\n",
      "Epoch [1/10], Step [780/1126], Loss: 0.2045\n",
      "Epoch [1/10], Step [784/1126], Loss: 0.2045\n",
      "Epoch [1/10], Step [788/1126], Loss: 0.2672\n",
      "Epoch [1/10], Step [792/1126], Loss: 0.1902\n",
      "Epoch [1/10], Step [796/1126], Loss: 0.1839\n",
      "Epoch [1/10], Step [800/1126], Loss: 0.1633\n",
      "Epoch [1/10], Step [804/1126], Loss: 0.1583\n",
      "Epoch [1/10], Step [808/1126], Loss: 0.2001\n",
      "Epoch [1/10], Step [812/1126], Loss: 0.2493\n",
      "Epoch [1/10], Step [816/1126], Loss: 0.1575\n",
      "Epoch [1/10], Step [820/1126], Loss: 0.1678\n",
      "Epoch [1/10], Step [824/1126], Loss: 0.2081\n",
      "Epoch [1/10], Step [828/1126], Loss: 0.1842\n",
      "Epoch [1/10], Step [832/1126], Loss: 0.2486\n",
      "Epoch [1/10], Step [836/1126], Loss: 0.3233\n",
      "Epoch [1/10], Step [840/1126], Loss: 0.1524\n",
      "Epoch [1/10], Step [844/1126], Loss: 0.2058\n",
      "Epoch [1/10], Step [848/1126], Loss: 0.1877\n",
      "Epoch [1/10], Step [852/1126], Loss: 0.1885\n",
      "Epoch [1/10], Step [856/1126], Loss: 0.1942\n",
      "Epoch [1/10], Step [860/1126], Loss: 0.1290\n",
      "Epoch [1/10], Step [864/1126], Loss: 0.1849\n",
      "Epoch [1/10], Step [868/1126], Loss: 0.2333\n",
      "Epoch [1/10], Step [872/1126], Loss: 0.1719\n",
      "Epoch [1/10], Step [876/1126], Loss: 0.2137\n",
      "Epoch [1/10], Step [880/1126], Loss: 0.1720\n",
      "Epoch [1/10], Step [884/1126], Loss: 0.2003\n",
      "Epoch [1/10], Step [888/1126], Loss: 0.2866\n",
      "Epoch [1/10], Step [892/1126], Loss: 0.2217\n",
      "Epoch [1/10], Step [896/1126], Loss: 0.2194\n",
      "Epoch [1/10], Step [900/1126], Loss: 0.1977\n",
      "Epoch [1/10], Step [904/1126], Loss: 0.1646\n",
      "Epoch [1/10], Step [908/1126], Loss: 0.1198\n",
      "Epoch [1/10], Step [912/1126], Loss: 0.1463\n",
      "Epoch [1/10], Step [916/1126], Loss: 0.2548\n",
      "Epoch [1/10], Step [920/1126], Loss: 0.1867\n",
      "Epoch [1/10], Step [924/1126], Loss: 0.1482\n",
      "Epoch [1/10], Step [928/1126], Loss: 0.1550\n",
      "Epoch [1/10], Step [932/1126], Loss: 0.1571\n",
      "Epoch [1/10], Step [936/1126], Loss: 0.2218\n",
      "Epoch [1/10], Step [940/1126], Loss: 0.1901\n",
      "Epoch [1/10], Step [944/1126], Loss: 0.1710\n",
      "Epoch [1/10], Step [948/1126], Loss: 0.2016\n",
      "Epoch [1/10], Step [952/1126], Loss: 0.1994\n",
      "Epoch [1/10], Step [956/1126], Loss: 0.2296\n",
      "Epoch [1/10], Step [960/1126], Loss: 0.1971\n",
      "Epoch [1/10], Step [964/1126], Loss: 0.1539\n",
      "Epoch [1/10], Step [968/1126], Loss: 0.1868\n",
      "Epoch [1/10], Step [972/1126], Loss: 0.0907\n",
      "Epoch [1/10], Step [976/1126], Loss: 0.1672\n",
      "Epoch [1/10], Step [980/1126], Loss: 0.1652\n",
      "Epoch [1/10], Step [984/1126], Loss: 0.1636\n",
      "Epoch [1/10], Step [988/1126], Loss: 0.2253\n",
      "Epoch [1/10], Step [992/1126], Loss: 0.1683\n",
      "Epoch [1/10], Step [996/1126], Loss: 0.2894\n",
      "Epoch [1/10], Step [1000/1126], Loss: 0.1572\n",
      "Epoch [1/10], Step [1004/1126], Loss: 0.2008\n",
      "Epoch [1/10], Step [1008/1126], Loss: 0.1314\n",
      "Epoch [1/10], Step [1012/1126], Loss: 0.2028\n",
      "Epoch [1/10], Step [1016/1126], Loss: 0.1821\n",
      "Epoch [1/10], Step [1020/1126], Loss: 0.1980\n",
      "Epoch [1/10], Step [1024/1126], Loss: 0.2304\n",
      "Epoch [1/10], Step [1028/1126], Loss: 0.1793\n",
      "Epoch [1/10], Step [1032/1126], Loss: 0.1879\n",
      "Epoch [1/10], Step [1036/1126], Loss: 0.1684\n",
      "Epoch [1/10], Step [1040/1126], Loss: 0.2585\n",
      "Epoch [1/10], Step [1044/1126], Loss: 0.1273\n",
      "Epoch [1/10], Step [1048/1126], Loss: 0.2565\n",
      "Epoch [1/10], Step [1052/1126], Loss: 0.2626\n",
      "Epoch [1/10], Step [1056/1126], Loss: 0.1815\n",
      "Epoch [1/10], Step [1060/1126], Loss: 0.1990\n",
      "Epoch [1/10], Step [1064/1126], Loss: 0.2640\n",
      "Epoch [1/10], Step [1068/1126], Loss: 0.1384\n",
      "Epoch [1/10], Step [1072/1126], Loss: 0.2499\n",
      "Epoch [1/10], Step [1076/1126], Loss: 0.1691\n",
      "Epoch [1/10], Step [1080/1126], Loss: 0.2166\n",
      "Epoch [1/10], Step [1084/1126], Loss: 0.1844\n",
      "Epoch [1/10], Step [1088/1126], Loss: 0.1780\n",
      "Epoch [1/10], Step [1092/1126], Loss: 0.1991\n",
      "Epoch [1/10], Step [1096/1126], Loss: 0.1788\n",
      "Epoch [1/10], Step [1100/1126], Loss: 0.2438\n",
      "Epoch [1/10], Step [1104/1126], Loss: 0.2301\n",
      "Epoch [1/10], Step [1108/1126], Loss: 0.1848\n",
      "Epoch [1/10], Step [1112/1126], Loss: 0.1970\n",
      "Epoch [1/10], Step [1116/1126], Loss: 0.1933\n",
      "Epoch [1/10], Step [1120/1126], Loss: 0.1697\n",
      "Epoch [1/10], Step [1124/1126], Loss: 0.2627\n",
      "Epoch [2/10], Step [4/1126], Loss: 0.2469\n",
      "Epoch [2/10], Step [8/1126], Loss: 0.1477\n",
      "Epoch [2/10], Step [12/1126], Loss: 0.1456\n",
      "Epoch [2/10], Step [16/1126], Loss: 0.1321\n",
      "Epoch [2/10], Step [20/1126], Loss: 0.1217\n",
      "Epoch [2/10], Step [24/1126], Loss: 0.1102\n",
      "Epoch [2/10], Step [28/1126], Loss: 0.1781\n",
      "Epoch [2/10], Step [32/1126], Loss: 0.0785\n",
      "Epoch [2/10], Step [36/1126], Loss: 0.2303\n",
      "Epoch [2/10], Step [40/1126], Loss: 0.1427\n",
      "Epoch [2/10], Step [44/1126], Loss: 0.1925\n",
      "Epoch [2/10], Step [48/1126], Loss: 0.1286\n",
      "Epoch [2/10], Step [52/1126], Loss: 0.1853\n",
      "Epoch [2/10], Step [56/1126], Loss: 0.1668\n",
      "Epoch [2/10], Step [60/1126], Loss: 0.1738\n",
      "Epoch [2/10], Step [64/1126], Loss: 0.1174\n",
      "Epoch [2/10], Step [68/1126], Loss: 0.1676\n",
      "Epoch [2/10], Step [72/1126], Loss: 0.1755\n",
      "Epoch [2/10], Step [76/1126], Loss: 0.1134\n",
      "Epoch [2/10], Step [80/1126], Loss: 0.1529\n",
      "Epoch [2/10], Step [84/1126], Loss: 0.1431\n",
      "Epoch [2/10], Step [88/1126], Loss: 0.1894\n",
      "Epoch [2/10], Step [92/1126], Loss: 0.1810\n",
      "Epoch [2/10], Step [96/1126], Loss: 0.1446\n",
      "Epoch [2/10], Step [100/1126], Loss: 0.1385\n",
      "Epoch [2/10], Step [104/1126], Loss: 0.1353\n",
      "Epoch [2/10], Step [108/1126], Loss: 0.1504\n",
      "Epoch [2/10], Step [112/1126], Loss: 0.1424\n",
      "Epoch [2/10], Step [116/1126], Loss: 0.1455\n",
      "Epoch [2/10], Step [120/1126], Loss: 0.1189\n",
      "Epoch [2/10], Step [124/1126], Loss: 0.1663\n",
      "Epoch [2/10], Step [128/1126], Loss: 0.1696\n",
      "Epoch [2/10], Step [132/1126], Loss: 0.1288\n",
      "Epoch [2/10], Step [136/1126], Loss: 0.1790\n",
      "Epoch [2/10], Step [140/1126], Loss: 0.1437\n",
      "Epoch [2/10], Step [144/1126], Loss: 0.1182\n",
      "Epoch [2/10], Step [148/1126], Loss: 0.2121\n",
      "Epoch [2/10], Step [152/1126], Loss: 0.1755\n",
      "Epoch [2/10], Step [156/1126], Loss: 0.1232\n",
      "Epoch [2/10], Step [160/1126], Loss: 0.0942\n",
      "Epoch [2/10], Step [164/1126], Loss: 0.0874\n",
      "Epoch [2/10], Step [168/1126], Loss: 0.1497\n",
      "Epoch [2/10], Step [172/1126], Loss: 0.1044\n",
      "Epoch [2/10], Step [176/1126], Loss: 0.1037\n",
      "Epoch [2/10], Step [180/1126], Loss: 0.1027\n",
      "Epoch [2/10], Step [184/1126], Loss: 0.1204\n",
      "Epoch [2/10], Step [188/1126], Loss: 0.1070\n",
      "Epoch [2/10], Step [192/1126], Loss: 0.1075\n",
      "Epoch [2/10], Step [196/1126], Loss: 0.1332\n",
      "Epoch [2/10], Step [200/1126], Loss: 0.1245\n",
      "Epoch [2/10], Step [204/1126], Loss: 0.1436\n",
      "Epoch [2/10], Step [208/1126], Loss: 0.1916\n",
      "Epoch [2/10], Step [212/1126], Loss: 0.1302\n",
      "Epoch [2/10], Step [216/1126], Loss: 0.1165\n",
      "Epoch [2/10], Step [220/1126], Loss: 0.1176\n",
      "Epoch [2/10], Step [224/1126], Loss: 0.1441\n",
      "Epoch [2/10], Step [228/1126], Loss: 0.1038\n",
      "Epoch [2/10], Step [232/1126], Loss: 0.1684\n",
      "Epoch [2/10], Step [236/1126], Loss: 0.0924\n",
      "Epoch [2/10], Step [240/1126], Loss: 0.1103\n",
      "Epoch [2/10], Step [244/1126], Loss: 0.1223\n",
      "Epoch [2/10], Step [248/1126], Loss: 0.1210\n",
      "Epoch [2/10], Step [252/1126], Loss: 0.0835\n",
      "Epoch [2/10], Step [256/1126], Loss: 0.1127\n",
      "Epoch [2/10], Step [260/1126], Loss: 0.2049\n",
      "Epoch [2/10], Step [264/1126], Loss: 0.1159\n",
      "Epoch [2/10], Step [268/1126], Loss: 0.1432\n",
      "Epoch [2/10], Step [272/1126], Loss: 0.0744\n",
      "Epoch [2/10], Step [276/1126], Loss: 0.1016\n",
      "Epoch [2/10], Step [280/1126], Loss: 0.1338\n",
      "Epoch [2/10], Step [284/1126], Loss: 0.1692\n",
      "Epoch [2/10], Step [288/1126], Loss: 0.1274\n",
      "Epoch [2/10], Step [292/1126], Loss: 0.1019\n",
      "Epoch [2/10], Step [296/1126], Loss: 0.1059\n",
      "Epoch [2/10], Step [300/1126], Loss: 0.1216\n",
      "Epoch [2/10], Step [304/1126], Loss: 0.1277\n",
      "Epoch [2/10], Step [308/1126], Loss: 0.1240\n",
      "Epoch [2/10], Step [312/1126], Loss: 0.1059\n",
      "Epoch [2/10], Step [316/1126], Loss: 0.1058\n",
      "Epoch [2/10], Step [320/1126], Loss: 0.1589\n",
      "Epoch [2/10], Step [324/1126], Loss: 0.1436\n",
      "Epoch [2/10], Step [328/1126], Loss: 0.0899\n",
      "Epoch [2/10], Step [332/1126], Loss: 0.1836\n",
      "Epoch [2/10], Step [336/1126], Loss: 0.0965\n",
      "Epoch [2/10], Step [340/1126], Loss: 0.1288\n",
      "Epoch [2/10], Step [344/1126], Loss: 0.0954\n",
      "Epoch [2/10], Step [348/1126], Loss: 0.2375\n",
      "Epoch [2/10], Step [352/1126], Loss: 0.1090\n",
      "Epoch [2/10], Step [356/1126], Loss: 0.0771\n",
      "Epoch [2/10], Step [360/1126], Loss: 0.0918\n",
      "Epoch [2/10], Step [364/1126], Loss: 0.0807\n",
      "Epoch [2/10], Step [368/1126], Loss: 0.1375\n",
      "Epoch [2/10], Step [372/1126], Loss: 0.1393\n",
      "Epoch [2/10], Step [376/1126], Loss: 0.1710\n",
      "Epoch [2/10], Step [380/1126], Loss: 0.0810\n",
      "Epoch [2/10], Step [384/1126], Loss: 0.1079\n",
      "Epoch [2/10], Step [388/1126], Loss: 0.0935\n",
      "Epoch [2/10], Step [392/1126], Loss: 0.1097\n",
      "Epoch [2/10], Step [396/1126], Loss: 0.0535\n",
      "Epoch [2/10], Step [400/1126], Loss: 0.0859\n",
      "Epoch [2/10], Step [404/1126], Loss: 0.1303\n",
      "Epoch [2/10], Step [408/1126], Loss: 0.1867\n",
      "Epoch [2/10], Step [412/1126], Loss: 0.0960\n",
      "Epoch [2/10], Step [416/1126], Loss: 0.1331\n",
      "Epoch [2/10], Step [420/1126], Loss: 0.1278\n",
      "Epoch [2/10], Step [424/1126], Loss: 0.1104\n",
      "Epoch [2/10], Step [428/1126], Loss: 0.1221\n",
      "Epoch [2/10], Step [432/1126], Loss: 0.1484\n",
      "Epoch [2/10], Step [436/1126], Loss: 0.0968\n",
      "Epoch [2/10], Step [440/1126], Loss: 0.1126\n",
      "Epoch [2/10], Step [444/1126], Loss: 0.0934\n",
      "Epoch [2/10], Step [448/1126], Loss: 0.1032\n",
      "Epoch [2/10], Step [452/1126], Loss: 0.1040\n",
      "Epoch [2/10], Step [456/1126], Loss: 0.0510\n",
      "Epoch [2/10], Step [460/1126], Loss: 0.1518\n",
      "Epoch [2/10], Step [464/1126], Loss: 0.1950\n",
      "Epoch [2/10], Step [468/1126], Loss: 0.0696\n",
      "Epoch [2/10], Step [472/1126], Loss: 0.1103\n",
      "Epoch [2/10], Step [476/1126], Loss: 0.1532\n",
      "Epoch [2/10], Step [480/1126], Loss: 0.0992\n",
      "Epoch [2/10], Step [484/1126], Loss: 0.1140\n",
      "Epoch [2/10], Step [488/1126], Loss: 0.0960\n",
      "Epoch [2/10], Step [492/1126], Loss: 0.0454\n",
      "Epoch [2/10], Step [496/1126], Loss: 0.1624\n",
      "Epoch [2/10], Step [500/1126], Loss: 0.0972\n",
      "Epoch [2/10], Step [504/1126], Loss: 0.0695\n",
      "Epoch [2/10], Step [508/1126], Loss: 0.0946\n",
      "Epoch [2/10], Step [512/1126], Loss: 0.1214\n",
      "Epoch [2/10], Step [516/1126], Loss: 0.0905\n",
      "Epoch [2/10], Step [520/1126], Loss: 0.1207\n",
      "Epoch [2/10], Step [524/1126], Loss: 0.0985\n",
      "Epoch [2/10], Step [528/1126], Loss: 0.0918\n",
      "Epoch [2/10], Step [532/1126], Loss: 0.1053\n",
      "Epoch [2/10], Step [536/1126], Loss: 0.1094\n",
      "Epoch [2/10], Step [540/1126], Loss: 0.1156\n",
      "Epoch [2/10], Step [544/1126], Loss: 0.0866\n",
      "Epoch [2/10], Step [548/1126], Loss: 0.0853\n",
      "Epoch [2/10], Step [552/1126], Loss: 0.0739\n",
      "Epoch [2/10], Step [556/1126], Loss: 0.1374\n",
      "Epoch [2/10], Step [560/1126], Loss: 0.1026\n",
      "Epoch [2/10], Step [564/1126], Loss: 0.2130\n",
      "Epoch [2/10], Step [568/1126], Loss: 0.1131\n",
      "Epoch [2/10], Step [572/1126], Loss: 0.1313\n",
      "Epoch [2/10], Step [576/1126], Loss: 0.0708\n",
      "Epoch [2/10], Step [580/1126], Loss: 0.1054\n",
      "Epoch [2/10], Step [584/1126], Loss: 0.1139\n",
      "Epoch [2/10], Step [588/1126], Loss: 0.1590\n",
      "Epoch [2/10], Step [592/1126], Loss: 0.1276\n",
      "Epoch [2/10], Step [596/1126], Loss: 0.0806\n",
      "Epoch [2/10], Step [600/1126], Loss: 0.1209\n",
      "Epoch [2/10], Step [604/1126], Loss: 0.0952\n",
      "Epoch [2/10], Step [608/1126], Loss: 0.1519\n",
      "Epoch [2/10], Step [612/1126], Loss: 0.0906\n",
      "Epoch [2/10], Step [616/1126], Loss: 0.1137\n",
      "Epoch [2/10], Step [620/1126], Loss: 0.1094\n",
      "Epoch [2/10], Step [624/1126], Loss: 0.1623\n",
      "Epoch [2/10], Step [628/1126], Loss: 0.1004\n",
      "Epoch [2/10], Step [632/1126], Loss: 0.1347\n",
      "Epoch [2/10], Step [636/1126], Loss: 0.0907\n",
      "Epoch [2/10], Step [640/1126], Loss: 0.0710\n",
      "Epoch [2/10], Step [644/1126], Loss: 0.1508\n",
      "Epoch [2/10], Step [648/1126], Loss: 0.0959\n",
      "Epoch [2/10], Step [652/1126], Loss: 0.1207\n",
      "Epoch [2/10], Step [656/1126], Loss: 0.0789\n",
      "Epoch [2/10], Step [660/1126], Loss: 0.1732\n",
      "Epoch [2/10], Step [664/1126], Loss: 0.0420\n",
      "Epoch [2/10], Step [668/1126], Loss: 0.0772\n",
      "Epoch [2/10], Step [672/1126], Loss: 0.0816\n",
      "Epoch [2/10], Step [676/1126], Loss: 0.0576\n",
      "Epoch [2/10], Step [680/1126], Loss: 0.0746\n",
      "Epoch [2/10], Step [684/1126], Loss: 0.1664\n",
      "Epoch [2/10], Step [688/1126], Loss: 0.0885\n",
      "Epoch [2/10], Step [692/1126], Loss: 0.0583\n",
      "Epoch [2/10], Step [696/1126], Loss: 0.1478\n",
      "Epoch [2/10], Step [700/1126], Loss: 0.0731\n",
      "Epoch [2/10], Step [704/1126], Loss: 0.0798\n",
      "Epoch [2/10], Step [708/1126], Loss: 0.1624\n",
      "Epoch [2/10], Step [712/1126], Loss: 0.0687\n",
      "Epoch [2/10], Step [716/1126], Loss: 0.1095\n",
      "Epoch [2/10], Step [720/1126], Loss: 0.0938\n",
      "Epoch [2/10], Step [724/1126], Loss: 0.0692\n",
      "Epoch [2/10], Step [728/1126], Loss: 0.1327\n",
      "Epoch [2/10], Step [732/1126], Loss: 0.1087\n",
      "Epoch [2/10], Step [736/1126], Loss: 0.1611\n",
      "Epoch [2/10], Step [740/1126], Loss: 0.1307\n",
      "Epoch [2/10], Step [744/1126], Loss: 0.1249\n",
      "Epoch [2/10], Step [748/1126], Loss: 0.0695\n",
      "Epoch [2/10], Step [752/1126], Loss: 0.1406\n",
      "Epoch [2/10], Step [756/1126], Loss: 0.1697\n",
      "Epoch [2/10], Step [760/1126], Loss: 0.1415\n",
      "Epoch [2/10], Step [764/1126], Loss: 0.1618\n",
      "Epoch [2/10], Step [768/1126], Loss: 0.0767\n",
      "Epoch [2/10], Step [772/1126], Loss: 0.0933\n",
      "Epoch [2/10], Step [776/1126], Loss: 0.1005\n",
      "Epoch [2/10], Step [780/1126], Loss: 0.0907\n",
      "Epoch [2/10], Step [784/1126], Loss: 0.1388\n",
      "Epoch [2/10], Step [788/1126], Loss: 0.1419\n",
      "Epoch [2/10], Step [792/1126], Loss: 0.1024\n",
      "Epoch [2/10], Step [796/1126], Loss: 0.1174\n",
      "Epoch [2/10], Step [800/1126], Loss: 0.1417\n",
      "Epoch [2/10], Step [804/1126], Loss: 0.0690\n",
      "Epoch [2/10], Step [808/1126], Loss: 0.1060\n",
      "Epoch [2/10], Step [812/1126], Loss: 0.1257\n",
      "Epoch [2/10], Step [816/1126], Loss: 0.1211\n",
      "Epoch [2/10], Step [820/1126], Loss: 0.1147\n",
      "Epoch [2/10], Step [824/1126], Loss: 0.1747\n",
      "Epoch [2/10], Step [828/1126], Loss: 0.0901\n",
      "Epoch [2/10], Step [832/1126], Loss: 0.1714\n",
      "Epoch [2/10], Step [836/1126], Loss: 0.2658\n",
      "Epoch [2/10], Step [840/1126], Loss: 0.0643\n",
      "Epoch [2/10], Step [844/1126], Loss: 0.1276\n",
      "Epoch [2/10], Step [848/1126], Loss: 0.1263\n",
      "Epoch [2/10], Step [852/1126], Loss: 0.0993\n",
      "Epoch [2/10], Step [856/1126], Loss: 0.1583\n",
      "Epoch [2/10], Step [860/1126], Loss: 0.0381\n",
      "Epoch [2/10], Step [864/1126], Loss: 0.0877\n",
      "Epoch [2/10], Step [868/1126], Loss: 0.1089\n",
      "Epoch [2/10], Step [872/1126], Loss: 0.0875\n",
      "Epoch [2/10], Step [876/1126], Loss: 0.1057\n",
      "Epoch [2/10], Step [880/1126], Loss: 0.1054\n",
      "Epoch [2/10], Step [884/1126], Loss: 0.0676\n",
      "Epoch [2/10], Step [888/1126], Loss: 0.1541\n",
      "Epoch [2/10], Step [892/1126], Loss: 0.1324\n",
      "Epoch [2/10], Step [896/1126], Loss: 0.1266\n",
      "Epoch [2/10], Step [900/1126], Loss: 0.1117\n",
      "Epoch [2/10], Step [904/1126], Loss: 0.0959\n",
      "Epoch [2/10], Step [908/1126], Loss: 0.0948\n",
      "Epoch [2/10], Step [912/1126], Loss: 0.0766\n",
      "Epoch [2/10], Step [916/1126], Loss: 0.1418\n",
      "Epoch [2/10], Step [920/1126], Loss: 0.1797\n",
      "Epoch [2/10], Step [924/1126], Loss: 0.0396\n",
      "Epoch [2/10], Step [928/1126], Loss: 0.0662\n",
      "Epoch [2/10], Step [932/1126], Loss: 0.0767\n",
      "Epoch [2/10], Step [936/1126], Loss: 0.0644\n",
      "Epoch [2/10], Step [940/1126], Loss: 0.0864\n",
      "Epoch [2/10], Step [944/1126], Loss: 0.0925\n",
      "Epoch [2/10], Step [948/1126], Loss: 0.1432\n",
      "Epoch [2/10], Step [952/1126], Loss: 0.1328\n",
      "Epoch [2/10], Step [956/1126], Loss: 0.1401\n",
      "Epoch [2/10], Step [960/1126], Loss: 0.1074\n",
      "Epoch [2/10], Step [964/1126], Loss: 0.1037\n",
      "Epoch [2/10], Step [968/1126], Loss: 0.1249\n",
      "Epoch [2/10], Step [972/1126], Loss: 0.0792\n",
      "Epoch [2/10], Step [976/1126], Loss: 0.0759\n",
      "Epoch [2/10], Step [980/1126], Loss: 0.1204\n",
      "Epoch [2/10], Step [984/1126], Loss: 0.0792\n",
      "Epoch [2/10], Step [988/1126], Loss: 0.1057\n",
      "Epoch [2/10], Step [992/1126], Loss: 0.0748\n",
      "Epoch [2/10], Step [996/1126], Loss: 0.1649\n",
      "Epoch [2/10], Step [1000/1126], Loss: 0.1603\n",
      "Epoch [2/10], Step [1004/1126], Loss: 0.1224\n",
      "Epoch [2/10], Step [1008/1126], Loss: 0.0830\n",
      "Epoch [2/10], Step [1012/1126], Loss: 0.0865\n",
      "Epoch [2/10], Step [1016/1126], Loss: 0.1140\n",
      "Epoch [2/10], Step [1020/1126], Loss: 0.0987\n",
      "Epoch [2/10], Step [1024/1126], Loss: 0.1277\n",
      "Epoch [2/10], Step [1028/1126], Loss: 0.1401\n",
      "Epoch [2/10], Step [1032/1126], Loss: 0.1085\n",
      "Epoch [2/10], Step [1036/1126], Loss: 0.1451\n",
      "Epoch [2/10], Step [1040/1126], Loss: 0.1811\n",
      "Epoch [2/10], Step [1044/1126], Loss: 0.0658\n",
      "Epoch [2/10], Step [1048/1126], Loss: 0.1012\n",
      "Epoch [2/10], Step [1052/1126], Loss: 0.1333\n",
      "Epoch [2/10], Step [1056/1126], Loss: 0.1077\n",
      "Epoch [2/10], Step [1060/1126], Loss: 0.1009\n",
      "Epoch [2/10], Step [1064/1126], Loss: 0.1805\n",
      "Epoch [2/10], Step [1068/1126], Loss: 0.0888\n",
      "Epoch [2/10], Step [1072/1126], Loss: 0.1435\n",
      "Epoch [2/10], Step [1076/1126], Loss: 0.0864\n",
      "Epoch [2/10], Step [1080/1126], Loss: 0.0901\n",
      "Epoch [2/10], Step [1084/1126], Loss: 0.1206\n",
      "Epoch [2/10], Step [1088/1126], Loss: 0.0865\n",
      "Epoch [2/10], Step [1092/1126], Loss: 0.1305\n",
      "Epoch [2/10], Step [1096/1126], Loss: 0.0683\n",
      "Epoch [2/10], Step [1100/1126], Loss: 0.1257\n",
      "Epoch [2/10], Step [1104/1126], Loss: 0.1345\n",
      "Epoch [2/10], Step [1108/1126], Loss: 0.1501\n",
      "Epoch [2/10], Step [1112/1126], Loss: 0.0796\n",
      "Epoch [2/10], Step [1116/1126], Loss: 0.1259\n",
      "Epoch [2/10], Step [1120/1126], Loss: 0.0819\n",
      "Epoch [2/10], Step [1124/1126], Loss: 0.1425\n",
      "Epoch [3/10], Step [4/1126], Loss: 0.2397\n",
      "Epoch [3/10], Step [8/1126], Loss: 0.0816\n",
      "Epoch [3/10], Step [12/1126], Loss: 0.0370\n",
      "Epoch [3/10], Step [16/1126], Loss: 0.0967\n",
      "Epoch [3/10], Step [20/1126], Loss: 0.1061\n",
      "Epoch [3/10], Step [24/1126], Loss: 0.0666\n",
      "Epoch [3/10], Step [28/1126], Loss: 0.0857\n",
      "Epoch [3/10], Step [32/1126], Loss: 0.0966\n",
      "Epoch [3/10], Step [36/1126], Loss: 0.1073\n",
      "Epoch [3/10], Step [40/1126], Loss: 0.1048\n",
      "Epoch [3/10], Step [44/1126], Loss: 0.1601\n",
      "Epoch [3/10], Step [48/1126], Loss: 0.0950\n",
      "Epoch [3/10], Step [52/1126], Loss: 0.1619\n",
      "Epoch [3/10], Step [56/1126], Loss: 0.1218\n",
      "Epoch [3/10], Step [60/1126], Loss: 0.0925\n",
      "Epoch [3/10], Step [64/1126], Loss: 0.0828\n",
      "Epoch [3/10], Step [68/1126], Loss: 0.1389\n",
      "Epoch [3/10], Step [72/1126], Loss: 0.1072\n",
      "Epoch [3/10], Step [76/1126], Loss: 0.1003\n",
      "Epoch [3/10], Step [80/1126], Loss: 0.1134\n",
      "Epoch [3/10], Step [84/1126], Loss: 0.1130\n",
      "Epoch [3/10], Step [88/1126], Loss: 0.1390\n",
      "Epoch [3/10], Step [92/1126], Loss: 0.0920\n",
      "Epoch [3/10], Step [96/1126], Loss: 0.1571\n",
      "Epoch [3/10], Step [100/1126], Loss: 0.1141\n",
      "Epoch [3/10], Step [104/1126], Loss: 0.0989\n",
      "Epoch [3/10], Step [108/1126], Loss: 0.0637\n",
      "Epoch [3/10], Step [112/1126], Loss: 0.0620\n",
      "Epoch [3/10], Step [116/1126], Loss: 0.1071\n",
      "Epoch [3/10], Step [120/1126], Loss: 0.0882\n",
      "Epoch [3/10], Step [124/1126], Loss: 0.0813\n",
      "Epoch [3/10], Step [128/1126], Loss: 0.1271\n",
      "Epoch [3/10], Step [132/1126], Loss: 0.0468\n",
      "Epoch [3/10], Step [136/1126], Loss: 0.0657\n",
      "Epoch [3/10], Step [140/1126], Loss: 0.0761\n",
      "Epoch [3/10], Step [144/1126], Loss: 0.0615\n",
      "Epoch [3/10], Step [148/1126], Loss: 0.1128\n",
      "Epoch [3/10], Step [152/1126], Loss: 0.1473\n",
      "Epoch [3/10], Step [156/1126], Loss: 0.0705\n",
      "Epoch [3/10], Step [160/1126], Loss: 0.0728\n",
      "Epoch [3/10], Step [164/1126], Loss: 0.0454\n",
      "Epoch [3/10], Step [168/1126], Loss: 0.0892\n",
      "Epoch [3/10], Step [172/1126], Loss: 0.1038\n",
      "Epoch [3/10], Step [176/1126], Loss: 0.0935\n",
      "Epoch [3/10], Step [180/1126], Loss: 0.0600\n",
      "Epoch [3/10], Step [184/1126], Loss: 0.0478\n",
      "Epoch [3/10], Step [188/1126], Loss: 0.0839\n",
      "Epoch [3/10], Step [192/1126], Loss: 0.0881\n",
      "Epoch [3/10], Step [196/1126], Loss: 0.0959\n",
      "Epoch [3/10], Step [200/1126], Loss: 0.1566\n",
      "Epoch [3/10], Step [204/1126], Loss: 0.0752\n",
      "Epoch [3/10], Step [208/1126], Loss: 0.1412\n",
      "Epoch [3/10], Step [212/1126], Loss: 0.0541\n",
      "Epoch [3/10], Step [216/1126], Loss: 0.0915\n",
      "Epoch [3/10], Step [220/1126], Loss: 0.1046\n",
      "Epoch [3/10], Step [224/1126], Loss: 0.1099\n",
      "Epoch [3/10], Step [228/1126], Loss: 0.1019\n",
      "Epoch [3/10], Step [232/1126], Loss: 0.1123\n",
      "Epoch [3/10], Step [236/1126], Loss: 0.0605\n",
      "Epoch [3/10], Step [240/1126], Loss: 0.0699\n",
      "Epoch [3/10], Step [244/1126], Loss: 0.0981\n",
      "Epoch [3/10], Step [248/1126], Loss: 0.0648\n",
      "Epoch [3/10], Step [252/1126], Loss: 0.0694\n",
      "Epoch [3/10], Step [256/1126], Loss: 0.1031\n",
      "Epoch [3/10], Step [260/1126], Loss: 0.1281\n",
      "Epoch [3/10], Step [264/1126], Loss: 0.0546\n",
      "Epoch [3/10], Step [268/1126], Loss: 0.1041\n",
      "Epoch [3/10], Step [272/1126], Loss: 0.0846\n",
      "Epoch [3/10], Step [276/1126], Loss: 0.0562\n",
      "Epoch [3/10], Step [280/1126], Loss: 0.0627\n",
      "Epoch [3/10], Step [284/1126], Loss: 0.1412\n",
      "Epoch [3/10], Step [288/1126], Loss: 0.0899\n",
      "Epoch [3/10], Step [292/1126], Loss: 0.0670\n",
      "Epoch [3/10], Step [296/1126], Loss: 0.0795\n",
      "Epoch [3/10], Step [300/1126], Loss: 0.1036\n",
      "Epoch [3/10], Step [304/1126], Loss: 0.0808\n",
      "Epoch [3/10], Step [308/1126], Loss: 0.1004\n",
      "Epoch [3/10], Step [312/1126], Loss: 0.0723\n",
      "Epoch [3/10], Step [316/1126], Loss: 0.1084\n",
      "Epoch [3/10], Step [320/1126], Loss: 0.1008\n",
      "Epoch [3/10], Step [324/1126], Loss: 0.0865\n",
      "Epoch [3/10], Step [328/1126], Loss: 0.0806\n",
      "Epoch [3/10], Step [332/1126], Loss: 0.1190\n",
      "Epoch [3/10], Step [336/1126], Loss: 0.0749\n",
      "Epoch [3/10], Step [340/1126], Loss: 0.1127\n",
      "Epoch [3/10], Step [344/1126], Loss: 0.1147\n",
      "Epoch [3/10], Step [348/1126], Loss: 0.1203\n",
      "Epoch [3/10], Step [352/1126], Loss: 0.0902\n",
      "Epoch [3/10], Step [356/1126], Loss: 0.0225\n",
      "Epoch [3/10], Step [360/1126], Loss: 0.0494\n",
      "Epoch [3/10], Step [364/1126], Loss: 0.0481\n",
      "Epoch [3/10], Step [368/1126], Loss: 0.1013\n",
      "Epoch [3/10], Step [372/1126], Loss: 0.1445\n",
      "Epoch [3/10], Step [376/1126], Loss: 0.1694\n",
      "Epoch [3/10], Step [380/1126], Loss: 0.0848\n",
      "Epoch [3/10], Step [384/1126], Loss: 0.1345\n",
      "Epoch [3/10], Step [388/1126], Loss: 0.0845\n",
      "Epoch [3/10], Step [392/1126], Loss: 0.0841\n",
      "Epoch [3/10], Step [396/1126], Loss: 0.0459\n",
      "Epoch [3/10], Step [400/1126], Loss: 0.0114\n",
      "Epoch [3/10], Step [404/1126], Loss: 0.0614\n",
      "Epoch [3/10], Step [408/1126], Loss: 0.2125\n",
      "Epoch [3/10], Step [412/1126], Loss: 0.0866\n",
      "Epoch [3/10], Step [416/1126], Loss: 0.0971\n",
      "Epoch [3/10], Step [420/1126], Loss: 0.1279\n",
      "Epoch [3/10], Step [424/1126], Loss: 0.0897\n",
      "Epoch [3/10], Step [428/1126], Loss: 0.1720\n",
      "Epoch [3/10], Step [432/1126], Loss: 0.1357\n",
      "Epoch [3/10], Step [436/1126], Loss: 0.0610\n",
      "Epoch [3/10], Step [440/1126], Loss: 0.1016\n",
      "Epoch [3/10], Step [444/1126], Loss: 0.0509\n",
      "Epoch [3/10], Step [448/1126], Loss: 0.0876\n",
      "Epoch [3/10], Step [452/1126], Loss: 0.0785\n",
      "Epoch [3/10], Step [456/1126], Loss: 0.1226\n",
      "Epoch [3/10], Step [460/1126], Loss: 0.1108\n",
      "Epoch [3/10], Step [464/1126], Loss: 0.0818\n",
      "Epoch [3/10], Step [468/1126], Loss: 0.1201\n",
      "Epoch [3/10], Step [472/1126], Loss: 0.1005\n",
      "Epoch [3/10], Step [476/1126], Loss: 0.0776\n",
      "Epoch [3/10], Step [480/1126], Loss: 0.0670\n",
      "Epoch [3/10], Step [484/1126], Loss: 0.0532\n",
      "Epoch [3/10], Step [488/1126], Loss: 0.0679\n",
      "Epoch [3/10], Step [492/1126], Loss: 0.0418\n",
      "Epoch [3/10], Step [496/1126], Loss: 0.1085\n",
      "Epoch [3/10], Step [500/1126], Loss: 0.1027\n",
      "Epoch [3/10], Step [504/1126], Loss: 0.0845\n",
      "Epoch [3/10], Step [508/1126], Loss: 0.0802\n",
      "Epoch [3/10], Step [512/1126], Loss: 0.0813\n",
      "Epoch [3/10], Step [516/1126], Loss: 0.0887\n",
      "Epoch [3/10], Step [520/1126], Loss: 0.0697\n",
      "Epoch [3/10], Step [524/1126], Loss: 0.0447\n",
      "Epoch [3/10], Step [528/1126], Loss: 0.0584\n",
      "Epoch [3/10], Step [532/1126], Loss: 0.1331\n",
      "Epoch [3/10], Step [536/1126], Loss: 0.0853\n",
      "Epoch [3/10], Step [540/1126], Loss: 0.0815\n",
      "Epoch [3/10], Step [544/1126], Loss: 0.0985\n",
      "Epoch [3/10], Step [548/1126], Loss: 0.0923\n",
      "Epoch [3/10], Step [552/1126], Loss: 0.0548\n",
      "Epoch [3/10], Step [556/1126], Loss: 0.1058\n",
      "Epoch [3/10], Step [560/1126], Loss: 0.1024\n",
      "Epoch [3/10], Step [564/1126], Loss: 0.1465\n",
      "Epoch [3/10], Step [568/1126], Loss: 0.0825\n",
      "Epoch [3/10], Step [572/1126], Loss: 0.0884\n",
      "Epoch [3/10], Step [576/1126], Loss: 0.0711\n",
      "Epoch [3/10], Step [580/1126], Loss: 0.0845\n",
      "Epoch [3/10], Step [584/1126], Loss: 0.0735\n",
      "Epoch [3/10], Step [588/1126], Loss: 0.0865\n",
      "Epoch [3/10], Step [592/1126], Loss: 0.0881\n",
      "Epoch [3/10], Step [596/1126], Loss: 0.0966\n",
      "Epoch [3/10], Step [600/1126], Loss: 0.0867\n",
      "Epoch [3/10], Step [604/1126], Loss: 0.1260\n",
      "Epoch [3/10], Step [608/1126], Loss: 0.1174\n",
      "Epoch [3/10], Step [612/1126], Loss: 0.0835\n",
      "Epoch [3/10], Step [616/1126], Loss: 0.0881\n",
      "Epoch [3/10], Step [620/1126], Loss: 0.0735\n",
      "Epoch [3/10], Step [624/1126], Loss: 0.2272\n",
      "Epoch [3/10], Step [628/1126], Loss: 0.0515\n",
      "Epoch [3/10], Step [632/1126], Loss: 0.4155\n",
      "Epoch [3/10], Step [636/1126], Loss: 0.0377\n",
      "Epoch [3/10], Step [640/1126], Loss: 0.0552\n",
      "Epoch [3/10], Step [644/1126], Loss: 0.0984\n",
      "Epoch [3/10], Step [648/1126], Loss: 0.0924\n",
      "Epoch [3/10], Step [652/1126], Loss: 0.0776\n",
      "Epoch [3/10], Step [656/1126], Loss: 0.0684\n",
      "Epoch [3/10], Step [660/1126], Loss: 0.1369\n",
      "Epoch [3/10], Step [664/1126], Loss: 0.0460\n",
      "Epoch [3/10], Step [668/1126], Loss: 0.1167\n",
      "Epoch [3/10], Step [672/1126], Loss: 0.0548\n",
      "Epoch [3/10], Step [676/1126], Loss: 0.0476\n",
      "Epoch [3/10], Step [680/1126], Loss: 0.0622\n",
      "Epoch [3/10], Step [684/1126], Loss: 0.0921\n",
      "Epoch [3/10], Step [688/1126], Loss: 0.0584\n",
      "Epoch [3/10], Step [692/1126], Loss: 0.0537\n",
      "Epoch [3/10], Step [696/1126], Loss: 0.0808\n",
      "Epoch [3/10], Step [700/1126], Loss: 0.0690\n",
      "Epoch [3/10], Step [704/1126], Loss: 0.0416\n",
      "Epoch [3/10], Step [708/1126], Loss: 0.1249\n",
      "Epoch [3/10], Step [712/1126], Loss: 0.0700\n",
      "Epoch [3/10], Step [716/1126], Loss: 0.1750\n",
      "Epoch [3/10], Step [720/1126], Loss: 0.0934\n",
      "Epoch [3/10], Step [724/1126], Loss: 0.0835\n",
      "Epoch [3/10], Step [728/1126], Loss: 0.1314\n",
      "Epoch [3/10], Step [732/1126], Loss: 0.1713\n",
      "Epoch [3/10], Step [736/1126], Loss: 0.1632\n",
      "Epoch [3/10], Step [740/1126], Loss: 0.1189\n",
      "Epoch [3/10], Step [744/1126], Loss: 0.1366\n",
      "Epoch [3/10], Step [748/1126], Loss: 0.1024\n",
      "Epoch [3/10], Step [752/1126], Loss: 0.1115\n",
      "Epoch [3/10], Step [756/1126], Loss: 0.1178\n",
      "Epoch [3/10], Step [760/1126], Loss: 0.1036\n",
      "Epoch [3/10], Step [764/1126], Loss: 0.3338\n",
      "Epoch [3/10], Step [768/1126], Loss: 0.0632\n",
      "Epoch [3/10], Step [772/1126], Loss: 0.1683\n",
      "Epoch [3/10], Step [776/1126], Loss: 0.0899\n",
      "Epoch [3/10], Step [780/1126], Loss: 0.0806\n",
      "Epoch [3/10], Step [784/1126], Loss: 0.1323\n",
      "Epoch [3/10], Step [788/1126], Loss: 0.1009\n",
      "Epoch [3/10], Step [792/1126], Loss: 0.1269\n",
      "Epoch [3/10], Step [796/1126], Loss: 0.0644\n",
      "Epoch [3/10], Step [800/1126], Loss: 0.0689\n",
      "Epoch [3/10], Step [804/1126], Loss: 0.0452\n",
      "Epoch [3/10], Step [808/1126], Loss: 0.0393\n",
      "Epoch [3/10], Step [812/1126], Loss: 0.1362\n",
      "Epoch [3/10], Step [816/1126], Loss: 0.0734\n",
      "Epoch [3/10], Step [820/1126], Loss: 0.0817\n",
      "Epoch [3/10], Step [824/1126], Loss: 0.1294\n",
      "Epoch [3/10], Step [828/1126], Loss: 0.1010\n",
      "Epoch [3/10], Step [832/1126], Loss: 0.1398\n",
      "Epoch [3/10], Step [836/1126], Loss: 0.1636\n",
      "Epoch [3/10], Step [840/1126], Loss: 0.0492\n",
      "Epoch [3/10], Step [844/1126], Loss: 0.1019\n",
      "Epoch [3/10], Step [848/1126], Loss: 0.1122\n",
      "Epoch [3/10], Step [852/1126], Loss: 0.0967\n",
      "Epoch [3/10], Step [856/1126], Loss: 0.1158\n",
      "Epoch [3/10], Step [860/1126], Loss: 0.0716\n",
      "Epoch [3/10], Step [864/1126], Loss: 0.0654\n",
      "Epoch [3/10], Step [868/1126], Loss: 0.0779\n",
      "Epoch [3/10], Step [872/1126], Loss: 0.0429\n",
      "Epoch [3/10], Step [876/1126], Loss: 0.1071\n",
      "Epoch [3/10], Step [880/1126], Loss: 0.0763\n",
      "Epoch [3/10], Step [884/1126], Loss: 0.0589\n",
      "Epoch [3/10], Step [888/1126], Loss: 0.0636\n",
      "Epoch [3/10], Step [892/1126], Loss: 0.0867\n",
      "Epoch [3/10], Step [896/1126], Loss: 0.1335\n",
      "Epoch [3/10], Step [900/1126], Loss: 0.0486\n",
      "Epoch [3/10], Step [904/1126], Loss: 0.0646\n",
      "Epoch [3/10], Step [908/1126], Loss: 0.0629\n",
      "Epoch [3/10], Step [912/1126], Loss: 0.0502\n",
      "Epoch [3/10], Step [916/1126], Loss: 0.0747\n",
      "Epoch [3/10], Step [920/1126], Loss: 0.1349\n",
      "Epoch [3/10], Step [924/1126], Loss: 0.0665\n",
      "Epoch [3/10], Step [928/1126], Loss: 0.0547\n",
      "Epoch [3/10], Step [932/1126], Loss: 0.0493\n",
      "Epoch [3/10], Step [936/1126], Loss: 0.0992\n",
      "Epoch [3/10], Step [940/1126], Loss: 0.0561\n",
      "Epoch [3/10], Step [944/1126], Loss: 0.0564\n",
      "Epoch [3/10], Step [948/1126], Loss: 0.1099\n",
      "Epoch [3/10], Step [952/1126], Loss: 0.1625\n",
      "Epoch [3/10], Step [956/1126], Loss: 0.1178\n",
      "Epoch [3/10], Step [960/1126], Loss: 0.1002\n",
      "Epoch [3/10], Step [964/1126], Loss: 0.0635\n",
      "Epoch [3/10], Step [968/1126], Loss: 0.1214\n",
      "Epoch [3/10], Step [972/1126], Loss: 0.0107\n",
      "Epoch [3/10], Step [976/1126], Loss: 0.0775\n",
      "Epoch [3/10], Step [980/1126], Loss: 0.0381\n",
      "Epoch [3/10], Step [984/1126], Loss: 0.0643\n",
      "Epoch [3/10], Step [988/1126], Loss: 0.1130\n",
      "Epoch [3/10], Step [992/1126], Loss: 0.0695\n",
      "Epoch [3/10], Step [996/1126], Loss: 0.0705\n",
      "Epoch [3/10], Step [1000/1126], Loss: 0.0977\n",
      "Epoch [3/10], Step [1004/1126], Loss: 0.1154\n",
      "Epoch [3/10], Step [1008/1126], Loss: 0.0893\n",
      "Epoch [3/10], Step [1012/1126], Loss: 0.1249\n",
      "Epoch [3/10], Step [1016/1126], Loss: 0.0975\n",
      "Epoch [3/10], Step [1020/1126], Loss: 0.0957\n",
      "Epoch [3/10], Step [1024/1126], Loss: 0.1104\n",
      "Epoch [3/10], Step [1028/1126], Loss: 0.0969\n",
      "Epoch [3/10], Step [1032/1126], Loss: 0.0822\n",
      "Epoch [3/10], Step [1036/1126], Loss: 0.1886\n",
      "Epoch [3/10], Step [1040/1126], Loss: 0.1799\n",
      "Epoch [3/10], Step [1044/1126], Loss: 0.0416\n",
      "Epoch [3/10], Step [1048/1126], Loss: 0.1109\n",
      "Epoch [3/10], Step [1052/1126], Loss: 0.1032\n",
      "Epoch [3/10], Step [1056/1126], Loss: 0.0829\n",
      "Epoch [3/10], Step [1060/1126], Loss: 0.0642\n",
      "Epoch [3/10], Step [1064/1126], Loss: 0.1515\n",
      "Epoch [3/10], Step [1068/1126], Loss: 0.1078\n",
      "Epoch [3/10], Step [1072/1126], Loss: 0.1215\n",
      "Epoch [3/10], Step [1076/1126], Loss: 0.1222\n",
      "Epoch [3/10], Step [1080/1126], Loss: 0.0753\n",
      "Epoch [3/10], Step [1084/1126], Loss: 0.1736\n",
      "Epoch [3/10], Step [1088/1126], Loss: 0.0781\n",
      "Epoch [3/10], Step [1092/1126], Loss: 0.0888\n",
      "Epoch [3/10], Step [1096/1126], Loss: 0.0486\n",
      "Epoch [3/10], Step [1100/1126], Loss: 0.1342\n",
      "Epoch [3/10], Step [1104/1126], Loss: 0.1342\n",
      "Epoch [3/10], Step [1108/1126], Loss: 0.0726\n",
      "Epoch [3/10], Step [1112/1126], Loss: 0.0822\n",
      "Epoch [3/10], Step [1116/1126], Loss: 0.0770\n",
      "Epoch [3/10], Step [1120/1126], Loss: 0.0678\n",
      "Epoch [3/10], Step [1124/1126], Loss: 0.0907\n",
      "Epoch [4/10], Step [4/1126], Loss: 0.1621\n",
      "Epoch [4/10], Step [8/1126], Loss: 0.0871\n",
      "Epoch [4/10], Step [12/1126], Loss: 0.0554\n",
      "Epoch [4/10], Step [16/1126], Loss: 0.0518\n",
      "Epoch [4/10], Step [20/1126], Loss: 0.0600\n",
      "Epoch [4/10], Step [24/1126], Loss: 0.0585\n",
      "Epoch [4/10], Step [28/1126], Loss: 0.0401\n",
      "Epoch [4/10], Step [32/1126], Loss: 0.1040\n",
      "Epoch [4/10], Step [36/1126], Loss: 0.1043\n",
      "Epoch [4/10], Step [40/1126], Loss: 0.0776\n",
      "Epoch [4/10], Step [44/1126], Loss: 0.0832\n",
      "Epoch [4/10], Step [48/1126], Loss: 0.1117\n",
      "Epoch [4/10], Step [52/1126], Loss: 0.0923\n",
      "Epoch [4/10], Step [56/1126], Loss: 0.1056\n",
      "Epoch [4/10], Step [60/1126], Loss: 0.0646\n",
      "Epoch [4/10], Step [64/1126], Loss: 0.0677\n",
      "Epoch [4/10], Step [68/1126], Loss: 0.1082\n",
      "Epoch [4/10], Step [72/1126], Loss: 0.0654\n",
      "Epoch [4/10], Step [76/1126], Loss: 0.0730\n",
      "Epoch [4/10], Step [80/1126], Loss: 0.1172\n",
      "Epoch [4/10], Step [84/1126], Loss: 0.1733\n",
      "Epoch [4/10], Step [88/1126], Loss: 0.0790\n",
      "Epoch [4/10], Step [92/1126], Loss: 0.0794\n",
      "Epoch [4/10], Step [96/1126], Loss: 0.0858\n",
      "Epoch [4/10], Step [100/1126], Loss: 0.1088\n",
      "Epoch [4/10], Step [104/1126], Loss: 0.0493\n",
      "Epoch [4/10], Step [108/1126], Loss: 0.0386\n",
      "Epoch [4/10], Step [112/1126], Loss: 0.0489\n",
      "Epoch [4/10], Step [116/1126], Loss: 0.0826\n",
      "Epoch [4/10], Step [120/1126], Loss: 0.0636\n",
      "Epoch [4/10], Step [124/1126], Loss: 0.0323\n",
      "Epoch [4/10], Step [128/1126], Loss: 0.1194\n",
      "Epoch [4/10], Step [132/1126], Loss: 0.1054\n",
      "Epoch [4/10], Step [136/1126], Loss: 0.0534\n",
      "Epoch [4/10], Step [140/1126], Loss: 0.0464\n",
      "Epoch [4/10], Step [144/1126], Loss: 0.0676\n",
      "Epoch [4/10], Step [148/1126], Loss: 0.0873\n",
      "Epoch [4/10], Step [152/1126], Loss: 0.1181\n",
      "Epoch [4/10], Step [156/1126], Loss: 0.0840\n",
      "Epoch [4/10], Step [160/1126], Loss: 0.0565\n",
      "Epoch [4/10], Step [164/1126], Loss: 0.0346\n",
      "Epoch [4/10], Step [168/1126], Loss: 0.2275\n",
      "Epoch [4/10], Step [172/1126], Loss: 0.1574\n",
      "Epoch [4/10], Step [176/1126], Loss: 0.0808\n",
      "Epoch [4/10], Step [180/1126], Loss: 0.0517\n",
      "Epoch [4/10], Step [184/1126], Loss: 0.0393\n",
      "Epoch [4/10], Step [188/1126], Loss: 0.0634\n",
      "Epoch [4/10], Step [192/1126], Loss: 0.0279\n",
      "Epoch [4/10], Step [196/1126], Loss: 0.0294\n",
      "Epoch [4/10], Step [200/1126], Loss: 0.0345\n",
      "Epoch [4/10], Step [204/1126], Loss: 0.0599\n",
      "Epoch [4/10], Step [208/1126], Loss: 0.0634\n",
      "Epoch [4/10], Step [212/1126], Loss: 0.1408\n",
      "Epoch [4/10], Step [216/1126], Loss: 0.0531\n",
      "Epoch [4/10], Step [220/1126], Loss: 0.1063\n",
      "Epoch [4/10], Step [224/1126], Loss: 0.1303\n",
      "Epoch [4/10], Step [228/1126], Loss: 0.0642\n",
      "Epoch [4/10], Step [232/1126], Loss: 0.0889\n",
      "Epoch [4/10], Step [236/1126], Loss: 0.0616\n",
      "Epoch [4/10], Step [240/1126], Loss: 0.0845\n",
      "Epoch [4/10], Step [244/1126], Loss: 0.0734\n",
      "Epoch [4/10], Step [248/1126], Loss: 0.0767\n",
      "Epoch [4/10], Step [252/1126], Loss: 0.0413\n",
      "Epoch [4/10], Step [256/1126], Loss: 0.0581\n",
      "Epoch [4/10], Step [260/1126], Loss: 0.0974\n",
      "Epoch [4/10], Step [264/1126], Loss: 0.1526\n",
      "Epoch [4/10], Step [268/1126], Loss: 0.1296\n",
      "Epoch [4/10], Step [272/1126], Loss: 0.0224\n",
      "Epoch [4/10], Step [276/1126], Loss: 0.0386\n",
      "Epoch [4/10], Step [280/1126], Loss: 0.0351\n",
      "Epoch [4/10], Step [284/1126], Loss: 0.0881\n",
      "Epoch [4/10], Step [288/1126], Loss: 0.0753\n",
      "Epoch [4/10], Step [292/1126], Loss: 0.0442\n",
      "Epoch [4/10], Step [296/1126], Loss: 0.0536\n",
      "Epoch [4/10], Step [300/1126], Loss: 0.0619\n",
      "Epoch [4/10], Step [304/1126], Loss: 0.0584\n",
      "Epoch [4/10], Step [308/1126], Loss: 0.0773\n",
      "Epoch [4/10], Step [312/1126], Loss: 0.0858\n",
      "Epoch [4/10], Step [316/1126], Loss: 0.0440\n",
      "Epoch [4/10], Step [320/1126], Loss: 0.0596\n",
      "Epoch [4/10], Step [324/1126], Loss: 0.0843\n",
      "Epoch [4/10], Step [328/1126], Loss: 0.0518\n",
      "Epoch [4/10], Step [332/1126], Loss: 0.0768\n",
      "Epoch [4/10], Step [336/1126], Loss: 0.0393\n",
      "Epoch [4/10], Step [340/1126], Loss: 0.0759\n",
      "Epoch [4/10], Step [344/1126], Loss: 0.0630\n",
      "Epoch [4/10], Step [348/1126], Loss: 0.0749\n",
      "Epoch [4/10], Step [352/1126], Loss: 0.0963\n",
      "Epoch [4/10], Step [356/1126], Loss: 0.0652\n",
      "Epoch [4/10], Step [360/1126], Loss: 0.0937\n",
      "Epoch [4/10], Step [364/1126], Loss: 0.0363\n",
      "Epoch [4/10], Step [368/1126], Loss: 0.0840\n",
      "Epoch [4/10], Step [372/1126], Loss: 0.0802\n",
      "Epoch [4/10], Step [376/1126], Loss: 0.1408\n",
      "Epoch [4/10], Step [380/1126], Loss: 0.0605\n",
      "Epoch [4/10], Step [384/1126], Loss: 0.0794\n",
      "Epoch [4/10], Step [388/1126], Loss: 0.0884\n",
      "Epoch [4/10], Step [392/1126], Loss: 0.0731\n",
      "Epoch [4/10], Step [396/1126], Loss: 0.0331\n",
      "Epoch [4/10], Step [400/1126], Loss: 0.0335\n",
      "Epoch [4/10], Step [404/1126], Loss: 0.0562\n",
      "Epoch [4/10], Step [408/1126], Loss: 0.0945\n",
      "Epoch [4/10], Step [412/1126], Loss: 0.0656\n",
      "Epoch [4/10], Step [416/1126], Loss: 0.0744\n",
      "Epoch [4/10], Step [420/1126], Loss: 0.0856\n",
      "Epoch [4/10], Step [424/1126], Loss: 0.0922\n",
      "Epoch [4/10], Step [428/1126], Loss: 0.1222\n",
      "Epoch [4/10], Step [432/1126], Loss: 0.0410\n",
      "Epoch [4/10], Step [436/1126], Loss: 0.1512\n",
      "Epoch [4/10], Step [440/1126], Loss: 0.0655\n",
      "Epoch [4/10], Step [444/1126], Loss: 0.0969\n",
      "Epoch [4/10], Step [448/1126], Loss: 0.0548\n",
      "Epoch [4/10], Step [452/1126], Loss: 0.0496\n",
      "Epoch [4/10], Step [456/1126], Loss: 0.0346\n",
      "Epoch [4/10], Step [460/1126], Loss: 0.0977\n",
      "Epoch [4/10], Step [464/1126], Loss: 0.0658\n",
      "Epoch [4/10], Step [468/1126], Loss: 0.0759\n",
      "Epoch [4/10], Step [472/1126], Loss: 0.0441\n",
      "Epoch [4/10], Step [476/1126], Loss: 0.0617\n",
      "Epoch [4/10], Step [480/1126], Loss: 0.0651\n",
      "Epoch [4/10], Step [484/1126], Loss: 0.1034\n",
      "Epoch [4/10], Step [488/1126], Loss: 0.0495\n",
      "Epoch [4/10], Step [492/1126], Loss: 0.0371\n",
      "Epoch [4/10], Step [496/1126], Loss: 0.1341\n",
      "Epoch [4/10], Step [500/1126], Loss: 0.0435\n",
      "Epoch [4/10], Step [504/1126], Loss: 0.0887\n",
      "Epoch [4/10], Step [508/1126], Loss: 0.0437\n",
      "Epoch [4/10], Step [512/1126], Loss: 0.1135\n",
      "Epoch [4/10], Step [516/1126], Loss: 0.1114\n",
      "Epoch [4/10], Step [520/1126], Loss: 0.0627\n",
      "Epoch [4/10], Step [524/1126], Loss: 0.0945\n",
      "Epoch [4/10], Step [528/1126], Loss: 0.0461\n",
      "Epoch [4/10], Step [532/1126], Loss: 0.0953\n",
      "Epoch [4/10], Step [536/1126], Loss: 0.0574\n",
      "Epoch [4/10], Step [540/1126], Loss: 0.0689\n",
      "Epoch [4/10], Step [544/1126], Loss: 0.0691\n",
      "Epoch [4/10], Step [548/1126], Loss: 0.0735\n",
      "Epoch [4/10], Step [552/1126], Loss: 0.0665\n",
      "Epoch [4/10], Step [556/1126], Loss: 0.0727\n",
      "Epoch [4/10], Step [560/1126], Loss: 0.0503\n",
      "Epoch [4/10], Step [564/1126], Loss: 0.1067\n",
      "Epoch [4/10], Step [568/1126], Loss: 0.0671\n",
      "Epoch [4/10], Step [572/1126], Loss: 0.0615\n",
      "Epoch [4/10], Step [576/1126], Loss: 0.0519\n",
      "Epoch [4/10], Step [580/1126], Loss: 0.0489\n",
      "Epoch [4/10], Step [584/1126], Loss: 0.0666\n",
      "Epoch [4/10], Step [588/1126], Loss: 0.1839\n",
      "Epoch [4/10], Step [592/1126], Loss: 0.0890\n",
      "Epoch [4/10], Step [596/1126], Loss: 0.0925\n",
      "Epoch [4/10], Step [600/1126], Loss: 0.0801\n",
      "Epoch [4/10], Step [604/1126], Loss: 0.0821\n",
      "Epoch [4/10], Step [608/1126], Loss: 0.0933\n",
      "Epoch [4/10], Step [612/1126], Loss: 0.0701\n",
      "Epoch [4/10], Step [616/1126], Loss: 0.0879\n",
      "Epoch [4/10], Step [620/1126], Loss: 0.0740\n",
      "Epoch [4/10], Step [624/1126], Loss: 0.0872\n",
      "Epoch [4/10], Step [628/1126], Loss: 0.0552\n",
      "Epoch [4/10], Step [632/1126], Loss: 0.0693\n",
      "Epoch [4/10], Step [636/1126], Loss: 0.0451\n",
      "Epoch [4/10], Step [640/1126], Loss: 0.0655\n",
      "Epoch [4/10], Step [644/1126], Loss: 0.0690\n",
      "Epoch [4/10], Step [648/1126], Loss: 0.0771\n",
      "Epoch [4/10], Step [652/1126], Loss: 0.0439\n",
      "Epoch [4/10], Step [656/1126], Loss: 0.0638\n",
      "Epoch [4/10], Step [660/1126], Loss: 0.1020\n",
      "Epoch [4/10], Step [664/1126], Loss: 0.0501\n",
      "Epoch [4/10], Step [668/1126], Loss: 0.1229\n",
      "Epoch [4/10], Step [672/1126], Loss: 0.1302\n",
      "Epoch [4/10], Step [676/1126], Loss: 0.0528\n",
      "Epoch [4/10], Step [680/1126], Loss: 0.0442\n",
      "Epoch [4/10], Step [684/1126], Loss: 0.0742\n",
      "Epoch [4/10], Step [688/1126], Loss: 0.0357\n",
      "Epoch [4/10], Step [692/1126], Loss: 0.0362\n",
      "Epoch [4/10], Step [696/1126], Loss: 0.0709\n",
      "Epoch [4/10], Step [700/1126], Loss: 0.1217\n",
      "Epoch [4/10], Step [704/1126], Loss: 0.0334\n",
      "Epoch [4/10], Step [708/1126], Loss: 0.0822\n",
      "Epoch [4/10], Step [712/1126], Loss: 0.0252\n",
      "Epoch [4/10], Step [716/1126], Loss: 0.0910\n",
      "Epoch [4/10], Step [720/1126], Loss: 0.0624\n",
      "Epoch [4/10], Step [724/1126], Loss: 0.0993\n",
      "Epoch [4/10], Step [728/1126], Loss: 0.0909\n",
      "Epoch [4/10], Step [732/1126], Loss: 0.1339\n",
      "Epoch [4/10], Step [736/1126], Loss: 0.0935\n",
      "Epoch [4/10], Step [740/1126], Loss: 0.0977\n",
      "Epoch [4/10], Step [744/1126], Loss: 0.1078\n",
      "Epoch [4/10], Step [748/1126], Loss: 0.0486\n",
      "Epoch [4/10], Step [752/1126], Loss: 0.0934\n",
      "Epoch [4/10], Step [756/1126], Loss: 0.0857\n",
      "Epoch [4/10], Step [760/1126], Loss: 0.0898\n",
      "Epoch [4/10], Step [764/1126], Loss: 0.0832\n",
      "Epoch [4/10], Step [768/1126], Loss: 0.1052\n",
      "Epoch [4/10], Step [772/1126], Loss: 0.0623\n",
      "Epoch [4/10], Step [776/1126], Loss: 0.0664\n",
      "Epoch [4/10], Step [780/1126], Loss: 0.0718\n",
      "Epoch [4/10], Step [784/1126], Loss: 0.0686\n",
      "Epoch [4/10], Step [788/1126], Loss: 0.0770\n",
      "Epoch [4/10], Step [792/1126], Loss: 0.0588\n",
      "Epoch [4/10], Step [796/1126], Loss: 0.0370\n",
      "Epoch [4/10], Step [800/1126], Loss: 0.0584\n",
      "Epoch [4/10], Step [804/1126], Loss: 0.0357\n",
      "Epoch [4/10], Step [808/1126], Loss: 0.0397\n",
      "Epoch [4/10], Step [812/1126], Loss: 0.0615\n",
      "Epoch [4/10], Step [816/1126], Loss: 0.0502\n",
      "Epoch [4/10], Step [820/1126], Loss: 0.0428\n",
      "Epoch [4/10], Step [824/1126], Loss: 0.0735\n",
      "Epoch [4/10], Step [828/1126], Loss: 0.0456\n",
      "Epoch [4/10], Step [832/1126], Loss: 0.1241\n",
      "Epoch [4/10], Step [836/1126], Loss: 0.0809\n",
      "Epoch [4/10], Step [840/1126], Loss: 0.0504\n",
      "Epoch [4/10], Step [844/1126], Loss: 0.1180\n",
      "Epoch [4/10], Step [848/1126], Loss: 0.0931\n",
      "Epoch [4/10], Step [852/1126], Loss: 0.0572\n",
      "Epoch [4/10], Step [856/1126], Loss: 0.0940\n",
      "Epoch [4/10], Step [860/1126], Loss: 0.0347\n",
      "Epoch [4/10], Step [864/1126], Loss: 0.0489\n",
      "Epoch [4/10], Step [868/1126], Loss: 0.0668\n",
      "Epoch [4/10], Step [872/1126], Loss: 0.0381\n",
      "Epoch [4/10], Step [876/1126], Loss: 0.0644\n",
      "Epoch [4/10], Step [880/1126], Loss: 0.0585\n",
      "Epoch [4/10], Step [884/1126], Loss: 0.0661\n",
      "Epoch [4/10], Step [888/1126], Loss: 0.0648\n",
      "Epoch [4/10], Step [892/1126], Loss: 0.1093\n",
      "Epoch [4/10], Step [896/1126], Loss: 0.0820\n",
      "Epoch [4/10], Step [900/1126], Loss: 0.0466\n",
      "Epoch [4/10], Step [904/1126], Loss: 0.0427\n",
      "Epoch [4/10], Step [908/1126], Loss: 0.0601\n",
      "Epoch [4/10], Step [912/1126], Loss: 0.0365\n",
      "Epoch [4/10], Step [916/1126], Loss: 0.0992\n",
      "Epoch [4/10], Step [920/1126], Loss: 0.0317\n",
      "Epoch [4/10], Step [924/1126], Loss: 0.0937\n",
      "Epoch [4/10], Step [928/1126], Loss: 0.0304\n",
      "Epoch [4/10], Step [932/1126], Loss: 0.0343\n",
      "Epoch [4/10], Step [936/1126], Loss: 0.0316\n",
      "Epoch [4/10], Step [940/1126], Loss: 0.0643\n",
      "Epoch [4/10], Step [944/1126], Loss: 0.0351\n",
      "Epoch [4/10], Step [948/1126], Loss: 0.0740\n",
      "Epoch [4/10], Step [952/1126], Loss: 0.1145\n",
      "Epoch [4/10], Step [956/1126], Loss: 0.0699\n",
      "Epoch [4/10], Step [960/1126], Loss: 0.0601\n",
      "Epoch [4/10], Step [964/1126], Loss: 0.0668\n",
      "Epoch [4/10], Step [968/1126], Loss: 0.0601\n",
      "Epoch [4/10], Step [972/1126], Loss: 0.0234\n",
      "Epoch [4/10], Step [976/1126], Loss: 0.0491\n",
      "Epoch [4/10], Step [980/1126], Loss: 0.0844\n",
      "Epoch [4/10], Step [984/1126], Loss: 0.0409\n",
      "Epoch [4/10], Step [988/1126], Loss: 0.0699\n",
      "Epoch [4/10], Step [992/1126], Loss: 0.0761\n",
      "Epoch [4/10], Step [996/1126], Loss: 0.0536\n",
      "Epoch [4/10], Step [1000/1126], Loss: 0.0744\n",
      "Epoch [4/10], Step [1004/1126], Loss: 0.0885\n",
      "Epoch [4/10], Step [1008/1126], Loss: 0.0568\n",
      "Epoch [4/10], Step [1012/1126], Loss: 0.0574\n",
      "Epoch [4/10], Step [1016/1126], Loss: 0.0632\n",
      "Epoch [4/10], Step [1020/1126], Loss: 0.0860\n",
      "Epoch [4/10], Step [1024/1126], Loss: 0.1345\n",
      "Epoch [4/10], Step [1028/1126], Loss: 0.0574\n",
      "Epoch [4/10], Step [1032/1126], Loss: 0.0615\n",
      "Epoch [4/10], Step [1036/1126], Loss: 0.0970\n",
      "Epoch [4/10], Step [1040/1126], Loss: 0.1064\n",
      "Epoch [4/10], Step [1044/1126], Loss: 0.0351\n",
      "Epoch [4/10], Step [1048/1126], Loss: 0.0794\n",
      "Epoch [4/10], Step [1052/1126], Loss: 0.0846\n",
      "Epoch [4/10], Step [1056/1126], Loss: 0.0901\n",
      "Epoch [4/10], Step [1060/1126], Loss: 0.0714\n",
      "Epoch [4/10], Step [1064/1126], Loss: 0.1201\n",
      "Epoch [4/10], Step [1068/1126], Loss: 0.0725\n",
      "Epoch [4/10], Step [1072/1126], Loss: 0.2010\n",
      "Epoch [4/10], Step [1076/1126], Loss: 0.0478\n",
      "Epoch [4/10], Step [1080/1126], Loss: 0.0759\n",
      "Epoch [4/10], Step [1084/1126], Loss: 0.0591\n",
      "Epoch [4/10], Step [1088/1126], Loss: 0.0622\n",
      "Epoch [4/10], Step [1092/1126], Loss: 0.0597\n",
      "Epoch [4/10], Step [1096/1126], Loss: 0.0361\n",
      "Epoch [4/10], Step [1100/1126], Loss: 0.0847\n",
      "Epoch [4/10], Step [1104/1126], Loss: 0.1038\n",
      "Epoch [4/10], Step [1108/1126], Loss: 0.1350\n",
      "Epoch [4/10], Step [1112/1126], Loss: 0.0851\n",
      "Epoch [4/10], Step [1116/1126], Loss: 0.0576\n",
      "Epoch [4/10], Step [1120/1126], Loss: 0.0624\n",
      "Epoch [4/10], Step [1124/1126], Loss: 0.1018\n",
      "Epoch [5/10], Step [4/1126], Loss: 0.1440\n",
      "Epoch [5/10], Step [8/1126], Loss: 0.0774\n",
      "Epoch [5/10], Step [12/1126], Loss: 0.0752\n",
      "Epoch [5/10], Step [16/1126], Loss: 0.0748\n",
      "Epoch [5/10], Step [20/1126], Loss: 0.0604\n",
      "Epoch [5/10], Step [24/1126], Loss: 0.0240\n",
      "Epoch [5/10], Step [28/1126], Loss: 0.0400\n",
      "Epoch [5/10], Step [32/1126], Loss: 0.0628\n",
      "Epoch [5/10], Step [36/1126], Loss: 0.0745\n",
      "Epoch [5/10], Step [40/1126], Loss: 0.0853\n",
      "Epoch [5/10], Step [44/1126], Loss: 0.0413\n",
      "Epoch [5/10], Step [48/1126], Loss: 0.0933\n",
      "Epoch [5/10], Step [52/1126], Loss: 0.0837\n",
      "Epoch [5/10], Step [56/1126], Loss: 0.0866\n",
      "Epoch [5/10], Step [60/1126], Loss: 0.0743\n",
      "Epoch [5/10], Step [64/1126], Loss: 0.0856\n",
      "Epoch [5/10], Step [68/1126], Loss: 0.0687\n",
      "Epoch [5/10], Step [72/1126], Loss: 0.0630\n",
      "Epoch [5/10], Step [76/1126], Loss: 0.1255\n",
      "Epoch [5/10], Step [80/1126], Loss: 0.1303\n",
      "Epoch [5/10], Step [84/1126], Loss: 0.1731\n",
      "Epoch [5/10], Step [88/1126], Loss: 0.0627\n",
      "Epoch [5/10], Step [92/1126], Loss: 0.0740\n",
      "Epoch [5/10], Step [96/1126], Loss: 0.0681\n",
      "Epoch [5/10], Step [100/1126], Loss: 0.0693\n",
      "Epoch [5/10], Step [104/1126], Loss: 0.0413\n",
      "Epoch [5/10], Step [108/1126], Loss: 0.0233\n",
      "Epoch [5/10], Step [112/1126], Loss: 0.0417\n",
      "Epoch [5/10], Step [116/1126], Loss: 0.0447\n",
      "Epoch [5/10], Step [120/1126], Loss: 0.1361\n",
      "Epoch [5/10], Step [124/1126], Loss: 0.0677\n",
      "Epoch [5/10], Step [128/1126], Loss: 0.0963\n",
      "Epoch [5/10], Step [132/1126], Loss: 0.0374\n",
      "Epoch [5/10], Step [136/1126], Loss: 0.0408\n",
      "Epoch [5/10], Step [140/1126], Loss: 0.0611\n",
      "Epoch [5/10], Step [144/1126], Loss: 0.0641\n",
      "Epoch [5/10], Step [148/1126], Loss: 0.1243\n",
      "Epoch [5/10], Step [152/1126], Loss: 0.1246\n",
      "Epoch [5/10], Step [156/1126], Loss: 0.0751\n",
      "Epoch [5/10], Step [160/1126], Loss: 0.0575\n",
      "Epoch [5/10], Step [164/1126], Loss: 0.0626\n",
      "Epoch [5/10], Step [168/1126], Loss: 0.1067\n",
      "Epoch [5/10], Step [172/1126], Loss: 0.0430\n",
      "Epoch [5/10], Step [176/1126], Loss: 0.0446\n",
      "Epoch [5/10], Step [180/1126], Loss: 0.0343\n",
      "Epoch [5/10], Step [184/1126], Loss: 0.0336\n",
      "Epoch [5/10], Step [188/1126], Loss: 0.0606\n",
      "Epoch [5/10], Step [192/1126], Loss: 0.0635\n",
      "Epoch [5/10], Step [196/1126], Loss: 0.0290\n",
      "Epoch [5/10], Step [200/1126], Loss: 0.0591\n",
      "Epoch [5/10], Step [204/1126], Loss: 0.0350\n",
      "Epoch [5/10], Step [208/1126], Loss: 0.0770\n",
      "Epoch [5/10], Step [212/1126], Loss: 0.0849\n",
      "Epoch [5/10], Step [216/1126], Loss: 0.0814\n",
      "Epoch [5/10], Step [220/1126], Loss: 0.0912\n",
      "Epoch [5/10], Step [224/1126], Loss: 0.0864\n",
      "Epoch [5/10], Step [228/1126], Loss: 0.0716\n",
      "Epoch [5/10], Step [232/1126], Loss: 0.0729\n",
      "Epoch [5/10], Step [236/1126], Loss: 0.0557\n",
      "Epoch [5/10], Step [240/1126], Loss: 0.0858\n",
      "Epoch [5/10], Step [244/1126], Loss: 0.0539\n",
      "Epoch [5/10], Step [248/1126], Loss: 0.1717\n",
      "Epoch [5/10], Step [252/1126], Loss: 0.0456\n",
      "Epoch [5/10], Step [256/1126], Loss: 0.0385\n",
      "Epoch [5/10], Step [260/1126], Loss: 0.0720\n",
      "Epoch [5/10], Step [264/1126], Loss: 0.0292\n",
      "Epoch [5/10], Step [268/1126], Loss: 0.0919\n",
      "Epoch [5/10], Step [272/1126], Loss: 0.0296\n",
      "Epoch [5/10], Step [276/1126], Loss: 0.0345\n",
      "Epoch [5/10], Step [280/1126], Loss: 0.0784\n",
      "Epoch [5/10], Step [284/1126], Loss: 0.0724\n",
      "Epoch [5/10], Step [288/1126], Loss: 0.0894\n",
      "Epoch [5/10], Step [292/1126], Loss: 0.0448\n",
      "Epoch [5/10], Step [296/1126], Loss: 0.0946\n",
      "Epoch [5/10], Step [300/1126], Loss: 0.0708\n",
      "Epoch [5/10], Step [304/1126], Loss: 0.0296\n",
      "Epoch [5/10], Step [308/1126], Loss: 0.0753\n",
      "Epoch [5/10], Step [312/1126], Loss: 0.0614\n",
      "Epoch [5/10], Step [316/1126], Loss: 0.0492\n",
      "Epoch [5/10], Step [320/1126], Loss: 0.0783\n",
      "Epoch [5/10], Step [324/1126], Loss: 0.0425\n",
      "Epoch [5/10], Step [328/1126], Loss: 0.0675\n",
      "Epoch [5/10], Step [332/1126], Loss: 0.0871\n",
      "Epoch [5/10], Step [336/1126], Loss: 0.0284\n",
      "Epoch [5/10], Step [340/1126], Loss: 0.1101\n",
      "Epoch [5/10], Step [344/1126], Loss: 0.0485\n",
      "Epoch [5/10], Step [348/1126], Loss: 0.0686\n",
      "Epoch [5/10], Step [352/1126], Loss: 0.0719\n",
      "Epoch [5/10], Step [356/1126], Loss: 0.0483\n",
      "Epoch [5/10], Step [360/1126], Loss: 0.0381\n",
      "Epoch [5/10], Step [364/1126], Loss: 0.0179\n",
      "Epoch [5/10], Step [368/1126], Loss: 0.0988\n",
      "Epoch [5/10], Step [372/1126], Loss: 0.0508\n",
      "Epoch [5/10], Step [376/1126], Loss: 0.1221\n",
      "Epoch [5/10], Step [380/1126], Loss: 0.0545\n",
      "Epoch [5/10], Step [384/1126], Loss: 0.0965\n",
      "Epoch [5/10], Step [388/1126], Loss: 0.0426\n",
      "Epoch [5/10], Step [392/1126], Loss: 0.0599\n",
      "Epoch [5/10], Step [396/1126], Loss: 0.0388\n",
      "Epoch [5/10], Step [400/1126], Loss: 0.0121\n",
      "Epoch [5/10], Step [404/1126], Loss: 0.0374\n",
      "Epoch [5/10], Step [408/1126], Loss: 0.0976\n",
      "Epoch [5/10], Step [412/1126], Loss: 0.0624\n",
      "Epoch [5/10], Step [416/1126], Loss: 0.0536\n",
      "Epoch [5/10], Step [420/1126], Loss: 0.1018\n",
      "Epoch [5/10], Step [424/1126], Loss: 0.0446\n",
      "Epoch [5/10], Step [428/1126], Loss: 0.1299\n",
      "Epoch [5/10], Step [432/1126], Loss: 0.0777\n",
      "Epoch [5/10], Step [436/1126], Loss: 0.0821\n",
      "Epoch [5/10], Step [440/1126], Loss: 0.0734\n",
      "Epoch [5/10], Step [444/1126], Loss: 0.0385\n",
      "Epoch [5/10], Step [448/1126], Loss: 0.0533\n",
      "Epoch [5/10], Step [452/1126], Loss: 0.0682\n",
      "Epoch [5/10], Step [456/1126], Loss: 0.0377\n",
      "Epoch [5/10], Step [460/1126], Loss: 0.0460\n",
      "Epoch [5/10], Step [464/1126], Loss: 0.0582\n",
      "Epoch [5/10], Step [468/1126], Loss: 0.0378\n",
      "Epoch [5/10], Step [472/1126], Loss: 0.0685\n",
      "Epoch [5/10], Step [476/1126], Loss: 0.0437\n",
      "Epoch [5/10], Step [480/1126], Loss: 0.0453\n",
      "Epoch [5/10], Step [484/1126], Loss: 0.0718\n",
      "Epoch [5/10], Step [488/1126], Loss: 0.0496\n",
      "Epoch [5/10], Step [492/1126], Loss: 0.0139\n",
      "Epoch [5/10], Step [496/1126], Loss: 0.0741\n",
      "Epoch [5/10], Step [500/1126], Loss: 0.0462\n",
      "Epoch [5/10], Step [504/1126], Loss: 0.0662\n",
      "Epoch [5/10], Step [508/1126], Loss: 0.0279\n",
      "Epoch [5/10], Step [512/1126], Loss: 0.0566\n",
      "Epoch [5/10], Step [516/1126], Loss: 0.0784\n",
      "Epoch [5/10], Step [520/1126], Loss: 0.0550\n",
      "Epoch [5/10], Step [524/1126], Loss: 0.1024\n",
      "Epoch [5/10], Step [528/1126], Loss: 0.0476\n",
      "Epoch [5/10], Step [532/1126], Loss: 0.0930\n",
      "Epoch [5/10], Step [536/1126], Loss: 0.1350\n",
      "Epoch [5/10], Step [540/1126], Loss: 0.0444\n",
      "Epoch [5/10], Step [544/1126], Loss: 0.0863\n",
      "Epoch [5/10], Step [548/1126], Loss: 0.0547\n",
      "Epoch [5/10], Step [552/1126], Loss: 0.0552\n",
      "Epoch [5/10], Step [556/1126], Loss: 0.0655\n",
      "Epoch [5/10], Step [560/1126], Loss: 0.0489\n",
      "Epoch [5/10], Step [564/1126], Loss: 0.0871\n",
      "Epoch [5/10], Step [568/1126], Loss: 0.0917\n",
      "Epoch [5/10], Step [572/1126], Loss: 0.1044\n",
      "Epoch [5/10], Step [576/1126], Loss: 0.0471\n",
      "Epoch [5/10], Step [580/1126], Loss: 0.0752\n",
      "Epoch [5/10], Step [584/1126], Loss: 0.0741\n",
      "Epoch [5/10], Step [588/1126], Loss: 0.0595\n",
      "Epoch [5/10], Step [592/1126], Loss: 0.1547\n",
      "Epoch [5/10], Step [596/1126], Loss: 0.0745\n",
      "Epoch [5/10], Step [600/1126], Loss: 0.0577\n",
      "Epoch [5/10], Step [604/1126], Loss: 0.1378\n",
      "Epoch [5/10], Step [608/1126], Loss: 0.0866\n",
      "Epoch [5/10], Step [612/1126], Loss: 0.0332\n",
      "Epoch [5/10], Step [616/1126], Loss: 0.0686\n",
      "Epoch [5/10], Step [620/1126], Loss: 0.1169\n",
      "Epoch [5/10], Step [624/1126], Loss: 0.1070\n",
      "Epoch [5/10], Step [628/1126], Loss: 0.0370\n",
      "Epoch [5/10], Step [632/1126], Loss: 0.0433\n",
      "Epoch [5/10], Step [636/1126], Loss: 0.0399\n",
      "Epoch [5/10], Step [640/1126], Loss: 0.0289\n",
      "Epoch [5/10], Step [644/1126], Loss: 0.0445\n",
      "Epoch [5/10], Step [648/1126], Loss: 0.0687\n",
      "Epoch [5/10], Step [652/1126], Loss: 0.0615\n",
      "Epoch [5/10], Step [656/1126], Loss: 0.0394\n",
      "Epoch [5/10], Step [660/1126], Loss: 0.0703\n",
      "Epoch [5/10], Step [664/1126], Loss: 0.0613\n",
      "Epoch [5/10], Step [668/1126], Loss: 0.1293\n",
      "Epoch [5/10], Step [672/1126], Loss: 0.0466\n",
      "Epoch [5/10], Step [676/1126], Loss: 0.0176\n",
      "Epoch [5/10], Step [680/1126], Loss: 0.0312\n",
      "Epoch [5/10], Step [684/1126], Loss: 0.0904\n",
      "Epoch [5/10], Step [688/1126], Loss: 0.0346\n",
      "Epoch [5/10], Step [692/1126], Loss: 0.0145\n",
      "Epoch [5/10], Step [696/1126], Loss: 0.0523\n",
      "Epoch [5/10], Step [700/1126], Loss: 0.1188\n",
      "Epoch [5/10], Step [704/1126], Loss: 0.0905\n",
      "Epoch [5/10], Step [708/1126], Loss: 0.0669\n",
      "Epoch [5/10], Step [712/1126], Loss: 0.0592\n",
      "Epoch [5/10], Step [716/1126], Loss: 0.1268\n",
      "Epoch [5/10], Step [720/1126], Loss: 0.0493\n",
      "Epoch [5/10], Step [724/1126], Loss: 0.0569\n",
      "Epoch [5/10], Step [728/1126], Loss: 0.1005\n",
      "Epoch [5/10], Step [732/1126], Loss: 0.1363\n",
      "Epoch [5/10], Step [736/1126], Loss: 0.1068\n",
      "Epoch [5/10], Step [740/1126], Loss: 0.0403\n",
      "Epoch [5/10], Step [744/1126], Loss: 0.0840\n",
      "Epoch [5/10], Step [748/1126], Loss: 0.0314\n",
      "Epoch [5/10], Step [752/1126], Loss: 0.0667\n",
      "Epoch [5/10], Step [756/1126], Loss: 0.0846\n",
      "Epoch [5/10], Step [760/1126], Loss: 0.0519\n",
      "Epoch [5/10], Step [764/1126], Loss: 0.0784\n",
      "Epoch [5/10], Step [768/1126], Loss: 0.0863\n",
      "Epoch [5/10], Step [772/1126], Loss: 0.0576\n",
      "Epoch [5/10], Step [776/1126], Loss: 0.0869\n",
      "Epoch [5/10], Step [780/1126], Loss: 0.0990\n",
      "Epoch [5/10], Step [784/1126], Loss: 0.0663\n",
      "Epoch [5/10], Step [788/1126], Loss: 0.0558\n",
      "Epoch [5/10], Step [792/1126], Loss: 0.0650\n",
      "Epoch [5/10], Step [796/1126], Loss: 0.0399\n",
      "Epoch [5/10], Step [800/1126], Loss: 0.0556\n",
      "Epoch [5/10], Step [804/1126], Loss: 0.0417\n",
      "Epoch [5/10], Step [808/1126], Loss: 0.0636\n",
      "Epoch [5/10], Step [812/1126], Loss: 0.1278\n",
      "Epoch [5/10], Step [816/1126], Loss: 0.0816\n",
      "Epoch [5/10], Step [820/1126], Loss: 0.0558\n",
      "Epoch [5/10], Step [824/1126], Loss: 0.0535\n",
      "Epoch [5/10], Step [828/1126], Loss: 0.0414\n",
      "Epoch [5/10], Step [832/1126], Loss: 0.1015\n",
      "Epoch [5/10], Step [836/1126], Loss: 0.1457\n",
      "Epoch [5/10], Step [840/1126], Loss: 0.0365\n",
      "Epoch [5/10], Step [844/1126], Loss: 0.0803\n",
      "Epoch [5/10], Step [848/1126], Loss: 0.0801\n",
      "Epoch [5/10], Step [852/1126], Loss: 0.0586\n",
      "Epoch [5/10], Step [856/1126], Loss: 0.0610\n",
      "Epoch [5/10], Step [860/1126], Loss: 0.0417\n",
      "Epoch [5/10], Step [864/1126], Loss: 0.0782\n",
      "Epoch [5/10], Step [868/1126], Loss: 0.0913\n",
      "Epoch [5/10], Step [872/1126], Loss: 0.0226\n",
      "Epoch [5/10], Step [876/1126], Loss: 0.0337\n",
      "Epoch [5/10], Step [880/1126], Loss: 0.0942\n",
      "Epoch [5/10], Step [884/1126], Loss: 0.0632\n",
      "Epoch [5/10], Step [888/1126], Loss: 0.0913\n",
      "Epoch [5/10], Step [892/1126], Loss: 0.0738\n",
      "Epoch [5/10], Step [896/1126], Loss: 0.1237\n",
      "Epoch [5/10], Step [900/1126], Loss: 0.0330\n",
      "Epoch [5/10], Step [904/1126], Loss: 0.0448\n",
      "Epoch [5/10], Step [908/1126], Loss: 0.1088\n",
      "Epoch [5/10], Step [912/1126], Loss: 0.0341\n",
      "Epoch [5/10], Step [916/1126], Loss: 0.1197\n",
      "Epoch [5/10], Step [920/1126], Loss: 0.0677\n",
      "Epoch [5/10], Step [924/1126], Loss: 0.0140\n",
      "Epoch [5/10], Step [928/1126], Loss: 0.0140\n",
      "Epoch [5/10], Step [932/1126], Loss: 0.0360\n",
      "Epoch [5/10], Step [936/1126], Loss: 0.0407\n",
      "Epoch [5/10], Step [940/1126], Loss: 0.0444\n",
      "Epoch [5/10], Step [944/1126], Loss: 0.0291\n",
      "Epoch [5/10], Step [948/1126], Loss: 0.0413\n",
      "Epoch [5/10], Step [952/1126], Loss: 0.0961\n",
      "Epoch [5/10], Step [956/1126], Loss: 0.0453\n",
      "Epoch [5/10], Step [960/1126], Loss: 0.0528\n",
      "Epoch [5/10], Step [964/1126], Loss: 0.0318\n",
      "Epoch [5/10], Step [968/1126], Loss: 0.0580\n",
      "Epoch [5/10], Step [972/1126], Loss: 0.0179\n",
      "Epoch [5/10], Step [976/1126], Loss: 0.0803\n",
      "Epoch [5/10], Step [980/1126], Loss: 0.0769\n",
      "Epoch [5/10], Step [984/1126], Loss: 0.0197\n",
      "Epoch [5/10], Step [988/1126], Loss: 0.0512\n",
      "Epoch [5/10], Step [992/1126], Loss: 0.0996\n",
      "Epoch [5/10], Step [996/1126], Loss: 0.0747\n",
      "Epoch [5/10], Step [1000/1126], Loss: 0.0661\n",
      "Epoch [5/10], Step [1004/1126], Loss: 0.0754\n",
      "Epoch [5/10], Step [1008/1126], Loss: 0.0451\n",
      "Epoch [5/10], Step [1012/1126], Loss: 0.0780\n",
      "Epoch [5/10], Step [1016/1126], Loss: 0.0377\n",
      "Epoch [5/10], Step [1020/1126], Loss: 0.0602\n",
      "Epoch [5/10], Step [1024/1126], Loss: 0.1035\n",
      "Epoch [5/10], Step [1028/1126], Loss: 0.0914\n",
      "Epoch [5/10], Step [1032/1126], Loss: 0.0456\n",
      "Epoch [5/10], Step [1036/1126], Loss: 0.0468\n",
      "Epoch [5/10], Step [1040/1126], Loss: 0.0913\n",
      "Epoch [5/10], Step [1044/1126], Loss: 0.0348\n",
      "Epoch [5/10], Step [1048/1126], Loss: 0.0614\n",
      "Epoch [5/10], Step [1052/1126], Loss: 0.2009\n",
      "Epoch [5/10], Step [1056/1126], Loss: 0.0695\n",
      "Epoch [5/10], Step [1060/1126], Loss: 0.0565\n",
      "Epoch [5/10], Step [1064/1126], Loss: 0.1489\n",
      "Epoch [5/10], Step [1068/1126], Loss: 0.0555\n",
      "Epoch [5/10], Step [1072/1126], Loss: 0.0737\n",
      "Epoch [5/10], Step [1076/1126], Loss: 0.0433\n",
      "Epoch [5/10], Step [1080/1126], Loss: 0.0615\n",
      "Epoch [5/10], Step [1084/1126], Loss: 0.0604\n",
      "Epoch [5/10], Step [1088/1126], Loss: 0.0392\n",
      "Epoch [5/10], Step [1092/1126], Loss: 0.0758\n",
      "Epoch [5/10], Step [1096/1126], Loss: 0.0476\n",
      "Epoch [5/10], Step [1100/1126], Loss: 0.0488\n",
      "Epoch [5/10], Step [1104/1126], Loss: 0.1050\n",
      "Epoch [5/10], Step [1108/1126], Loss: 0.0759\n",
      "Epoch [5/10], Step [1112/1126], Loss: 0.0505\n",
      "Epoch [5/10], Step [1116/1126], Loss: 0.0655\n",
      "Epoch [5/10], Step [1120/1126], Loss: 0.1266\n",
      "Epoch [5/10], Step [1124/1126], Loss: 0.0844\n",
      "Epoch [6/10], Step [4/1126], Loss: 0.1341\n",
      "Epoch [6/10], Step [8/1126], Loss: 0.0617\n",
      "Epoch [6/10], Step [12/1126], Loss: 0.0394\n",
      "Epoch [6/10], Step [16/1126], Loss: 0.0422\n",
      "Epoch [6/10], Step [20/1126], Loss: 0.0559\n",
      "Epoch [6/10], Step [24/1126], Loss: 0.0502\n",
      "Epoch [6/10], Step [28/1126], Loss: 0.0292\n",
      "Epoch [6/10], Step [32/1126], Loss: 0.0466\n",
      "Epoch [6/10], Step [36/1126], Loss: 0.1217\n",
      "Epoch [6/10], Step [40/1126], Loss: 0.0593\n",
      "Epoch [6/10], Step [44/1126], Loss: 0.0367\n",
      "Epoch [6/10], Step [48/1126], Loss: 0.0579\n",
      "Epoch [6/10], Step [52/1126], Loss: 0.1887\n",
      "Epoch [6/10], Step [56/1126], Loss: 0.0765\n",
      "Epoch [6/10], Step [60/1126], Loss: 0.0814\n",
      "Epoch [6/10], Step [64/1126], Loss: 0.1043\n",
      "Epoch [6/10], Step [68/1126], Loss: 0.0735\n",
      "Epoch [6/10], Step [72/1126], Loss: 0.0378\n",
      "Epoch [6/10], Step [76/1126], Loss: 0.0503\n",
      "Epoch [6/10], Step [80/1126], Loss: 0.0670\n",
      "Epoch [6/10], Step [84/1126], Loss: 0.0628\n",
      "Epoch [6/10], Step [88/1126], Loss: 0.0564\n",
      "Epoch [6/10], Step [92/1126], Loss: 0.0887\n",
      "Epoch [6/10], Step [96/1126], Loss: 0.0584\n",
      "Epoch [6/10], Step [100/1126], Loss: 0.1530\n",
      "Epoch [6/10], Step [104/1126], Loss: 0.0673\n",
      "Epoch [6/10], Step [108/1126], Loss: 0.0194\n",
      "Epoch [6/10], Step [112/1126], Loss: 0.0316\n",
      "Epoch [6/10], Step [116/1126], Loss: 0.0624\n",
      "Epoch [6/10], Step [120/1126], Loss: 0.0613\n",
      "Epoch [6/10], Step [124/1126], Loss: 0.0363\n",
      "Epoch [6/10], Step [128/1126], Loss: 0.0518\n",
      "Epoch [6/10], Step [132/1126], Loss: 0.0259\n",
      "Epoch [6/10], Step [136/1126], Loss: 0.0319\n",
      "Epoch [6/10], Step [140/1126], Loss: 0.0345\n",
      "Epoch [6/10], Step [144/1126], Loss: 0.0315\n",
      "Epoch [6/10], Step [148/1126], Loss: 0.0689\n",
      "Epoch [6/10], Step [152/1126], Loss: 0.1346\n",
      "Epoch [6/10], Step [156/1126], Loss: 0.0689\n",
      "Epoch [6/10], Step [160/1126], Loss: 0.0614\n",
      "Epoch [6/10], Step [164/1126], Loss: 0.0336\n",
      "Epoch [6/10], Step [168/1126], Loss: 0.1282\n",
      "Epoch [6/10], Step [172/1126], Loss: 0.0442\n",
      "Epoch [6/10], Step [176/1126], Loss: 0.1014\n",
      "Epoch [6/10], Step [180/1126], Loss: 0.0635\n",
      "Epoch [6/10], Step [184/1126], Loss: 0.0528\n",
      "Epoch [6/10], Step [188/1126], Loss: 0.0496\n",
      "Epoch [6/10], Step [192/1126], Loss: 0.0950\n",
      "Epoch [6/10], Step [196/1126], Loss: 0.0571\n",
      "Epoch [6/10], Step [200/1126], Loss: 0.0633\n",
      "Epoch [6/10], Step [204/1126], Loss: 0.0608\n",
      "Epoch [6/10], Step [208/1126], Loss: 0.0892\n",
      "Epoch [6/10], Step [212/1126], Loss: 0.0627\n",
      "Epoch [6/10], Step [216/1126], Loss: 0.0655\n",
      "Epoch [6/10], Step [220/1126], Loss: 0.0724\n",
      "Epoch [6/10], Step [224/1126], Loss: 0.1130\n",
      "Epoch [6/10], Step [228/1126], Loss: 0.0505\n",
      "Epoch [6/10], Step [232/1126], Loss: 0.1005\n",
      "Epoch [6/10], Step [236/1126], Loss: 0.0431\n",
      "Epoch [6/10], Step [240/1126], Loss: 0.0893\n",
      "Epoch [6/10], Step [244/1126], Loss: 0.1180\n",
      "Epoch [6/10], Step [248/1126], Loss: 0.0990\n",
      "Epoch [6/10], Step [252/1126], Loss: 0.0365\n",
      "Epoch [6/10], Step [256/1126], Loss: 0.0348\n",
      "Epoch [6/10], Step [260/1126], Loss: 0.1040\n",
      "Epoch [6/10], Step [264/1126], Loss: 0.0321\n",
      "Epoch [6/10], Step [268/1126], Loss: 0.0902\n",
      "Epoch [6/10], Step [272/1126], Loss: 0.0300\n",
      "Epoch [6/10], Step [276/1126], Loss: 0.0276\n",
      "Epoch [6/10], Step [280/1126], Loss: 0.0380\n",
      "Epoch [6/10], Step [284/1126], Loss: 0.1631\n",
      "Epoch [6/10], Step [288/1126], Loss: 0.0693\n",
      "Epoch [6/10], Step [292/1126], Loss: 0.0550\n",
      "Epoch [6/10], Step [296/1126], Loss: 0.0445\n",
      "Epoch [6/10], Step [300/1126], Loss: 0.0438\n",
      "Epoch [6/10], Step [304/1126], Loss: 0.0593\n",
      "Epoch [6/10], Step [308/1126], Loss: 0.0720\n",
      "Epoch [6/10], Step [312/1126], Loss: 0.0387\n",
      "Epoch [6/10], Step [316/1126], Loss: 0.0534\n",
      "Epoch [6/10], Step [320/1126], Loss: 0.0523\n",
      "Epoch [6/10], Step [324/1126], Loss: 0.1355\n",
      "Epoch [6/10], Step [328/1126], Loss: 0.0405\n",
      "Epoch [6/10], Step [332/1126], Loss: 0.0508\n",
      "Epoch [6/10], Step [336/1126], Loss: 0.0427\n",
      "Epoch [6/10], Step [340/1126], Loss: 0.0380\n",
      "Epoch [6/10], Step [344/1126], Loss: 0.0324\n",
      "Epoch [6/10], Step [348/1126], Loss: 0.1002\n",
      "Epoch [6/10], Step [352/1126], Loss: 0.0489\n",
      "Epoch [6/10], Step [356/1126], Loss: 0.0151\n",
      "Epoch [6/10], Step [360/1126], Loss: 0.0199\n",
      "Epoch [6/10], Step [364/1126], Loss: 0.0213\n",
      "Epoch [6/10], Step [368/1126], Loss: 0.0758\n",
      "Epoch [6/10], Step [372/1126], Loss: 0.0390\n",
      "Epoch [6/10], Step [376/1126], Loss: 0.0646\n",
      "Epoch [6/10], Step [380/1126], Loss: 0.0828\n",
      "Epoch [6/10], Step [384/1126], Loss: 0.0808\n",
      "Epoch [6/10], Step [388/1126], Loss: 0.0523\n",
      "Epoch [6/10], Step [392/1126], Loss: 0.0697\n",
      "Epoch [6/10], Step [396/1126], Loss: 0.0270\n",
      "Epoch [6/10], Step [400/1126], Loss: 0.0130\n",
      "Epoch [6/10], Step [404/1126], Loss: 0.0235\n",
      "Epoch [6/10], Step [408/1126], Loss: 0.1083\n",
      "Epoch [6/10], Step [412/1126], Loss: 0.0778\n",
      "Epoch [6/10], Step [416/1126], Loss: 0.1003\n",
      "Epoch [6/10], Step [420/1126], Loss: 0.0575\n",
      "Epoch [6/10], Step [424/1126], Loss: 0.1134\n",
      "Epoch [6/10], Step [428/1126], Loss: 0.0454\n",
      "Epoch [6/10], Step [432/1126], Loss: 0.1086\n",
      "Epoch [6/10], Step [436/1126], Loss: 0.1029\n",
      "Epoch [6/10], Step [440/1126], Loss: 0.1179\n",
      "Epoch [6/10], Step [444/1126], Loss: 0.0614\n",
      "Epoch [6/10], Step [448/1126], Loss: 0.1335\n",
      "Epoch [6/10], Step [452/1126], Loss: 0.0675\n",
      "Epoch [6/10], Step [456/1126], Loss: 0.0926\n",
      "Epoch [6/10], Step [460/1126], Loss: 0.0757\n",
      "Epoch [6/10], Step [464/1126], Loss: 0.0841\n",
      "Epoch [6/10], Step [468/1126], Loss: 0.0998\n",
      "Epoch [6/10], Step [472/1126], Loss: 0.0986\n",
      "Epoch [6/10], Step [476/1126], Loss: 0.1106\n",
      "Epoch [6/10], Step [480/1126], Loss: 0.0573\n",
      "Epoch [6/10], Step [484/1126], Loss: 0.0649\n",
      "Epoch [6/10], Step [488/1126], Loss: 0.0401\n",
      "Epoch [6/10], Step [492/1126], Loss: 0.0358\n",
      "Epoch [6/10], Step [496/1126], Loss: 0.0928\n",
      "Epoch [6/10], Step [500/1126], Loss: 0.0324\n",
      "Epoch [6/10], Step [504/1126], Loss: 0.0386\n",
      "Epoch [6/10], Step [508/1126], Loss: 0.0413\n",
      "Epoch [6/10], Step [512/1126], Loss: 0.0752\n",
      "Epoch [6/10], Step [516/1126], Loss: 0.0406\n",
      "Epoch [6/10], Step [520/1126], Loss: 0.0657\n",
      "Epoch [6/10], Step [524/1126], Loss: 0.0481\n",
      "Epoch [6/10], Step [528/1126], Loss: 0.0471\n",
      "Epoch [6/10], Step [532/1126], Loss: 0.1198\n",
      "Epoch [6/10], Step [536/1126], Loss: 0.0662\n",
      "Epoch [6/10], Step [540/1126], Loss: 0.0585\n",
      "Epoch [6/10], Step [544/1126], Loss: 0.0785\n",
      "Epoch [6/10], Step [548/1126], Loss: 0.0902\n",
      "Epoch [6/10], Step [552/1126], Loss: 0.0581\n",
      "Epoch [6/10], Step [556/1126], Loss: 0.0679\n",
      "Epoch [6/10], Step [560/1126], Loss: 0.0398\n",
      "Epoch [6/10], Step [564/1126], Loss: 0.1008\n",
      "Epoch [6/10], Step [568/1126], Loss: 0.0647\n",
      "Epoch [6/10], Step [572/1126], Loss: 0.1369\n",
      "Epoch [6/10], Step [576/1126], Loss: 0.0422\n",
      "Epoch [6/10], Step [580/1126], Loss: 0.0886\n",
      "Epoch [6/10], Step [584/1126], Loss: 0.0721\n",
      "Epoch [6/10], Step [588/1126], Loss: 0.0437\n",
      "Epoch [6/10], Step [592/1126], Loss: 0.1084\n",
      "Epoch [6/10], Step [596/1126], Loss: 0.0913\n",
      "Epoch [6/10], Step [600/1126], Loss: 0.0717\n",
      "Epoch [6/10], Step [604/1126], Loss: 0.0481\n",
      "Epoch [6/10], Step [608/1126], Loss: 0.0704\n",
      "Epoch [6/10], Step [612/1126], Loss: 0.0289\n",
      "Epoch [6/10], Step [616/1126], Loss: 0.0478\n",
      "Epoch [6/10], Step [620/1126], Loss: 0.0441\n",
      "Epoch [6/10], Step [624/1126], Loss: 0.0504\n",
      "Epoch [6/10], Step [628/1126], Loss: 0.0318\n",
      "Epoch [6/10], Step [632/1126], Loss: 0.0691\n",
      "Epoch [6/10], Step [636/1126], Loss: 0.0607\n",
      "Epoch [6/10], Step [640/1126], Loss: 0.0444\n",
      "Epoch [6/10], Step [644/1126], Loss: 0.0453\n",
      "Epoch [6/10], Step [648/1126], Loss: 0.0760\n",
      "Epoch [6/10], Step [652/1126], Loss: 0.0459\n",
      "Epoch [6/10], Step [656/1126], Loss: 0.0657\n",
      "Epoch [6/10], Step [660/1126], Loss: 0.1226\n",
      "Epoch [6/10], Step [664/1126], Loss: 0.0212\n",
      "Epoch [6/10], Step [668/1126], Loss: 0.0935\n",
      "Epoch [6/10], Step [672/1126], Loss: 0.0518\n",
      "Epoch [6/10], Step [676/1126], Loss: 0.0326\n",
      "Epoch [6/10], Step [680/1126], Loss: 0.0304\n",
      "Epoch [6/10], Step [684/1126], Loss: 0.0654\n",
      "Epoch [6/10], Step [688/1126], Loss: 0.0587\n",
      "Epoch [6/10], Step [692/1126], Loss: 0.0159\n",
      "Epoch [6/10], Step [696/1126], Loss: 0.0523\n",
      "Epoch [6/10], Step [700/1126], Loss: 0.0800\n",
      "Epoch [6/10], Step [704/1126], Loss: 0.0243\n",
      "Epoch [6/10], Step [708/1126], Loss: 0.2020\n",
      "Epoch [6/10], Step [712/1126], Loss: 0.0862\n",
      "Epoch [6/10], Step [716/1126], Loss: 0.0473\n",
      "Epoch [6/10], Step [720/1126], Loss: 0.1292\n",
      "Epoch [6/10], Step [724/1126], Loss: 0.0829\n",
      "Epoch [6/10], Step [728/1126], Loss: 0.1616\n",
      "Epoch [6/10], Step [732/1126], Loss: 0.1000\n",
      "Epoch [6/10], Step [736/1126], Loss: 0.1136\n",
      "Epoch [6/10], Step [740/1126], Loss: 0.0428\n",
      "Epoch [6/10], Step [744/1126], Loss: 0.0596\n",
      "Epoch [6/10], Step [748/1126], Loss: 0.0390\n",
      "Epoch [6/10], Step [752/1126], Loss: 0.1155\n",
      "Epoch [6/10], Step [756/1126], Loss: 0.1276\n",
      "Epoch [6/10], Step [760/1126], Loss: 0.0836\n",
      "Epoch [6/10], Step [764/1126], Loss: 0.0684\n",
      "Epoch [6/10], Step [768/1126], Loss: 0.0585\n",
      "Epoch [6/10], Step [772/1126], Loss: 0.0639\n",
      "Epoch [6/10], Step [776/1126], Loss: 0.0508\n",
      "Epoch [6/10], Step [780/1126], Loss: 0.0875\n",
      "Epoch [6/10], Step [784/1126], Loss: 0.0725\n",
      "Epoch [6/10], Step [788/1126], Loss: 0.0737\n",
      "Epoch [6/10], Step [792/1126], Loss: 0.0418\n",
      "Epoch [6/10], Step [796/1126], Loss: 0.0723\n",
      "Epoch [6/10], Step [800/1126], Loss: 0.0600\n",
      "Epoch [6/10], Step [804/1126], Loss: 0.0226\n",
      "Epoch [6/10], Step [808/1126], Loss: 0.0438\n",
      "Epoch [6/10], Step [812/1126], Loss: 0.0575\n",
      "Epoch [6/10], Step [816/1126], Loss: 0.0826\n",
      "Epoch [6/10], Step [820/1126], Loss: 0.0770\n",
      "Epoch [6/10], Step [824/1126], Loss: 0.0930\n",
      "Epoch [6/10], Step [828/1126], Loss: 0.0480\n",
      "Epoch [6/10], Step [832/1126], Loss: 0.1221\n",
      "Epoch [6/10], Step [836/1126], Loss: 0.0752\n",
      "Epoch [6/10], Step [840/1126], Loss: 0.0269\n",
      "Epoch [6/10], Step [844/1126], Loss: 0.0944\n",
      "Epoch [6/10], Step [848/1126], Loss: 0.0522\n",
      "Epoch [6/10], Step [852/1126], Loss: 0.0633\n",
      "Epoch [6/10], Step [856/1126], Loss: 0.0251\n",
      "Epoch [6/10], Step [860/1126], Loss: 0.0237\n",
      "Epoch [6/10], Step [864/1126], Loss: 0.0496\n",
      "Epoch [6/10], Step [868/1126], Loss: 0.0745\n",
      "Epoch [6/10], Step [872/1126], Loss: 0.0586\n",
      "Epoch [6/10], Step [876/1126], Loss: 0.0790\n",
      "Epoch [6/10], Step [880/1126], Loss: 0.1099\n",
      "Epoch [6/10], Step [884/1126], Loss: 0.0564\n",
      "Epoch [6/10], Step [888/1126], Loss: 0.2612\n",
      "Epoch [6/10], Step [892/1126], Loss: 0.0788\n",
      "Epoch [6/10], Step [896/1126], Loss: 0.0585\n",
      "Epoch [6/10], Step [900/1126], Loss: 0.0389\n",
      "Epoch [6/10], Step [904/1126], Loss: 0.0545\n",
      "Epoch [6/10], Step [908/1126], Loss: 0.0579\n",
      "Epoch [6/10], Step [912/1126], Loss: 0.0166\n",
      "Epoch [6/10], Step [916/1126], Loss: 0.0922\n",
      "Epoch [6/10], Step [920/1126], Loss: 0.0376\n",
      "Epoch [6/10], Step [924/1126], Loss: 0.0467\n",
      "Epoch [6/10], Step [928/1126], Loss: 0.0383\n",
      "Epoch [6/10], Step [932/1126], Loss: 0.0399\n",
      "Epoch [6/10], Step [936/1126], Loss: 0.0546\n",
      "Epoch [6/10], Step [940/1126], Loss: 0.0545\n",
      "Epoch [6/10], Step [944/1126], Loss: 0.0794\n",
      "Epoch [6/10], Step [948/1126], Loss: 0.1122\n",
      "Epoch [6/10], Step [952/1126], Loss: 0.0210\n",
      "Epoch [6/10], Step [956/1126], Loss: 0.0842\n",
      "Epoch [6/10], Step [960/1126], Loss: 0.0605\n",
      "Epoch [6/10], Step [964/1126], Loss: 0.0590\n",
      "Epoch [6/10], Step [968/1126], Loss: 0.0538\n",
      "Epoch [6/10], Step [972/1126], Loss: 0.0132\n",
      "Epoch [6/10], Step [976/1126], Loss: 0.0544\n",
      "Epoch [6/10], Step [980/1126], Loss: 0.0257\n",
      "Epoch [6/10], Step [984/1126], Loss: 0.0201\n",
      "Epoch [6/10], Step [988/1126], Loss: 0.0446\n",
      "Epoch [6/10], Step [992/1126], Loss: 0.0749\n",
      "Epoch [6/10], Step [996/1126], Loss: 0.0600\n",
      "Epoch [6/10], Step [1000/1126], Loss: 0.0908\n",
      "Epoch [6/10], Step [1004/1126], Loss: 0.1259\n",
      "Epoch [6/10], Step [1008/1126], Loss: 0.0442\n",
      "Epoch [6/10], Step [1012/1126], Loss: 0.0775\n",
      "Epoch [6/10], Step [1016/1126], Loss: 0.0523\n",
      "Epoch [6/10], Step [1020/1126], Loss: 0.0596\n",
      "Epoch [6/10], Step [1024/1126], Loss: 0.0759\n",
      "Epoch [6/10], Step [1028/1126], Loss: 0.0615\n",
      "Epoch [6/10], Step [1032/1126], Loss: 0.0426\n",
      "Epoch [6/10], Step [1036/1126], Loss: 0.0994\n",
      "Epoch [6/10], Step [1040/1126], Loss: 0.0683\n",
      "Epoch [6/10], Step [1044/1126], Loss: 0.0313\n",
      "Epoch [6/10], Step [1048/1126], Loss: 0.0680\n",
      "Epoch [6/10], Step [1052/1126], Loss: 0.1409\n",
      "Epoch [6/10], Step [1056/1126], Loss: 0.0900\n",
      "Epoch [6/10], Step [1060/1126], Loss: 0.0542\n",
      "Epoch [6/10], Step [1064/1126], Loss: 0.2197\n",
      "Epoch [6/10], Step [1068/1126], Loss: 0.0538\n",
      "Epoch [6/10], Step [1072/1126], Loss: 0.0873\n",
      "Epoch [6/10], Step [1076/1126], Loss: 0.0480\n",
      "Epoch [6/10], Step [1080/1126], Loss: 0.0763\n",
      "Epoch [6/10], Step [1084/1126], Loss: 0.0685\n",
      "Epoch [6/10], Step [1088/1126], Loss: 0.0444\n",
      "Epoch [6/10], Step [1092/1126], Loss: 0.0574\n",
      "Epoch [6/10], Step [1096/1126], Loss: 0.0432\n",
      "Epoch [6/10], Step [1100/1126], Loss: 0.2732\n",
      "Epoch [6/10], Step [1104/1126], Loss: 0.1178\n",
      "Epoch [6/10], Step [1108/1126], Loss: 0.0569\n",
      "Epoch [6/10], Step [1112/1126], Loss: 0.0508\n",
      "Epoch [6/10], Step [1116/1126], Loss: 0.0610\n",
      "Epoch [6/10], Step [1120/1126], Loss: 0.0437\n",
      "Epoch [6/10], Step [1124/1126], Loss: 0.1258\n",
      "Epoch [7/10], Step [4/1126], Loss: 0.0682\n",
      "Epoch [7/10], Step [8/1126], Loss: 0.0417\n",
      "Epoch [7/10], Step [12/1126], Loss: 0.0324\n",
      "Epoch [7/10], Step [16/1126], Loss: 0.0834\n",
      "Epoch [7/10], Step [20/1126], Loss: 0.0303\n",
      "Epoch [7/10], Step [24/1126], Loss: 0.0409\n",
      "Epoch [7/10], Step [28/1126], Loss: 0.0208\n",
      "Epoch [7/10], Step [32/1126], Loss: 0.0982\n",
      "Epoch [7/10], Step [36/1126], Loss: 0.0712\n",
      "Epoch [7/10], Step [40/1126], Loss: 0.0642\n",
      "Epoch [7/10], Step [44/1126], Loss: 0.0364\n",
      "Epoch [7/10], Step [48/1126], Loss: 0.0471\n",
      "Epoch [7/10], Step [52/1126], Loss: 0.1177\n",
      "Epoch [7/10], Step [56/1126], Loss: 0.0665\n",
      "Epoch [7/10], Step [60/1126], Loss: 0.0896\n",
      "Epoch [7/10], Step [64/1126], Loss: 0.1138\n",
      "Epoch [7/10], Step [68/1126], Loss: 0.0843\n",
      "Epoch [7/10], Step [72/1126], Loss: 0.0503\n",
      "Epoch [7/10], Step [76/1126], Loss: 0.0391\n",
      "Epoch [7/10], Step [80/1126], Loss: 0.0694\n",
      "Epoch [7/10], Step [84/1126], Loss: 0.0634\n",
      "Epoch [7/10], Step [88/1126], Loss: 0.0457\n",
      "Epoch [7/10], Step [92/1126], Loss: 0.0509\n",
      "Epoch [7/10], Step [96/1126], Loss: 0.0536\n",
      "Epoch [7/10], Step [100/1126], Loss: 0.0562\n",
      "Epoch [7/10], Step [104/1126], Loss: 0.0454\n",
      "Epoch [7/10], Step [108/1126], Loss: 0.0468\n",
      "Epoch [7/10], Step [112/1126], Loss: 0.0301\n",
      "Epoch [7/10], Step [116/1126], Loss: 0.0427\n",
      "Epoch [7/10], Step [120/1126], Loss: 0.0396\n",
      "Epoch [7/10], Step [124/1126], Loss: 0.0282\n",
      "Epoch [7/10], Step [128/1126], Loss: 0.0631\n",
      "Epoch [7/10], Step [132/1126], Loss: 0.0259\n",
      "Epoch [7/10], Step [136/1126], Loss: 0.0345\n",
      "Epoch [7/10], Step [140/1126], Loss: 0.1119\n",
      "Epoch [7/10], Step [144/1126], Loss: 0.0859\n",
      "Epoch [7/10], Step [148/1126], Loss: 0.0624\n",
      "Epoch [7/10], Step [152/1126], Loss: 0.0749\n",
      "Epoch [7/10], Step [156/1126], Loss: 0.0373\n",
      "Epoch [7/10], Step [160/1126], Loss: 0.0568\n",
      "Epoch [7/10], Step [164/1126], Loss: 0.0381\n",
      "Epoch [7/10], Step [168/1126], Loss: 0.0629\n",
      "Epoch [7/10], Step [172/1126], Loss: 0.0398\n",
      "Epoch [7/10], Step [176/1126], Loss: 0.0286\n",
      "Epoch [7/10], Step [180/1126], Loss: 0.0412\n",
      "Epoch [7/10], Step [184/1126], Loss: 0.0700\n",
      "Epoch [7/10], Step [188/1126], Loss: 0.1150\n",
      "Epoch [7/10], Step [192/1126], Loss: 0.0546\n",
      "Epoch [7/10], Step [196/1126], Loss: 0.0246\n",
      "Epoch [7/10], Step [200/1126], Loss: 0.0377\n",
      "Epoch [7/10], Step [204/1126], Loss: 0.1239\n",
      "Epoch [7/10], Step [208/1126], Loss: 0.0912\n",
      "Epoch [7/10], Step [212/1126], Loss: 0.0351\n",
      "Epoch [7/10], Step [216/1126], Loss: 0.0381\n",
      "Epoch [7/10], Step [220/1126], Loss: 0.0487\n",
      "Epoch [7/10], Step [224/1126], Loss: 0.0988\n",
      "Epoch [7/10], Step [228/1126], Loss: 0.0380\n",
      "Epoch [7/10], Step [232/1126], Loss: 0.0715\n",
      "Epoch [7/10], Step [236/1126], Loss: 0.0623\n",
      "Epoch [7/10], Step [240/1126], Loss: 0.0889\n",
      "Epoch [7/10], Step [244/1126], Loss: 0.0582\n",
      "Epoch [7/10], Step [248/1126], Loss: 0.0859\n",
      "Epoch [7/10], Step [252/1126], Loss: 0.0523\n",
      "Epoch [7/10], Step [256/1126], Loss: 0.0607\n",
      "Epoch [7/10], Step [260/1126], Loss: 0.0672\n",
      "Epoch [7/10], Step [264/1126], Loss: 0.0874\n",
      "Epoch [7/10], Step [268/1126], Loss: 0.0759\n",
      "Epoch [7/10], Step [272/1126], Loss: 0.0223\n",
      "Epoch [7/10], Step [276/1126], Loss: 0.0141\n",
      "Epoch [7/10], Step [280/1126], Loss: 0.0273\n",
      "Epoch [7/10], Step [284/1126], Loss: 0.0607\n",
      "Epoch [7/10], Step [288/1126], Loss: 0.0899\n",
      "Epoch [7/10], Step [292/1126], Loss: 0.0339\n",
      "Epoch [7/10], Step [296/1126], Loss: 0.0314\n",
      "Epoch [7/10], Step [300/1126], Loss: 0.0680\n",
      "Epoch [7/10], Step [304/1126], Loss: 0.0522\n",
      "Epoch [7/10], Step [308/1126], Loss: 0.0760\n",
      "Epoch [7/10], Step [312/1126], Loss: 0.0300\n",
      "Epoch [7/10], Step [316/1126], Loss: 0.0597\n",
      "Epoch [7/10], Step [320/1126], Loss: 0.0471\n",
      "Epoch [7/10], Step [324/1126], Loss: 0.0990\n",
      "Epoch [7/10], Step [328/1126], Loss: 0.0322\n",
      "Epoch [7/10], Step [332/1126], Loss: 0.0577\n",
      "Epoch [7/10], Step [336/1126], Loss: 0.0276\n",
      "Epoch [7/10], Step [340/1126], Loss: 0.0409\n",
      "Epoch [7/10], Step [344/1126], Loss: 0.0197\n",
      "Epoch [7/10], Step [348/1126], Loss: 0.0510\n",
      "Epoch [7/10], Step [352/1126], Loss: 0.0440\n",
      "Epoch [7/10], Step [356/1126], Loss: 0.0193\n",
      "Epoch [7/10], Step [360/1126], Loss: 0.0288\n",
      "Epoch [7/10], Step [364/1126], Loss: 0.0309\n",
      "Epoch [7/10], Step [368/1126], Loss: 0.0755\n",
      "Epoch [7/10], Step [372/1126], Loss: 0.1063\n",
      "Epoch [7/10], Step [376/1126], Loss: 0.0612\n",
      "Epoch [7/10], Step [380/1126], Loss: 0.0560\n",
      "Epoch [7/10], Step [384/1126], Loss: 0.0723\n",
      "Epoch [7/10], Step [388/1126], Loss: 0.0615\n",
      "Epoch [7/10], Step [392/1126], Loss: 0.0408\n",
      "Epoch [7/10], Step [396/1126], Loss: 0.0396\n",
      "Epoch [7/10], Step [400/1126], Loss: 0.0109\n",
      "Epoch [7/10], Step [404/1126], Loss: 0.0611\n",
      "Epoch [7/10], Step [408/1126], Loss: 0.1785\n",
      "Epoch [7/10], Step [412/1126], Loss: 0.0564\n",
      "Epoch [7/10], Step [416/1126], Loss: 0.2196\n",
      "Epoch [7/10], Step [420/1126], Loss: 0.0944\n",
      "Epoch [7/10], Step [424/1126], Loss: 0.0467\n",
      "Epoch [7/10], Step [428/1126], Loss: 0.2976\n",
      "Epoch [7/10], Step [432/1126], Loss: 0.0504\n",
      "Epoch [7/10], Step [436/1126], Loss: 0.0661\n",
      "Epoch [7/10], Step [440/1126], Loss: 0.0471\n",
      "Epoch [7/10], Step [444/1126], Loss: 0.0937\n",
      "Epoch [7/10], Step [448/1126], Loss: 0.0391\n",
      "Epoch [7/10], Step [452/1126], Loss: 0.0342\n",
      "Epoch [7/10], Step [456/1126], Loss: 0.0369\n",
      "Epoch [7/10], Step [460/1126], Loss: 0.1584\n",
      "Epoch [7/10], Step [464/1126], Loss: 0.0476\n",
      "Epoch [7/10], Step [468/1126], Loss: 0.0439\n",
      "Epoch [7/10], Step [472/1126], Loss: 0.0383\n",
      "Epoch [7/10], Step [476/1126], Loss: 0.0911\n",
      "Epoch [7/10], Step [480/1126], Loss: 0.0449\n",
      "Epoch [7/10], Step [484/1126], Loss: 0.0727\n",
      "Epoch [7/10], Step [488/1126], Loss: 0.0342\n",
      "Epoch [7/10], Step [492/1126], Loss: 0.0183\n",
      "Epoch [7/10], Step [496/1126], Loss: 0.0579\n",
      "Epoch [7/10], Step [500/1126], Loss: 0.0396\n",
      "Epoch [7/10], Step [504/1126], Loss: 0.0555\n",
      "Epoch [7/10], Step [508/1126], Loss: 0.0319\n",
      "Epoch [7/10], Step [512/1126], Loss: 0.0956\n",
      "Epoch [7/10], Step [516/1126], Loss: 0.0370\n",
      "Epoch [7/10], Step [520/1126], Loss: 0.0456\n",
      "Epoch [7/10], Step [524/1126], Loss: 0.0615\n",
      "Epoch [7/10], Step [528/1126], Loss: 0.0408\n",
      "Epoch [7/10], Step [532/1126], Loss: 0.2030\n",
      "Epoch [7/10], Step [536/1126], Loss: 0.0646\n",
      "Epoch [7/10], Step [540/1126], Loss: 0.0989\n",
      "Epoch [7/10], Step [544/1126], Loss: 0.0363\n",
      "Epoch [7/10], Step [548/1126], Loss: 0.1648\n",
      "Epoch [7/10], Step [552/1126], Loss: 0.0873\n",
      "Epoch [7/10], Step [556/1126], Loss: 0.0601\n",
      "Epoch [7/10], Step [560/1126], Loss: 0.0724\n",
      "Epoch [7/10], Step [564/1126], Loss: 0.1135\n",
      "Epoch [7/10], Step [568/1126], Loss: 0.1042\n",
      "Epoch [7/10], Step [572/1126], Loss: 0.1422\n",
      "Epoch [7/10], Step [576/1126], Loss: 0.0704\n",
      "Epoch [7/10], Step [580/1126], Loss: 0.0449\n",
      "Epoch [7/10], Step [584/1126], Loss: 0.0648\n",
      "Epoch [7/10], Step [588/1126], Loss: 0.0381\n",
      "Epoch [7/10], Step [592/1126], Loss: 0.0652\n",
      "Epoch [7/10], Step [596/1126], Loss: 0.0922\n",
      "Epoch [7/10], Step [600/1126], Loss: 0.0630\n",
      "Epoch [7/10], Step [604/1126], Loss: 0.0797\n",
      "Epoch [7/10], Step [608/1126], Loss: 0.0562\n",
      "Epoch [7/10], Step [612/1126], Loss: 0.0519\n",
      "Epoch [7/10], Step [616/1126], Loss: 0.0711\n",
      "Epoch [7/10], Step [620/1126], Loss: 0.0658\n",
      "Epoch [7/10], Step [624/1126], Loss: 0.0684\n",
      "Epoch [7/10], Step [628/1126], Loss: 0.0887\n",
      "Epoch [7/10], Step [632/1126], Loss: 0.0917\n",
      "Epoch [7/10], Step [636/1126], Loss: 0.0598\n",
      "Epoch [7/10], Step [640/1126], Loss: 0.0441\n",
      "Epoch [7/10], Step [644/1126], Loss: 0.0883\n",
      "Epoch [7/10], Step [648/1126], Loss: 0.0614\n",
      "Epoch [7/10], Step [652/1126], Loss: 0.0339\n",
      "Epoch [7/10], Step [656/1126], Loss: 0.0906\n",
      "Epoch [7/10], Step [660/1126], Loss: 0.0632\n",
      "Epoch [7/10], Step [664/1126], Loss: 0.0498\n",
      "Epoch [7/10], Step [668/1126], Loss: 0.1094\n",
      "Epoch [7/10], Step [672/1126], Loss: 0.0447\n",
      "Epoch [7/10], Step [676/1126], Loss: 0.0814\n",
      "Epoch [7/10], Step [680/1126], Loss: 0.0271\n",
      "Epoch [7/10], Step [684/1126], Loss: 0.0795\n",
      "Epoch [7/10], Step [688/1126], Loss: 0.0344\n",
      "Epoch [7/10], Step [692/1126], Loss: 0.0239\n",
      "Epoch [7/10], Step [696/1126], Loss: 0.0428\n",
      "Epoch [7/10], Step [700/1126], Loss: 0.0259\n",
      "Epoch [7/10], Step [704/1126], Loss: 0.0409\n",
      "Epoch [7/10], Step [708/1126], Loss: 0.1014\n",
      "Epoch [7/10], Step [712/1126], Loss: 0.0437\n",
      "Epoch [7/10], Step [716/1126], Loss: 0.0439\n",
      "Epoch [7/10], Step [720/1126], Loss: 0.0396\n",
      "Epoch [7/10], Step [724/1126], Loss: 0.0553\n",
      "Epoch [7/10], Step [728/1126], Loss: 0.0898\n",
      "Epoch [7/10], Step [732/1126], Loss: 0.0599\n",
      "Epoch [7/10], Step [736/1126], Loss: 0.0892\n",
      "Epoch [7/10], Step [740/1126], Loss: 0.0732\n",
      "Epoch [7/10], Step [744/1126], Loss: 0.0676\n",
      "Epoch [7/10], Step [748/1126], Loss: 0.0547\n",
      "Epoch [7/10], Step [752/1126], Loss: 0.1248\n",
      "Epoch [7/10], Step [756/1126], Loss: 0.0748\n",
      "Epoch [7/10], Step [760/1126], Loss: 0.0842\n",
      "Epoch [7/10], Step [764/1126], Loss: 0.0772\n",
      "Epoch [7/10], Step [768/1126], Loss: 0.0789\n",
      "Epoch [7/10], Step [772/1126], Loss: 0.0593\n",
      "Epoch [7/10], Step [776/1126], Loss: 0.0445\n",
      "Epoch [7/10], Step [780/1126], Loss: 0.0602\n",
      "Epoch [7/10], Step [784/1126], Loss: 0.0636\n",
      "Epoch [7/10], Step [788/1126], Loss: 0.0642\n",
      "Epoch [7/10], Step [792/1126], Loss: 0.0324\n",
      "Epoch [7/10], Step [796/1126], Loss: 0.0241\n",
      "Epoch [7/10], Step [800/1126], Loss: 0.0403\n",
      "Epoch [7/10], Step [804/1126], Loss: 0.0392\n",
      "Epoch [7/10], Step [808/1126], Loss: 0.0407\n",
      "Epoch [7/10], Step [812/1126], Loss: 0.0455\n",
      "Epoch [7/10], Step [816/1126], Loss: 0.0453\n",
      "Epoch [7/10], Step [820/1126], Loss: 0.0407\n",
      "Epoch [7/10], Step [824/1126], Loss: 0.0828\n",
      "Epoch [7/10], Step [828/1126], Loss: 0.0399\n",
      "Epoch [7/10], Step [832/1126], Loss: 0.1495\n",
      "Epoch [7/10], Step [836/1126], Loss: 0.0648\n",
      "Epoch [7/10], Step [840/1126], Loss: 0.0286\n",
      "Epoch [7/10], Step [844/1126], Loss: 0.0864\n",
      "Epoch [7/10], Step [848/1126], Loss: 0.0791\n",
      "Epoch [7/10], Step [852/1126], Loss: 0.0566\n",
      "Epoch [7/10], Step [856/1126], Loss: 0.0545\n",
      "Epoch [7/10], Step [860/1126], Loss: 0.0343\n",
      "Epoch [7/10], Step [864/1126], Loss: 0.0499\n",
      "Epoch [7/10], Step [868/1126], Loss: 0.0799\n",
      "Epoch [7/10], Step [872/1126], Loss: 0.0772\n",
      "Epoch [7/10], Step [876/1126], Loss: 0.0862\n",
      "Epoch [7/10], Step [880/1126], Loss: 0.0630\n",
      "Epoch [7/10], Step [884/1126], Loss: 0.0611\n",
      "Epoch [7/10], Step [888/1126], Loss: 0.1105\n",
      "Epoch [7/10], Step [892/1126], Loss: 0.0848\n",
      "Epoch [7/10], Step [896/1126], Loss: 0.0811\n",
      "Epoch [7/10], Step [900/1126], Loss: 0.0620\n",
      "Epoch [7/10], Step [904/1126], Loss: 0.1580\n",
      "Epoch [7/10], Step [908/1126], Loss: 0.0530\n",
      "Epoch [7/10], Step [912/1126], Loss: 0.0339\n",
      "Epoch [7/10], Step [916/1126], Loss: 0.1090\n",
      "Epoch [7/10], Step [920/1126], Loss: 0.0426\n",
      "Epoch [7/10], Step [924/1126], Loss: 0.0206\n",
      "Epoch [7/10], Step [928/1126], Loss: 0.0344\n",
      "Epoch [7/10], Step [932/1126], Loss: 0.0372\n",
      "Epoch [7/10], Step [936/1126], Loss: 0.0489\n",
      "Epoch [7/10], Step [940/1126], Loss: 0.0339\n",
      "Epoch [7/10], Step [944/1126], Loss: 0.0974\n",
      "Epoch [7/10], Step [948/1126], Loss: 0.0763\n",
      "Epoch [7/10], Step [952/1126], Loss: 0.0311\n",
      "Epoch [7/10], Step [956/1126], Loss: 0.0458\n",
      "Epoch [7/10], Step [960/1126], Loss: 0.1072\n",
      "Epoch [7/10], Step [964/1126], Loss: 0.0116\n",
      "Epoch [7/10], Step [968/1126], Loss: 0.1127\n",
      "Epoch [7/10], Step [972/1126], Loss: 0.0192\n",
      "Epoch [7/10], Step [976/1126], Loss: 0.0461\n",
      "Epoch [7/10], Step [980/1126], Loss: 0.0223\n",
      "Epoch [7/10], Step [984/1126], Loss: 0.0344\n",
      "Epoch [7/10], Step [988/1126], Loss: 0.0343\n",
      "Epoch [7/10], Step [992/1126], Loss: 0.1838\n",
      "Epoch [7/10], Step [996/1126], Loss: 0.0475\n",
      "Epoch [7/10], Step [1000/1126], Loss: 0.1002\n",
      "Epoch [7/10], Step [1004/1126], Loss: 0.0865\n",
      "Epoch [7/10], Step [1008/1126], Loss: 0.0611\n",
      "Epoch [7/10], Step [1012/1126], Loss: 0.0453\n",
      "Epoch [7/10], Step [1016/1126], Loss: 0.0561\n",
      "Epoch [7/10], Step [1020/1126], Loss: 0.0660\n",
      "Epoch [7/10], Step [1024/1126], Loss: 0.0942\n",
      "Epoch [7/10], Step [1028/1126], Loss: 0.0862\n",
      "Epoch [7/10], Step [1032/1126], Loss: 0.0495\n",
      "Epoch [7/10], Step [1036/1126], Loss: 0.0530\n",
      "Epoch [7/10], Step [1040/1126], Loss: 0.0728\n",
      "Epoch [7/10], Step [1044/1126], Loss: 0.0245\n",
      "Epoch [7/10], Step [1048/1126], Loss: 0.0935\n",
      "Epoch [7/10], Step [1052/1126], Loss: 0.0931\n",
      "Epoch [7/10], Step [1056/1126], Loss: 0.0633\n",
      "Epoch [7/10], Step [1060/1126], Loss: 0.0344\n",
      "Epoch [7/10], Step [1064/1126], Loss: 0.1285\n",
      "Epoch [7/10], Step [1068/1126], Loss: 0.0463\n",
      "Epoch [7/10], Step [1072/1126], Loss: 0.2000\n",
      "Epoch [7/10], Step [1076/1126], Loss: 0.0616\n",
      "Epoch [7/10], Step [1080/1126], Loss: 0.0577\n",
      "Epoch [7/10], Step [1084/1126], Loss: 0.0679\n",
      "Epoch [7/10], Step [1088/1126], Loss: 0.0507\n",
      "Epoch [7/10], Step [1092/1126], Loss: 0.0503\n",
      "Epoch [7/10], Step [1096/1126], Loss: 0.0335\n",
      "Epoch [7/10], Step [1100/1126], Loss: 0.1903\n",
      "Epoch [7/10], Step [1104/1126], Loss: 0.0813\n",
      "Epoch [7/10], Step [1108/1126], Loss: 0.0653\n",
      "Epoch [7/10], Step [1112/1126], Loss: 0.0532\n",
      "Epoch [7/10], Step [1116/1126], Loss: 0.0473\n",
      "Epoch [7/10], Step [1120/1126], Loss: 0.0288\n",
      "Epoch [7/10], Step [1124/1126], Loss: 0.0710\n",
      "Epoch [8/10], Step [4/1126], Loss: 0.5991\n",
      "Epoch [8/10], Step [8/1126], Loss: 0.0366\n",
      "Epoch [8/10], Step [12/1126], Loss: 0.0292\n",
      "Epoch [8/10], Step [16/1126], Loss: 0.2312\n",
      "Epoch [8/10], Step [20/1126], Loss: 0.0293\n",
      "Epoch [8/10], Step [24/1126], Loss: 0.0547\n",
      "Epoch [8/10], Step [28/1126], Loss: 0.0470\n",
      "Epoch [8/10], Step [32/1126], Loss: 0.0452\n",
      "Epoch [8/10], Step [36/1126], Loss: 0.0736\n",
      "Epoch [8/10], Step [40/1126], Loss: 0.0661\n",
      "Epoch [8/10], Step [44/1126], Loss: 0.0442\n",
      "Epoch [8/10], Step [48/1126], Loss: 0.0504\n",
      "Epoch [8/10], Step [52/1126], Loss: 0.1402\n",
      "Epoch [8/10], Step [56/1126], Loss: 0.0692\n",
      "Epoch [8/10], Step [60/1126], Loss: 0.0891\n",
      "Epoch [8/10], Step [64/1126], Loss: 0.0813\n",
      "Epoch [8/10], Step [68/1126], Loss: 0.0781\n",
      "Epoch [8/10], Step [72/1126], Loss: 0.0785\n",
      "Epoch [8/10], Step [76/1126], Loss: 0.0460\n",
      "Epoch [8/10], Step [80/1126], Loss: 0.0560\n",
      "Epoch [8/10], Step [84/1126], Loss: 0.1992\n",
      "Epoch [8/10], Step [88/1126], Loss: 0.1621\n",
      "Epoch [8/10], Step [92/1126], Loss: 0.1492\n",
      "Epoch [8/10], Step [96/1126], Loss: 0.0989\n",
      "Epoch [8/10], Step [100/1126], Loss: 0.1366\n",
      "Epoch [8/10], Step [104/1126], Loss: 0.0866\n",
      "Epoch [8/10], Step [108/1126], Loss: 0.0290\n",
      "Epoch [8/10], Step [112/1126], Loss: 0.0407\n",
      "Epoch [8/10], Step [116/1126], Loss: 0.1100\n",
      "Epoch [8/10], Step [120/1126], Loss: 0.0838\n",
      "Epoch [8/10], Step [124/1126], Loss: 0.0420\n",
      "Epoch [8/10], Step [128/1126], Loss: 0.0762\n",
      "Epoch [8/10], Step [132/1126], Loss: 0.0460\n",
      "Epoch [8/10], Step [136/1126], Loss: 0.0471\n",
      "Epoch [8/10], Step [140/1126], Loss: 0.0918\n",
      "Epoch [8/10], Step [144/1126], Loss: 0.1255\n",
      "Epoch [8/10], Step [148/1126], Loss: 0.0823\n",
      "Epoch [8/10], Step [152/1126], Loss: 0.0918\n",
      "Epoch [8/10], Step [156/1126], Loss: 0.0289\n",
      "Epoch [8/10], Step [160/1126], Loss: 0.1335\n",
      "Epoch [8/10], Step [164/1126], Loss: 0.0395\n",
      "Epoch [8/10], Step [168/1126], Loss: 0.0491\n",
      "Epoch [8/10], Step [172/1126], Loss: 0.0468\n",
      "Epoch [8/10], Step [176/1126], Loss: 0.0411\n",
      "Epoch [8/10], Step [180/1126], Loss: 0.0503\n",
      "Epoch [8/10], Step [184/1126], Loss: 0.1548\n",
      "Epoch [8/10], Step [188/1126], Loss: 0.1209\n",
      "Epoch [8/10], Step [192/1126], Loss: 0.0779\n",
      "Epoch [8/10], Step [196/1126], Loss: 0.0703\n",
      "Epoch [8/10], Step [200/1126], Loss: 0.0645\n",
      "Epoch [8/10], Step [204/1126], Loss: 0.0581\n",
      "Epoch [8/10], Step [208/1126], Loss: 0.0657\n",
      "Epoch [8/10], Step [212/1126], Loss: 0.0496\n",
      "Epoch [8/10], Step [216/1126], Loss: 0.0629\n",
      "Epoch [8/10], Step [220/1126], Loss: 0.0965\n",
      "Epoch [8/10], Step [224/1126], Loss: 0.1003\n",
      "Epoch [8/10], Step [228/1126], Loss: 0.0563\n",
      "Epoch [8/10], Step [232/1126], Loss: 0.1882\n",
      "Epoch [8/10], Step [236/1126], Loss: 0.0891\n",
      "Epoch [8/10], Step [240/1126], Loss: 0.0728\n",
      "Epoch [8/10], Step [244/1126], Loss: 0.0629\n",
      "Epoch [8/10], Step [248/1126], Loss: 0.0763\n",
      "Epoch [8/10], Step [252/1126], Loss: 0.0645\n",
      "Epoch [8/10], Step [256/1126], Loss: 0.0468\n",
      "Epoch [8/10], Step [260/1126], Loss: 0.0827\n",
      "Epoch [8/10], Step [264/1126], Loss: 0.0373\n",
      "Epoch [8/10], Step [268/1126], Loss: 0.1188\n",
      "Epoch [8/10], Step [272/1126], Loss: 0.0298\n",
      "Epoch [8/10], Step [276/1126], Loss: 0.0253\n",
      "Epoch [8/10], Step [280/1126], Loss: 0.0301\n",
      "Epoch [8/10], Step [284/1126], Loss: 0.0551\n",
      "Epoch [8/10], Step [288/1126], Loss: 0.0817\n",
      "Epoch [8/10], Step [292/1126], Loss: 0.0409\n",
      "Epoch [8/10], Step [296/1126], Loss: 0.0512\n",
      "Epoch [8/10], Step [300/1126], Loss: 0.0449\n",
      "Epoch [8/10], Step [304/1126], Loss: 0.0355\n",
      "Epoch [8/10], Step [308/1126], Loss: 0.0451\n",
      "Epoch [8/10], Step [312/1126], Loss: 0.0376\n",
      "Epoch [8/10], Step [316/1126], Loss: 0.0566\n",
      "Epoch [8/10], Step [320/1126], Loss: 0.0433\n",
      "Epoch [8/10], Step [324/1126], Loss: 0.1039\n",
      "Epoch [8/10], Step [328/1126], Loss: 0.0510\n",
      "Epoch [8/10], Step [332/1126], Loss: 0.0441\n",
      "Epoch [8/10], Step [336/1126], Loss: 0.0844\n",
      "Epoch [8/10], Step [340/1126], Loss: 0.1792\n",
      "Epoch [8/10], Step [344/1126], Loss: 0.0301\n",
      "Epoch [8/10], Step [348/1126], Loss: 0.0476\n",
      "Epoch [8/10], Step [352/1126], Loss: 0.2450\n",
      "Epoch [8/10], Step [356/1126], Loss: 0.0101\n",
      "Epoch [8/10], Step [360/1126], Loss: 0.0242\n",
      "Epoch [8/10], Step [364/1126], Loss: 0.0373\n",
      "Epoch [8/10], Step [368/1126], Loss: 0.1370\n",
      "Epoch [8/10], Step [372/1126], Loss: 0.0664\n",
      "Epoch [8/10], Step [376/1126], Loss: 0.1949\n",
      "Epoch [8/10], Step [380/1126], Loss: 0.0538\n",
      "Epoch [8/10], Step [384/1126], Loss: 0.0629\n",
      "Epoch [8/10], Step [388/1126], Loss: 0.0443\n",
      "Epoch [8/10], Step [392/1126], Loss: 0.0535\n",
      "Epoch [8/10], Step [396/1126], Loss: 0.0595\n",
      "Epoch [8/10], Step [400/1126], Loss: 0.0262\n",
      "Epoch [8/10], Step [404/1126], Loss: 0.0993\n",
      "Epoch [8/10], Step [408/1126], Loss: 0.1012\n",
      "Epoch [8/10], Step [412/1126], Loss: 0.0828\n",
      "Epoch [8/10], Step [416/1126], Loss: 0.0298\n",
      "Epoch [8/10], Step [420/1126], Loss: 0.0537\n",
      "Epoch [8/10], Step [424/1126], Loss: 0.0309\n",
      "Epoch [8/10], Step [428/1126], Loss: 0.3106\n",
      "Epoch [8/10], Step [432/1126], Loss: 0.0711\n",
      "Epoch [8/10], Step [436/1126], Loss: 0.0338\n",
      "Epoch [8/10], Step [440/1126], Loss: 0.0479\n",
      "Epoch [8/10], Step [444/1126], Loss: 0.0504\n",
      "Epoch [8/10], Step [448/1126], Loss: 0.0433\n",
      "Epoch [8/10], Step [452/1126], Loss: 0.0456\n",
      "Epoch [8/10], Step [456/1126], Loss: 0.0340\n",
      "Epoch [8/10], Step [460/1126], Loss: 0.0336\n",
      "Epoch [8/10], Step [464/1126], Loss: 0.0420\n",
      "Epoch [8/10], Step [468/1126], Loss: 0.0574\n",
      "Epoch [8/10], Step [472/1126], Loss: 0.0517\n",
      "Epoch [8/10], Step [476/1126], Loss: 0.0802\n",
      "Epoch [8/10], Step [480/1126], Loss: 0.0477\n",
      "Epoch [8/10], Step [484/1126], Loss: 0.0767\n",
      "Epoch [8/10], Step [488/1126], Loss: 0.0537\n",
      "Epoch [8/10], Step [492/1126], Loss: 0.0856\n",
      "Epoch [8/10], Step [496/1126], Loss: 0.0709\n",
      "Epoch [8/10], Step [500/1126], Loss: 0.0489\n",
      "Epoch [8/10], Step [504/1126], Loss: 0.0534\n",
      "Epoch [8/10], Step [508/1126], Loss: 0.0343\n",
      "Epoch [8/10], Step [512/1126], Loss: 0.0787\n",
      "Epoch [8/10], Step [516/1126], Loss: 0.0384\n",
      "Epoch [8/10], Step [520/1126], Loss: 0.0520\n",
      "Epoch [8/10], Step [524/1126], Loss: 0.0621\n",
      "Epoch [8/10], Step [528/1126], Loss: 0.0336\n",
      "Epoch [8/10], Step [532/1126], Loss: 0.1197\n",
      "Epoch [8/10], Step [536/1126], Loss: 0.0666\n",
      "Epoch [8/10], Step [540/1126], Loss: 0.0621\n",
      "Epoch [8/10], Step [544/1126], Loss: 0.0490\n",
      "Epoch [8/10], Step [548/1126], Loss: 0.0460\n",
      "Epoch [8/10], Step [552/1126], Loss: 0.0632\n",
      "Epoch [8/10], Step [556/1126], Loss: 0.0739\n",
      "Epoch [8/10], Step [560/1126], Loss: 0.0602\n",
      "Epoch [8/10], Step [564/1126], Loss: 0.0786\n",
      "Epoch [8/10], Step [568/1126], Loss: 0.1471\n",
      "Epoch [8/10], Step [572/1126], Loss: 0.0998\n",
      "Epoch [8/10], Step [576/1126], Loss: 0.0408\n",
      "Epoch [8/10], Step [580/1126], Loss: 0.0985\n",
      "Epoch [8/10], Step [584/1126], Loss: 0.0493\n",
      "Epoch [8/10], Step [588/1126], Loss: 0.0606\n",
      "Epoch [8/10], Step [592/1126], Loss: 0.0652\n",
      "Epoch [8/10], Step [596/1126], Loss: 0.0690\n",
      "Epoch [8/10], Step [600/1126], Loss: 0.1301\n",
      "Epoch [8/10], Step [604/1126], Loss: 0.0435\n",
      "Epoch [8/10], Step [608/1126], Loss: 0.0785\n",
      "Epoch [8/10], Step [612/1126], Loss: 0.0438\n",
      "Epoch [8/10], Step [616/1126], Loss: 0.0453\n",
      "Epoch [8/10], Step [620/1126], Loss: 0.0707\n",
      "Epoch [8/10], Step [624/1126], Loss: 0.1064\n",
      "Epoch [8/10], Step [628/1126], Loss: 0.0308\n",
      "Epoch [8/10], Step [632/1126], Loss: 0.0882\n",
      "Epoch [8/10], Step [636/1126], Loss: 0.0379\n",
      "Epoch [8/10], Step [640/1126], Loss: 0.0319\n",
      "Epoch [8/10], Step [644/1126], Loss: 0.0439\n",
      "Epoch [8/10], Step [648/1126], Loss: 0.0844\n",
      "Epoch [8/10], Step [652/1126], Loss: 0.1608\n",
      "Epoch [8/10], Step [656/1126], Loss: 0.0364\n",
      "Epoch [8/10], Step [660/1126], Loss: 0.0785\n",
      "Epoch [8/10], Step [664/1126], Loss: 0.0329\n",
      "Epoch [8/10], Step [668/1126], Loss: 0.0552\n",
      "Epoch [8/10], Step [672/1126], Loss: 0.0571\n",
      "Epoch [8/10], Step [676/1126], Loss: 0.0538\n",
      "Epoch [8/10], Step [680/1126], Loss: 0.0333\n",
      "Epoch [8/10], Step [684/1126], Loss: 0.0639\n",
      "Epoch [8/10], Step [688/1126], Loss: 0.0408\n",
      "Epoch [8/10], Step [692/1126], Loss: 0.0577\n",
      "Epoch [8/10], Step [696/1126], Loss: 0.1371\n",
      "Epoch [8/10], Step [700/1126], Loss: 0.0519\n",
      "Epoch [8/10], Step [704/1126], Loss: 0.0460\n",
      "Epoch [8/10], Step [708/1126], Loss: 0.0809\n",
      "Epoch [8/10], Step [712/1126], Loss: 0.0434\n",
      "Epoch [8/10], Step [716/1126], Loss: 0.1706\n",
      "Epoch [8/10], Step [720/1126], Loss: 0.1481\n",
      "Epoch [8/10], Step [724/1126], Loss: 0.0531\n",
      "Epoch [8/10], Step [728/1126], Loss: 0.0944\n",
      "Epoch [8/10], Step [732/1126], Loss: 0.0660\n",
      "Epoch [8/10], Step [736/1126], Loss: 0.1997\n",
      "Epoch [8/10], Step [740/1126], Loss: 0.0563\n",
      "Epoch [8/10], Step [744/1126], Loss: 0.1017\n",
      "Epoch [8/10], Step [748/1126], Loss: 0.0874\n",
      "Epoch [8/10], Step [752/1126], Loss: 0.1016\n",
      "Epoch [8/10], Step [756/1126], Loss: 0.0617\n",
      "Epoch [8/10], Step [760/1126], Loss: 0.0752\n",
      "Epoch [8/10], Step [764/1126], Loss: 0.0639\n",
      "Epoch [8/10], Step [768/1126], Loss: 0.0702\n",
      "Epoch [8/10], Step [772/1126], Loss: 0.0592\n",
      "Epoch [8/10], Step [776/1126], Loss: 0.0742\n",
      "Epoch [8/10], Step [780/1126], Loss: 0.0847\n",
      "Epoch [8/10], Step [784/1126], Loss: 0.0735\n",
      "Epoch [8/10], Step [788/1126], Loss: 0.0940\n",
      "Epoch [8/10], Step [792/1126], Loss: 0.0591\n",
      "Epoch [8/10], Step [796/1126], Loss: 0.0354\n",
      "Epoch [8/10], Step [800/1126], Loss: 0.0443\n",
      "Epoch [8/10], Step [804/1126], Loss: 0.0787\n",
      "Epoch [8/10], Step [808/1126], Loss: 0.0328\n",
      "Epoch [8/10], Step [812/1126], Loss: 0.0889\n",
      "Epoch [8/10], Step [816/1126], Loss: 0.0582\n",
      "Epoch [8/10], Step [820/1126], Loss: 0.0463\n",
      "Epoch [8/10], Step [824/1126], Loss: 0.1123\n",
      "Epoch [8/10], Step [828/1126], Loss: 0.0376\n",
      "Epoch [8/10], Step [832/1126], Loss: 0.1317\n",
      "Epoch [8/10], Step [836/1126], Loss: 0.0622\n",
      "Epoch [8/10], Step [840/1126], Loss: 0.0237\n",
      "Epoch [8/10], Step [844/1126], Loss: 0.0786\n",
      "Epoch [8/10], Step [848/1126], Loss: 0.1583\n",
      "Epoch [8/10], Step [852/1126], Loss: 0.0499\n",
      "Epoch [8/10], Step [856/1126], Loss: 0.0423\n",
      "Epoch [8/10], Step [860/1126], Loss: 0.0682\n",
      "Epoch [8/10], Step [864/1126], Loss: 0.0831\n",
      "Epoch [8/10], Step [868/1126], Loss: 0.0731\n",
      "Epoch [8/10], Step [872/1126], Loss: 0.0486\n",
      "Epoch [8/10], Step [876/1126], Loss: 0.0352\n",
      "Epoch [8/10], Step [880/1126], Loss: 0.0565\n",
      "Epoch [8/10], Step [884/1126], Loss: 0.0578\n",
      "Epoch [8/10], Step [888/1126], Loss: 0.1285\n",
      "Epoch [8/10], Step [892/1126], Loss: 0.0542\n",
      "Epoch [8/10], Step [896/1126], Loss: 0.0643\n",
      "Epoch [8/10], Step [900/1126], Loss: 0.0303\n",
      "Epoch [8/10], Step [904/1126], Loss: 0.0494\n",
      "Epoch [8/10], Step [908/1126], Loss: 0.0767\n",
      "Epoch [8/10], Step [912/1126], Loss: 0.0235\n",
      "Epoch [8/10], Step [916/1126], Loss: 0.1191\n",
      "Epoch [8/10], Step [920/1126], Loss: 0.0538\n",
      "Epoch [8/10], Step [924/1126], Loss: 0.0403\n",
      "Epoch [8/10], Step [928/1126], Loss: 0.0248\n",
      "Epoch [8/10], Step [932/1126], Loss: 0.0176\n",
      "Epoch [8/10], Step [936/1126], Loss: 0.0405\n",
      "Epoch [8/10], Step [940/1126], Loss: 0.0316\n",
      "Epoch [8/10], Step [944/1126], Loss: 0.0359\n",
      "Epoch [8/10], Step [948/1126], Loss: 0.1223\n",
      "Epoch [8/10], Step [952/1126], Loss: 0.0990\n",
      "Epoch [8/10], Step [956/1126], Loss: 0.0527\n",
      "Epoch [8/10], Step [960/1126], Loss: 0.0503\n",
      "Epoch [8/10], Step [964/1126], Loss: 0.0474\n",
      "Epoch [8/10], Step [968/1126], Loss: 0.0555\n",
      "Epoch [8/10], Step [972/1126], Loss: 0.0519\n",
      "Epoch [8/10], Step [976/1126], Loss: 0.0941\n",
      "Epoch [8/10], Step [980/1126], Loss: 0.0308\n",
      "Epoch [8/10], Step [984/1126], Loss: 0.0969\n",
      "Epoch [8/10], Step [988/1126], Loss: 0.0431\n",
      "Epoch [8/10], Step [992/1126], Loss: 0.0385\n",
      "Epoch [8/10], Step [996/1126], Loss: 0.0453\n",
      "Epoch [8/10], Step [1000/1126], Loss: 0.0380\n",
      "Epoch [8/10], Step [1004/1126], Loss: 0.0697\n",
      "Epoch [8/10], Step [1008/1126], Loss: 0.0434\n",
      "Epoch [8/10], Step [1012/1126], Loss: 0.0864\n",
      "Epoch [8/10], Step [1016/1126], Loss: 0.0330\n",
      "Epoch [8/10], Step [1020/1126], Loss: 0.0467\n",
      "Epoch [8/10], Step [1024/1126], Loss: 0.1033\n",
      "Epoch [8/10], Step [1028/1126], Loss: 0.0416\n",
      "Epoch [8/10], Step [1032/1126], Loss: 0.0380\n",
      "Epoch [8/10], Step [1036/1126], Loss: 0.0798\n",
      "Epoch [8/10], Step [1040/1126], Loss: 0.0766\n",
      "Epoch [8/10], Step [1044/1126], Loss: 0.0388\n",
      "Epoch [8/10], Step [1048/1126], Loss: 0.0454\n",
      "Epoch [8/10], Step [1052/1126], Loss: 0.0961\n",
      "Epoch [8/10], Step [1056/1126], Loss: 0.0784\n",
      "Epoch [8/10], Step [1060/1126], Loss: 0.0417\n",
      "Epoch [8/10], Step [1064/1126], Loss: 0.1102\n",
      "Epoch [8/10], Step [1068/1126], Loss: 0.0466\n",
      "Epoch [8/10], Step [1072/1126], Loss: 0.0789\n",
      "Epoch [8/10], Step [1076/1126], Loss: 0.0418\n",
      "Epoch [8/10], Step [1080/1126], Loss: 0.0740\n",
      "Epoch [8/10], Step [1084/1126], Loss: 0.0412\n",
      "Epoch [8/10], Step [1088/1126], Loss: 0.0458\n",
      "Epoch [8/10], Step [1092/1126], Loss: 0.1333\n",
      "Epoch [8/10], Step [1096/1126], Loss: 0.0279\n",
      "Epoch [8/10], Step [1100/1126], Loss: 0.0518\n",
      "Epoch [8/10], Step [1104/1126], Loss: 0.1107\n",
      "Epoch [8/10], Step [1108/1126], Loss: 0.0624\n",
      "Epoch [8/10], Step [1112/1126], Loss: 0.0496\n",
      "Epoch [8/10], Step [1116/1126], Loss: 0.0596\n",
      "Epoch [8/10], Step [1120/1126], Loss: 0.0363\n",
      "Epoch [8/10], Step [1124/1126], Loss: 0.0730\n",
      "Epoch [9/10], Step [4/1126], Loss: 0.0784\n",
      "Epoch [9/10], Step [8/1126], Loss: 0.0391\n",
      "Epoch [9/10], Step [12/1126], Loss: 0.0369\n",
      "Epoch [9/10], Step [16/1126], Loss: 0.0547\n",
      "Epoch [9/10], Step [20/1126], Loss: 0.0555\n",
      "Epoch [9/10], Step [24/1126], Loss: 0.0359\n",
      "Epoch [9/10], Step [28/1126], Loss: 0.0188\n",
      "Epoch [9/10], Step [32/1126], Loss: 0.1018\n",
      "Epoch [9/10], Step [36/1126], Loss: 0.0968\n",
      "Epoch [9/10], Step [40/1126], Loss: 0.0567\n",
      "Epoch [9/10], Step [44/1126], Loss: 0.0689\n",
      "Epoch [9/10], Step [48/1126], Loss: 0.0624\n",
      "Epoch [9/10], Step [52/1126], Loss: 0.0467\n",
      "Epoch [9/10], Step [56/1126], Loss: 0.0756\n",
      "Epoch [9/10], Step [60/1126], Loss: 0.0514\n",
      "Epoch [9/10], Step [64/1126], Loss: 0.2041\n",
      "Epoch [9/10], Step [68/1126], Loss: 0.0682\n",
      "Epoch [9/10], Step [72/1126], Loss: 0.0586\n",
      "Epoch [9/10], Step [76/1126], Loss: 0.0470\n",
      "Epoch [9/10], Step [80/1126], Loss: 0.0442\n",
      "Epoch [9/10], Step [84/1126], Loss: 0.0798\n",
      "Epoch [9/10], Step [88/1126], Loss: 0.0470\n",
      "Epoch [9/10], Step [92/1126], Loss: 0.0595\n",
      "Epoch [9/10], Step [96/1126], Loss: 0.0934\n",
      "Epoch [9/10], Step [100/1126], Loss: 0.0403\n",
      "Epoch [9/10], Step [104/1126], Loss: 0.0370\n",
      "Epoch [9/10], Step [108/1126], Loss: 0.0464\n",
      "Epoch [9/10], Step [112/1126], Loss: 0.0304\n",
      "Epoch [9/10], Step [116/1126], Loss: 0.0677\n",
      "Epoch [9/10], Step [120/1126], Loss: 0.0497\n",
      "Epoch [9/10], Step [124/1126], Loss: 0.0300\n",
      "Epoch [9/10], Step [128/1126], Loss: 0.0585\n",
      "Epoch [9/10], Step [132/1126], Loss: 0.1502\n",
      "Epoch [9/10], Step [136/1126], Loss: 0.0205\n",
      "Epoch [9/10], Step [140/1126], Loss: 0.0361\n",
      "Epoch [9/10], Step [144/1126], Loss: 0.0400\n",
      "Epoch [9/10], Step [148/1126], Loss: 0.0548\n",
      "Epoch [9/10], Step [152/1126], Loss: 0.0765\n",
      "Epoch [9/10], Step [156/1126], Loss: 0.0386\n",
      "Epoch [9/10], Step [160/1126], Loss: 0.0354\n",
      "Epoch [9/10], Step [164/1126], Loss: 0.0453\n",
      "Epoch [9/10], Step [168/1126], Loss: 0.0497\n",
      "Epoch [9/10], Step [172/1126], Loss: 0.0537\n",
      "Epoch [9/10], Step [176/1126], Loss: 0.0457\n",
      "Epoch [9/10], Step [180/1126], Loss: 0.0413\n",
      "Epoch [9/10], Step [184/1126], Loss: 0.0446\n",
      "Epoch [9/10], Step [188/1126], Loss: 0.0576\n",
      "Epoch [9/10], Step [192/1126], Loss: 0.0770\n",
      "Epoch [9/10], Step [196/1126], Loss: 0.0445\n",
      "Epoch [9/10], Step [200/1126], Loss: 0.0564\n",
      "Epoch [9/10], Step [204/1126], Loss: 0.0944\n",
      "Epoch [9/10], Step [208/1126], Loss: 0.0530\n",
      "Epoch [9/10], Step [212/1126], Loss: 0.0365\n",
      "Epoch [9/10], Step [216/1126], Loss: 0.0592\n",
      "Epoch [9/10], Step [220/1126], Loss: 0.0513\n",
      "Epoch [9/10], Step [224/1126], Loss: 0.1049\n",
      "Epoch [9/10], Step [228/1126], Loss: 0.0527\n",
      "Epoch [9/10], Step [232/1126], Loss: 0.0829\n",
      "Epoch [9/10], Step [236/1126], Loss: 0.0449\n",
      "Epoch [9/10], Step [240/1126], Loss: 0.0500\n",
      "Epoch [9/10], Step [244/1126], Loss: 0.0502\n",
      "Epoch [9/10], Step [248/1126], Loss: 0.0430\n",
      "Epoch [9/10], Step [252/1126], Loss: 0.0293\n",
      "Epoch [9/10], Step [256/1126], Loss: 0.0444\n",
      "Epoch [9/10], Step [260/1126], Loss: 0.0368\n",
      "Epoch [9/10], Step [264/1126], Loss: 0.0189\n",
      "Epoch [9/10], Step [268/1126], Loss: 0.0586\n",
      "Epoch [9/10], Step [272/1126], Loss: 0.0161\n",
      "Epoch [9/10], Step [276/1126], Loss: 0.0147\n",
      "Epoch [9/10], Step [280/1126], Loss: 0.0159\n",
      "Epoch [9/10], Step [284/1126], Loss: 0.1861\n",
      "Epoch [9/10], Step [288/1126], Loss: 0.0644\n",
      "Epoch [9/10], Step [292/1126], Loss: 0.0505\n",
      "Epoch [9/10], Step [296/1126], Loss: 0.0296\n",
      "Epoch [9/10], Step [300/1126], Loss: 0.0346\n",
      "Epoch [9/10], Step [304/1126], Loss: 0.0705\n",
      "Epoch [9/10], Step [308/1126], Loss: 0.0653\n",
      "Epoch [9/10], Step [312/1126], Loss: 0.0285\n",
      "Epoch [9/10], Step [316/1126], Loss: 0.0537\n",
      "Epoch [9/10], Step [320/1126], Loss: 0.0450\n",
      "Epoch [9/10], Step [324/1126], Loss: 0.0483\n",
      "Epoch [9/10], Step [328/1126], Loss: 0.0248\n",
      "Epoch [9/10], Step [332/1126], Loss: 0.0388\n",
      "Epoch [9/10], Step [336/1126], Loss: 0.0719\n",
      "Epoch [9/10], Step [340/1126], Loss: 0.0346\n",
      "Epoch [9/10], Step [344/1126], Loss: 0.0232\n",
      "Epoch [9/10], Step [348/1126], Loss: 0.0599\n",
      "Epoch [9/10], Step [352/1126], Loss: 0.0389\n",
      "Epoch [9/10], Step [356/1126], Loss: 0.0133\n",
      "Epoch [9/10], Step [360/1126], Loss: 0.0122\n",
      "Epoch [9/10], Step [364/1126], Loss: 0.0497\n",
      "Epoch [9/10], Step [368/1126], Loss: 0.1007\n",
      "Epoch [9/10], Step [372/1126], Loss: 0.0717\n",
      "Epoch [9/10], Step [376/1126], Loss: 0.0947\n",
      "Epoch [9/10], Step [380/1126], Loss: 0.0431\n",
      "Epoch [9/10], Step [384/1126], Loss: 0.0639\n",
      "Epoch [9/10], Step [388/1126], Loss: 0.0458\n",
      "Epoch [9/10], Step [392/1126], Loss: 0.0263\n",
      "Epoch [9/10], Step [396/1126], Loss: 0.0311\n",
      "Epoch [9/10], Step [400/1126], Loss: 0.0179\n",
      "Epoch [9/10], Step [404/1126], Loss: 0.0455\n",
      "Epoch [9/10], Step [408/1126], Loss: 0.0955\n",
      "Epoch [9/10], Step [412/1126], Loss: 0.0410\n",
      "Epoch [9/10], Step [416/1126], Loss: 0.0454\n",
      "Epoch [9/10], Step [420/1126], Loss: 0.0449\n",
      "Epoch [9/10], Step [424/1126], Loss: 0.1574\n",
      "Epoch [9/10], Step [428/1126], Loss: 0.1446\n",
      "Epoch [9/10], Step [432/1126], Loss: 0.1018\n",
      "Epoch [9/10], Step [436/1126], Loss: 0.0555\n",
      "Epoch [9/10], Step [440/1126], Loss: 0.0469\n",
      "Epoch [9/10], Step [444/1126], Loss: 0.0491\n",
      "Epoch [9/10], Step [448/1126], Loss: 0.0463\n",
      "Epoch [9/10], Step [452/1126], Loss: 0.0331\n",
      "Epoch [9/10], Step [456/1126], Loss: 0.0300\n",
      "Epoch [9/10], Step [460/1126], Loss: 0.0289\n",
      "Epoch [9/10], Step [464/1126], Loss: 0.0611\n",
      "Epoch [9/10], Step [468/1126], Loss: 0.0468\n",
      "Epoch [9/10], Step [472/1126], Loss: 0.0431\n",
      "Epoch [9/10], Step [476/1126], Loss: 0.0399\n",
      "Epoch [9/10], Step [480/1126], Loss: 0.0362\n",
      "Epoch [9/10], Step [484/1126], Loss: 0.0515\n",
      "Epoch [9/10], Step [488/1126], Loss: 0.0552\n",
      "Epoch [9/10], Step [492/1126], Loss: 0.0288\n",
      "Epoch [9/10], Step [496/1126], Loss: 0.0633\n",
      "Epoch [9/10], Step [500/1126], Loss: 0.0508\n",
      "Epoch [9/10], Step [504/1126], Loss: 0.0316\n",
      "Epoch [9/10], Step [508/1126], Loss: 0.0336\n",
      "Epoch [9/10], Step [512/1126], Loss: 0.1253\n",
      "Epoch [9/10], Step [516/1126], Loss: 0.0296\n",
      "Epoch [9/10], Step [520/1126], Loss: 0.0400\n",
      "Epoch [9/10], Step [524/1126], Loss: 0.0704\n",
      "Epoch [9/10], Step [528/1126], Loss: 0.0384\n",
      "Epoch [9/10], Step [532/1126], Loss: 0.0634\n",
      "Epoch [9/10], Step [536/1126], Loss: 0.0354\n",
      "Epoch [9/10], Step [540/1126], Loss: 0.1070\n",
      "Epoch [9/10], Step [544/1126], Loss: 0.0314\n",
      "Epoch [9/10], Step [548/1126], Loss: 0.0456\n",
      "Epoch [9/10], Step [552/1126], Loss: 0.0615\n",
      "Epoch [9/10], Step [556/1126], Loss: 0.0493\n",
      "Epoch [9/10], Step [560/1126], Loss: 0.0431\n",
      "Epoch [9/10], Step [564/1126], Loss: 0.0917\n",
      "Epoch [9/10], Step [568/1126], Loss: 0.0799\n",
      "Epoch [9/10], Step [572/1126], Loss: 0.0589\n",
      "Epoch [9/10], Step [576/1126], Loss: 0.0380\n",
      "Epoch [9/10], Step [580/1126], Loss: 0.0848\n",
      "Epoch [9/10], Step [584/1126], Loss: 0.0431\n",
      "Epoch [9/10], Step [588/1126], Loss: 0.0507\n",
      "Epoch [9/10], Step [592/1126], Loss: 0.0944\n",
      "Epoch [9/10], Step [596/1126], Loss: 0.0447\n",
      "Epoch [9/10], Step [600/1126], Loss: 0.0324\n",
      "Epoch [9/10], Step [604/1126], Loss: 0.0342\n",
      "Epoch [9/10], Step [608/1126], Loss: 0.0440\n",
      "Epoch [9/10], Step [612/1126], Loss: 0.0292\n",
      "Epoch [9/10], Step [616/1126], Loss: 0.0448\n",
      "Epoch [9/10], Step [620/1126], Loss: 0.1869\n",
      "Epoch [9/10], Step [624/1126], Loss: 0.0512\n",
      "Epoch [9/10], Step [628/1126], Loss: 0.0314\n",
      "Epoch [9/10], Step [632/1126], Loss: 0.0633\n",
      "Epoch [9/10], Step [636/1126], Loss: 0.0291\n",
      "Epoch [9/10], Step [640/1126], Loss: 0.0351\n",
      "Epoch [9/10], Step [644/1126], Loss: 0.0416\n",
      "Epoch [9/10], Step [648/1126], Loss: 0.0507\n",
      "Epoch [9/10], Step [652/1126], Loss: 0.0513\n",
      "Epoch [9/10], Step [656/1126], Loss: 0.0829\n",
      "Epoch [9/10], Step [660/1126], Loss: 0.0819\n",
      "Epoch [9/10], Step [664/1126], Loss: 0.0173\n",
      "Epoch [9/10], Step [668/1126], Loss: 0.0360\n",
      "Epoch [9/10], Step [672/1126], Loss: 0.0329\n",
      "Epoch [9/10], Step [676/1126], Loss: 0.0186\n",
      "Epoch [9/10], Step [680/1126], Loss: 0.0326\n",
      "Epoch [9/10], Step [684/1126], Loss: 0.0511\n",
      "Epoch [9/10], Step [688/1126], Loss: 0.0403\n",
      "Epoch [9/10], Step [692/1126], Loss: 0.0280\n",
      "Epoch [9/10], Step [696/1126], Loss: 0.0846\n",
      "Epoch [9/10], Step [700/1126], Loss: 0.0176\n",
      "Epoch [9/10], Step [704/1126], Loss: 0.0191\n",
      "Epoch [9/10], Step [708/1126], Loss: 0.1080\n",
      "Epoch [9/10], Step [712/1126], Loss: 0.0392\n",
      "Epoch [9/10], Step [716/1126], Loss: 0.0488\n",
      "Epoch [9/10], Step [720/1126], Loss: 0.0587\n",
      "Epoch [9/10], Step [724/1126], Loss: 0.0483\n",
      "Epoch [9/10], Step [728/1126], Loss: 0.1263\n",
      "Epoch [9/10], Step [732/1126], Loss: 0.0904\n",
      "Epoch [9/10], Step [736/1126], Loss: 0.0617\n",
      "Epoch [9/10], Step [740/1126], Loss: 0.0494\n",
      "Epoch [9/10], Step [744/1126], Loss: 0.0612\n",
      "Epoch [9/10], Step [748/1126], Loss: 0.0836\n",
      "Epoch [9/10], Step [752/1126], Loss: 0.1770\n",
      "Epoch [9/10], Step [756/1126], Loss: 0.0596\n",
      "Epoch [9/10], Step [760/1126], Loss: 0.0967\n",
      "Epoch [9/10], Step [764/1126], Loss: 0.0592\n",
      "Epoch [9/10], Step [768/1126], Loss: 0.0443\n",
      "Epoch [9/10], Step [772/1126], Loss: 0.0466\n",
      "Epoch [9/10], Step [776/1126], Loss: 0.0463\n",
      "Epoch [9/10], Step [780/1126], Loss: 0.0522\n",
      "Epoch [9/10], Step [784/1126], Loss: 0.0622\n",
      "Epoch [9/10], Step [788/1126], Loss: 0.0506\n",
      "Epoch [9/10], Step [792/1126], Loss: 0.0336\n",
      "Epoch [9/10], Step [796/1126], Loss: 0.0246\n",
      "Epoch [9/10], Step [800/1126], Loss: 0.0393\n",
      "Epoch [9/10], Step [804/1126], Loss: 0.0369\n",
      "Epoch [9/10], Step [808/1126], Loss: 0.0536\n",
      "Epoch [9/10], Step [812/1126], Loss: 0.0515\n",
      "Epoch [9/10], Step [816/1126], Loss: 0.0511\n",
      "Epoch [9/10], Step [820/1126], Loss: 0.0678\n",
      "Epoch [9/10], Step [824/1126], Loss: 0.1886\n",
      "Epoch [9/10], Step [828/1126], Loss: 0.0525\n",
      "Epoch [9/10], Step [832/1126], Loss: 0.0479\n",
      "Epoch [9/10], Step [836/1126], Loss: 0.0811\n",
      "Epoch [9/10], Step [840/1126], Loss: 0.0136\n",
      "Epoch [9/10], Step [844/1126], Loss: 0.0588\n",
      "Epoch [9/10], Step [848/1126], Loss: 0.0474\n",
      "Epoch [9/10], Step [852/1126], Loss: 0.0377\n",
      "Epoch [9/10], Step [856/1126], Loss: 0.0288\n",
      "Epoch [9/10], Step [860/1126], Loss: 0.0169\n",
      "Epoch [9/10], Step [864/1126], Loss: 0.0483\n",
      "Epoch [9/10], Step [868/1126], Loss: 0.0659\n",
      "Epoch [9/10], Step [872/1126], Loss: 0.0254\n",
      "Epoch [9/10], Step [876/1126], Loss: 0.0345\n",
      "Epoch [9/10], Step [880/1126], Loss: 0.0473\n",
      "Epoch [9/10], Step [884/1126], Loss: 0.0372\n",
      "Epoch [9/10], Step [888/1126], Loss: 0.0593\n",
      "Epoch [9/10], Step [892/1126], Loss: 0.0513\n",
      "Epoch [9/10], Step [896/1126], Loss: 0.0825\n",
      "Epoch [9/10], Step [900/1126], Loss: 0.0276\n",
      "Epoch [9/10], Step [904/1126], Loss: 0.0826\n",
      "Epoch [9/10], Step [908/1126], Loss: 0.0347\n",
      "Epoch [9/10], Step [912/1126], Loss: 0.0225\n",
      "Epoch [9/10], Step [916/1126], Loss: 0.1513\n",
      "Epoch [9/10], Step [920/1126], Loss: 0.0423\n",
      "Epoch [9/10], Step [924/1126], Loss: 0.0231\n",
      "Epoch [9/10], Step [928/1126], Loss: 0.0264\n",
      "Epoch [9/10], Step [932/1126], Loss: 0.0304\n",
      "Epoch [9/10], Step [936/1126], Loss: 0.0418\n",
      "Epoch [9/10], Step [940/1126], Loss: 0.0292\n",
      "Epoch [9/10], Step [944/1126], Loss: 0.0399\n",
      "Epoch [9/10], Step [948/1126], Loss: 0.0552\n",
      "Epoch [9/10], Step [952/1126], Loss: 0.0453\n",
      "Epoch [9/10], Step [956/1126], Loss: 0.0478\n",
      "Epoch [9/10], Step [960/1126], Loss: 0.0443\n",
      "Epoch [9/10], Step [964/1126], Loss: 0.0392\n",
      "Epoch [9/10], Step [968/1126], Loss: 0.0850\n",
      "Epoch [9/10], Step [972/1126], Loss: 0.0242\n",
      "Epoch [9/10], Step [976/1126], Loss: 0.0378\n",
      "Epoch [9/10], Step [980/1126], Loss: 0.0251\n",
      "Epoch [9/10], Step [984/1126], Loss: 0.3135\n",
      "Epoch [9/10], Step [988/1126], Loss: 0.0445\n",
      "Epoch [9/10], Step [992/1126], Loss: 0.0405\n",
      "Epoch [9/10], Step [996/1126], Loss: 0.0338\n",
      "Epoch [9/10], Step [1000/1126], Loss: 0.0359\n",
      "Epoch [9/10], Step [1004/1126], Loss: 0.0359\n",
      "Epoch [9/10], Step [1008/1126], Loss: 0.0483\n",
      "Epoch [9/10], Step [1012/1126], Loss: 0.0827\n",
      "Epoch [9/10], Step [1016/1126], Loss: 0.0232\n",
      "Epoch [9/10], Step [1020/1126], Loss: 0.0403\n",
      "Epoch [9/10], Step [1024/1126], Loss: 0.0854\n",
      "Epoch [9/10], Step [1028/1126], Loss: 0.0334\n",
      "Epoch [9/10], Step [1032/1126], Loss: 0.0341\n",
      "Epoch [9/10], Step [1036/1126], Loss: 0.0852\n",
      "Epoch [9/10], Step [1040/1126], Loss: 0.0564\n",
      "Epoch [9/10], Step [1044/1126], Loss: 0.0250\n",
      "Epoch [9/10], Step [1048/1126], Loss: 0.2019\n",
      "Epoch [9/10], Step [1052/1126], Loss: 0.0421\n",
      "Epoch [9/10], Step [1056/1126], Loss: 0.0613\n",
      "Epoch [9/10], Step [1060/1126], Loss: 0.0265\n",
      "Epoch [9/10], Step [1064/1126], Loss: 0.1053\n",
      "Epoch [9/10], Step [1068/1126], Loss: 0.0489\n",
      "Epoch [9/10], Step [1072/1126], Loss: 0.1254\n",
      "Epoch [9/10], Step [1076/1126], Loss: 0.0414\n",
      "Epoch [9/10], Step [1080/1126], Loss: 0.0467\n",
      "Epoch [9/10], Step [1084/1126], Loss: 0.0974\n",
      "Epoch [9/10], Step [1088/1126], Loss: 0.0335\n",
      "Epoch [9/10], Step [1092/1126], Loss: 0.0836\n",
      "Epoch [9/10], Step [1096/1126], Loss: 0.0433\n",
      "Epoch [9/10], Step [1100/1126], Loss: 0.0495\n",
      "Epoch [9/10], Step [1104/1126], Loss: 0.1070\n",
      "Epoch [9/10], Step [1108/1126], Loss: 0.0410\n",
      "Epoch [9/10], Step [1112/1126], Loss: 0.0486\n",
      "Epoch [9/10], Step [1116/1126], Loss: 0.0270\n",
      "Epoch [9/10], Step [1120/1126], Loss: 0.0521\n",
      "Epoch [9/10], Step [1124/1126], Loss: 0.0596\n",
      "Epoch [10/10], Step [4/1126], Loss: 0.0462\n",
      "Epoch [10/10], Step [8/1126], Loss: 0.0283\n",
      "Epoch [10/10], Step [12/1126], Loss: 0.0242\n",
      "Epoch [10/10], Step [16/1126], Loss: 0.0565\n",
      "Epoch [10/10], Step [20/1126], Loss: 0.0120\n",
      "Epoch [10/10], Step [24/1126], Loss: 0.0540\n",
      "Epoch [10/10], Step [28/1126], Loss: 0.0107\n",
      "Epoch [10/10], Step [32/1126], Loss: 0.0359\n",
      "Epoch [10/10], Step [36/1126], Loss: 0.0687\n",
      "Epoch [10/10], Step [40/1126], Loss: 0.0489\n",
      "Epoch [10/10], Step [44/1126], Loss: 0.0861\n",
      "Epoch [10/10], Step [48/1126], Loss: 0.0469\n",
      "Epoch [10/10], Step [52/1126], Loss: 0.0432\n",
      "Epoch [10/10], Step [56/1126], Loss: 0.0537\n",
      "Epoch [10/10], Step [60/1126], Loss: 0.0482\n",
      "Epoch [10/10], Step [64/1126], Loss: 0.0880\n",
      "Epoch [10/10], Step [68/1126], Loss: 0.0722\n",
      "Epoch [10/10], Step [72/1126], Loss: 0.0577\n",
      "Epoch [10/10], Step [76/1126], Loss: 0.0321\n",
      "Epoch [10/10], Step [80/1126], Loss: 0.0427\n",
      "Epoch [10/10], Step [84/1126], Loss: 0.1527\n",
      "Epoch [10/10], Step [88/1126], Loss: 0.0365\n",
      "Epoch [10/10], Step [92/1126], Loss: 0.0724\n",
      "Epoch [10/10], Step [96/1126], Loss: 0.0622\n",
      "Epoch [10/10], Step [100/1126], Loss: 0.0551\n",
      "Epoch [10/10], Step [104/1126], Loss: 0.0357\n",
      "Epoch [10/10], Step [108/1126], Loss: 0.0411\n",
      "Epoch [10/10], Step [112/1126], Loss: 0.0296\n",
      "Epoch [10/10], Step [116/1126], Loss: 0.0463\n",
      "Epoch [10/10], Step [120/1126], Loss: 0.0494\n",
      "Epoch [10/10], Step [124/1126], Loss: 0.0229\n",
      "Epoch [10/10], Step [128/1126], Loss: 0.0573\n",
      "Epoch [10/10], Step [132/1126], Loss: 0.0300\n",
      "Epoch [10/10], Step [136/1126], Loss: 0.0479\n",
      "Epoch [10/10], Step [140/1126], Loss: 0.0582\n",
      "Epoch [10/10], Step [144/1126], Loss: 0.1649\n",
      "Epoch [10/10], Step [148/1126], Loss: 0.1062\n",
      "Epoch [10/10], Step [152/1126], Loss: 0.0807\n",
      "Epoch [10/10], Step [156/1126], Loss: 0.0563\n",
      "Epoch [10/10], Step [160/1126], Loss: 0.0481\n",
      "Epoch [10/10], Step [164/1126], Loss: 0.0288\n",
      "Epoch [10/10], Step [168/1126], Loss: 0.0330\n",
      "Epoch [10/10], Step [172/1126], Loss: 0.0332\n",
      "Epoch [10/10], Step [176/1126], Loss: 0.0309\n",
      "Epoch [10/10], Step [180/1126], Loss: 0.0306\n",
      "Epoch [10/10], Step [184/1126], Loss: 0.0414\n",
      "Epoch [10/10], Step [188/1126], Loss: 0.0265\n",
      "Epoch [10/10], Step [192/1126], Loss: 0.0389\n",
      "Epoch [10/10], Step [196/1126], Loss: 0.0424\n",
      "Epoch [10/10], Step [200/1126], Loss: 0.0401\n",
      "Epoch [10/10], Step [204/1126], Loss: 0.0274\n",
      "Epoch [10/10], Step [208/1126], Loss: 0.0438\n",
      "Epoch [10/10], Step [212/1126], Loss: 0.0520\n",
      "Epoch [10/10], Step [216/1126], Loss: 0.0351\n",
      "Epoch [10/10], Step [220/1126], Loss: 0.0780\n",
      "Epoch [10/10], Step [224/1126], Loss: 0.1042\n",
      "Epoch [10/10], Step [228/1126], Loss: 0.0281\n",
      "Epoch [10/10], Step [232/1126], Loss: 0.0683\n",
      "Epoch [10/10], Step [236/1126], Loss: 0.0316\n",
      "Epoch [10/10], Step [240/1126], Loss: 0.0474\n",
      "Epoch [10/10], Step [244/1126], Loss: 0.0250\n",
      "Epoch [10/10], Step [248/1126], Loss: 0.1553\n",
      "Epoch [10/10], Step [252/1126], Loss: 0.0236\n",
      "Epoch [10/10], Step [256/1126], Loss: 0.0378\n",
      "Epoch [10/10], Step [260/1126], Loss: 0.0939\n",
      "Epoch [10/10], Step [264/1126], Loss: 0.0201\n",
      "Epoch [10/10], Step [268/1126], Loss: 0.0599\n",
      "Epoch [10/10], Step [272/1126], Loss: 0.0253\n",
      "Epoch [10/10], Step [276/1126], Loss: 0.0401\n",
      "Epoch [10/10], Step [280/1126], Loss: 0.0187\n",
      "Epoch [10/10], Step [284/1126], Loss: 0.0804\n",
      "Epoch [10/10], Step [288/1126], Loss: 0.0608\n",
      "Epoch [10/10], Step [292/1126], Loss: 0.0493\n",
      "Epoch [10/10], Step [296/1126], Loss: 0.0295\n",
      "Epoch [10/10], Step [300/1126], Loss: 0.0294\n",
      "Epoch [10/10], Step [304/1126], Loss: 0.0299\n",
      "Epoch [10/10], Step [308/1126], Loss: 0.0461\n",
      "Epoch [10/10], Step [312/1126], Loss: 0.0512\n",
      "Epoch [10/10], Step [316/1126], Loss: 0.0727\n",
      "Epoch [10/10], Step [320/1126], Loss: 0.0626\n",
      "Epoch [10/10], Step [324/1126], Loss: 0.0383\n",
      "Epoch [10/10], Step [328/1126], Loss: 0.0765\n",
      "Epoch [10/10], Step [332/1126], Loss: 0.0352\n",
      "Epoch [10/10], Step [336/1126], Loss: 0.0178\n",
      "Epoch [10/10], Step [340/1126], Loss: 0.0667\n",
      "Epoch [10/10], Step [344/1126], Loss: 0.0124\n",
      "Epoch [10/10], Step [348/1126], Loss: 0.0512\n",
      "Epoch [10/10], Step [352/1126], Loss: 0.0342\n",
      "Epoch [10/10], Step [356/1126], Loss: 0.0057\n",
      "Epoch [10/10], Step [360/1126], Loss: 0.0076\n",
      "Epoch [10/10], Step [364/1126], Loss: 0.0282\n",
      "Epoch [10/10], Step [368/1126], Loss: 0.0793\n",
      "Epoch [10/10], Step [372/1126], Loss: 0.0601\n",
      "Epoch [10/10], Step [376/1126], Loss: 0.0312\n",
      "Epoch [10/10], Step [380/1126], Loss: 0.0479\n",
      "Epoch [10/10], Step [384/1126], Loss: 0.0843\n",
      "Epoch [10/10], Step [388/1126], Loss: 0.0558\n",
      "Epoch [10/10], Step [392/1126], Loss: 0.0336\n",
      "Epoch [10/10], Step [396/1126], Loss: 0.0349\n",
      "Epoch [10/10], Step [400/1126], Loss: 0.0172\n",
      "Epoch [10/10], Step [404/1126], Loss: 0.0262\n",
      "Epoch [10/10], Step [408/1126], Loss: 0.2037\n",
      "Epoch [10/10], Step [412/1126], Loss: 0.0471\n",
      "Epoch [10/10], Step [416/1126], Loss: 0.0259\n",
      "Epoch [10/10], Step [420/1126], Loss: 0.0525\n",
      "Epoch [10/10], Step [424/1126], Loss: 0.0509\n",
      "Epoch [10/10], Step [428/1126], Loss: 0.0792\n",
      "Epoch [10/10], Step [432/1126], Loss: 0.1234\n",
      "Epoch [10/10], Step [436/1126], Loss: 0.0366\n",
      "Epoch [10/10], Step [440/1126], Loss: 0.0650\n",
      "Epoch [10/10], Step [444/1126], Loss: 0.0369\n",
      "Epoch [10/10], Step [448/1126], Loss: 0.0493\n",
      "Epoch [10/10], Step [452/1126], Loss: 0.0470\n",
      "Epoch [10/10], Step [456/1126], Loss: 0.0365\n",
      "Epoch [10/10], Step [460/1126], Loss: 0.0227\n",
      "Epoch [10/10], Step [464/1126], Loss: 0.0330\n",
      "Epoch [10/10], Step [468/1126], Loss: 0.0543\n",
      "Epoch [10/10], Step [472/1126], Loss: 0.0432\n",
      "Epoch [10/10], Step [476/1126], Loss: 0.0349\n",
      "Epoch [10/10], Step [480/1126], Loss: 0.0484\n",
      "Epoch [10/10], Step [484/1126], Loss: 0.0436\n",
      "Epoch [10/10], Step [488/1126], Loss: 0.0390\n",
      "Epoch [10/10], Step [492/1126], Loss: 0.0137\n",
      "Epoch [10/10], Step [496/1126], Loss: 0.1354\n",
      "Epoch [10/10], Step [500/1126], Loss: 0.1057\n",
      "Epoch [10/10], Step [504/1126], Loss: 0.0450\n",
      "Epoch [10/10], Step [508/1126], Loss: 0.0765\n",
      "Epoch [10/10], Step [512/1126], Loss: 0.0605\n",
      "Epoch [10/10], Step [516/1126], Loss: 0.0242\n",
      "Epoch [10/10], Step [520/1126], Loss: 0.0555\n",
      "Epoch [10/10], Step [524/1126], Loss: 0.1125\n",
      "Epoch [10/10], Step [528/1126], Loss: 0.0422\n",
      "Epoch [10/10], Step [532/1126], Loss: 0.2249\n",
      "Epoch [10/10], Step [536/1126], Loss: 0.0412\n",
      "Epoch [10/10], Step [540/1126], Loss: 0.0483\n",
      "Epoch [10/10], Step [544/1126], Loss: 0.0328\n",
      "Epoch [10/10], Step [548/1126], Loss: 0.0252\n",
      "Epoch [10/10], Step [552/1126], Loss: 0.0341\n",
      "Epoch [10/10], Step [556/1126], Loss: 0.0550\n",
      "Epoch [10/10], Step [560/1126], Loss: 0.0472\n",
      "Epoch [10/10], Step [564/1126], Loss: 0.0836\n",
      "Epoch [10/10], Step [568/1126], Loss: 0.0450\n",
      "Epoch [10/10], Step [572/1126], Loss: 0.0432\n",
      "Epoch [10/10], Step [576/1126], Loss: 0.0402\n",
      "Epoch [10/10], Step [580/1126], Loss: 0.0834\n",
      "Epoch [10/10], Step [584/1126], Loss: 0.0776\n",
      "Epoch [10/10], Step [588/1126], Loss: 0.0437\n",
      "Epoch [10/10], Step [592/1126], Loss: 0.1975\n",
      "Epoch [10/10], Step [596/1126], Loss: 0.0581\n",
      "Epoch [10/10], Step [600/1126], Loss: 0.0472\n",
      "Epoch [10/10], Step [604/1126], Loss: 0.0493\n",
      "Epoch [10/10], Step [608/1126], Loss: 0.0612\n",
      "Epoch [10/10], Step [612/1126], Loss: 0.0385\n",
      "Epoch [10/10], Step [616/1126], Loss: 0.0275\n",
      "Epoch [10/10], Step [620/1126], Loss: 0.0344\n",
      "Epoch [10/10], Step [624/1126], Loss: 0.0433\n",
      "Epoch [10/10], Step [628/1126], Loss: 0.0222\n",
      "Epoch [10/10], Step [632/1126], Loss: 0.0433\n",
      "Epoch [10/10], Step [636/1126], Loss: 0.0521\n",
      "Epoch [10/10], Step [640/1126], Loss: 0.0328\n",
      "Epoch [10/10], Step [644/1126], Loss: 0.0658\n",
      "Epoch [10/10], Step [648/1126], Loss: 0.0648\n",
      "Epoch [10/10], Step [652/1126], Loss: 0.1126\n",
      "Epoch [10/10], Step [656/1126], Loss: 0.0455\n",
      "Epoch [10/10], Step [660/1126], Loss: 0.0704\n",
      "Epoch [10/10], Step [664/1126], Loss: 0.0316\n",
      "Epoch [10/10], Step [668/1126], Loss: 0.0755\n",
      "Epoch [10/10], Step [672/1126], Loss: 0.0394\n",
      "Epoch [10/10], Step [676/1126], Loss: 0.0387\n",
      "Epoch [10/10], Step [680/1126], Loss: 0.0263\n",
      "Epoch [10/10], Step [684/1126], Loss: 0.0726\n",
      "Epoch [10/10], Step [688/1126], Loss: 0.2608\n",
      "Epoch [10/10], Step [692/1126], Loss: 0.0407\n",
      "Epoch [10/10], Step [696/1126], Loss: 0.0486\n",
      "Epoch [10/10], Step [700/1126], Loss: 0.0404\n",
      "Epoch [10/10], Step [704/1126], Loss: 0.0249\n",
      "Epoch [10/10], Step [708/1126], Loss: 0.0712\n",
      "Epoch [10/10], Step [712/1126], Loss: 0.0317\n",
      "Epoch [10/10], Step [716/1126], Loss: 0.0593\n",
      "Epoch [10/10], Step [720/1126], Loss: 0.0525\n",
      "Epoch [10/10], Step [724/1126], Loss: 0.0731\n",
      "Epoch [10/10], Step [728/1126], Loss: 0.1014\n",
      "Epoch [10/10], Step [732/1126], Loss: 0.0499\n",
      "Epoch [10/10], Step [736/1126], Loss: 0.0585\n",
      "Epoch [10/10], Step [740/1126], Loss: 0.0396\n",
      "Epoch [10/10], Step [744/1126], Loss: 0.0421\n",
      "Epoch [10/10], Step [748/1126], Loss: 0.0126\n",
      "Epoch [10/10], Step [752/1126], Loss: 0.0931\n",
      "Epoch [10/10], Step [756/1126], Loss: 0.0859\n",
      "Epoch [10/10], Step [760/1126], Loss: 0.0655\n",
      "Epoch [10/10], Step [764/1126], Loss: 0.0635\n",
      "Epoch [10/10], Step [768/1126], Loss: 0.1121\n",
      "Epoch [10/10], Step [772/1126], Loss: 0.0482\n",
      "Epoch [10/10], Step [776/1126], Loss: 0.0878\n",
      "Epoch [10/10], Step [780/1126], Loss: 0.0530\n",
      "Epoch [10/10], Step [784/1126], Loss: 0.0506\n",
      "Epoch [10/10], Step [788/1126], Loss: 0.0555\n",
      "Epoch [10/10], Step [792/1126], Loss: 0.0387\n",
      "Epoch [10/10], Step [796/1126], Loss: 0.0235\n",
      "Epoch [10/10], Step [800/1126], Loss: 0.0613\n",
      "Epoch [10/10], Step [804/1126], Loss: 0.0318\n",
      "Epoch [10/10], Step [808/1126], Loss: 0.0315\n",
      "Epoch [10/10], Step [812/1126], Loss: 0.0502\n",
      "Epoch [10/10], Step [816/1126], Loss: 0.0639\n",
      "Epoch [10/10], Step [820/1126], Loss: 0.0625\n",
      "Epoch [10/10], Step [824/1126], Loss: 0.0529\n",
      "Epoch [10/10], Step [828/1126], Loss: 0.0273\n",
      "Epoch [10/10], Step [832/1126], Loss: 0.0470\n",
      "Epoch [10/10], Step [836/1126], Loss: 0.0615\n",
      "Epoch [10/10], Step [840/1126], Loss: 0.0148\n",
      "Epoch [10/10], Step [844/1126], Loss: 0.0517\n",
      "Epoch [10/10], Step [848/1126], Loss: 0.0590\n",
      "Epoch [10/10], Step [852/1126], Loss: 0.0338\n",
      "Epoch [10/10], Step [856/1126], Loss: 0.0312\n",
      "Epoch [10/10], Step [860/1126], Loss: 0.0271\n",
      "Epoch [10/10], Step [864/1126], Loss: 0.0361\n",
      "Epoch [10/10], Step [868/1126], Loss: 0.0513\n",
      "Epoch [10/10], Step [872/1126], Loss: 0.0368\n",
      "Epoch [10/10], Step [876/1126], Loss: 0.0247\n",
      "Epoch [10/10], Step [880/1126], Loss: 0.0454\n",
      "Epoch [10/10], Step [884/1126], Loss: 0.0424\n",
      "Epoch [10/10], Step [888/1126], Loss: 0.0781\n",
      "Epoch [10/10], Step [892/1126], Loss: 0.0649\n",
      "Epoch [10/10], Step [896/1126], Loss: 0.0536\n",
      "Epoch [10/10], Step [900/1126], Loss: 0.0246\n",
      "Epoch [10/10], Step [904/1126], Loss: 0.0374\n",
      "Epoch [10/10], Step [908/1126], Loss: 0.0370\n",
      "Epoch [10/10], Step [912/1126], Loss: 0.0145\n",
      "Epoch [10/10], Step [916/1126], Loss: 0.0501\n",
      "Epoch [10/10], Step [920/1126], Loss: 0.0309\n",
      "Epoch [10/10], Step [924/1126], Loss: 0.0098\n",
      "Epoch [10/10], Step [928/1126], Loss: 0.0142\n",
      "Epoch [10/10], Step [932/1126], Loss: 0.0197\n",
      "Epoch [10/10], Step [936/1126], Loss: 0.0226\n",
      "Epoch [10/10], Step [940/1126], Loss: 0.0295\n",
      "Epoch [10/10], Step [944/1126], Loss: 0.0374\n",
      "Epoch [10/10], Step [948/1126], Loss: 0.0539\n",
      "Epoch [10/10], Step [952/1126], Loss: 0.0539\n",
      "Epoch [10/10], Step [956/1126], Loss: 0.1267\n",
      "Epoch [10/10], Step [960/1126], Loss: 0.0258\n",
      "Epoch [10/10], Step [964/1126], Loss: 0.0208\n",
      "Epoch [10/10], Step [968/1126], Loss: 0.0559\n",
      "Epoch [10/10], Step [972/1126], Loss: 0.0163\n",
      "Epoch [10/10], Step [976/1126], Loss: 0.0316\n",
      "Epoch [10/10], Step [980/1126], Loss: 0.0526\n",
      "Epoch [10/10], Step [984/1126], Loss: 0.1043\n",
      "Epoch [10/10], Step [988/1126], Loss: 0.0277\n",
      "Epoch [10/10], Step [992/1126], Loss: 0.0381\n",
      "Epoch [10/10], Step [996/1126], Loss: 0.0435\n",
      "Epoch [10/10], Step [1000/1126], Loss: 0.0299\n",
      "Epoch [10/10], Step [1004/1126], Loss: 0.0405\n",
      "Epoch [10/10], Step [1008/1126], Loss: 0.0360\n",
      "Epoch [10/10], Step [1012/1126], Loss: 0.0437\n",
      "Epoch [10/10], Step [1016/1126], Loss: 0.0285\n",
      "Epoch [10/10], Step [1020/1126], Loss: 0.0579\n",
      "Epoch [10/10], Step [1024/1126], Loss: 0.1031\n",
      "Epoch [10/10], Step [1028/1126], Loss: 0.1039\n",
      "Epoch [10/10], Step [1032/1126], Loss: 0.0369\n",
      "Epoch [10/10], Step [1036/1126], Loss: 0.0431\n",
      "Epoch [10/10], Step [1040/1126], Loss: 0.0582\n",
      "Epoch [10/10], Step [1044/1126], Loss: 0.0298\n",
      "Epoch [10/10], Step [1048/1126], Loss: 0.0404\n",
      "Epoch [10/10], Step [1052/1126], Loss: 0.0789\n",
      "Epoch [10/10], Step [1056/1126], Loss: 0.0628\n",
      "Epoch [10/10], Step [1060/1126], Loss: 0.0228\n",
      "Epoch [10/10], Step [1064/1126], Loss: 0.0940\n",
      "Epoch [10/10], Step [1068/1126], Loss: 0.0412\n",
      "Epoch [10/10], Step [1072/1126], Loss: 0.0938\n",
      "Epoch [10/10], Step [1076/1126], Loss: 0.0474\n",
      "Epoch [10/10], Step [1080/1126], Loss: 0.0338\n",
      "Epoch [10/10], Step [1084/1126], Loss: 0.0232\n",
      "Epoch [10/10], Step [1088/1126], Loss: 0.0364\n",
      "Epoch [10/10], Step [1092/1126], Loss: 0.0491\n",
      "Epoch [10/10], Step [1096/1126], Loss: 0.0331\n",
      "Epoch [10/10], Step [1100/1126], Loss: 0.0545\n",
      "Epoch [10/10], Step [1104/1126], Loss: 0.0711\n",
      "Epoch [10/10], Step [1108/1126], Loss: 0.0536\n",
      "Epoch [10/10], Step [1112/1126], Loss: 0.0464\n",
      "Epoch [10/10], Step [1116/1126], Loss: 0.0292\n",
      "Epoch [10/10], Step [1120/1126], Loss: 0.0520\n",
      "Epoch [10/10], Step [1124/1126], Loss: 0.1812\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           b       0.85      0.95      0.90     23193\n",
      "           t       0.94      0.90      0.92     21669\n",
      "           e       0.98      0.96      0.97     30494\n",
      "           m       0.98      0.87      0.92      9128\n",
      "\n",
      "    accuracy                           0.93     84484\n",
      "   macro avg       0.94      0.92      0.93     84484\n",
      "weighted avg       0.94      0.93      0.93     84484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function to perform multi-class classification\n",
    "if debug:\n",
    "    print('total texts in train:', len(train_df))\n",
    "if debug:\n",
    "    print('total texts in test:', len(test_df))\n",
    "\n",
    "# Getting all the vocabularies and indexing to a unique position\n",
    "vocab = Counter()\n",
    "# Indexing words from the training data\n",
    "for text in train_df['TITLE']:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()] += 1\n",
    "\n",
    "# Indexing words from the test data\n",
    "for text in test_df['TITLE']:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()] += 1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "\n",
    "    return word2index\n",
    "\n",
    "\n",
    "word2index = get_word_2_index(vocab)\n",
    "\n",
    "if debug:\n",
    "    print(len(word2index))\n",
    "if debug:\n",
    "    print(total_words)\n",
    "\n",
    "\n",
    "def get_batch(df, i, batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df['TITLE'].iloc[i * batch_size:i * batch_size + batch_size]\n",
    "    categories = df['CATEGORY'].iloc[i *\n",
    "                                     batch_size:i * batch_size + batch_size]\n",
    "\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words, dtype=float)\n",
    "        for word in text.split(' '):\n",
    "            layer[word2index[word.lower()]] += 1\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        # Map category labels to numerical values\n",
    "        results.append(label_mapping[category])\n",
    "\n",
    "    return np.array(batches), np.array(results)\n",
    "\n",
    "\n",
    "# Setup of the nn\n",
    "# Parameters\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 300\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "hidden_size = 100      # 1st layer and 2nd layer number of features\n",
    "input_size = total_words  # Words in vocab\n",
    "num_classes = len(label_mapping)   # Number of categories\n",
    "\n",
    "# Set the device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define the network\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "news_net = TextClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(news_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(train_df) / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = get_batch(train_df, i, batch_size)\n",
    "        articles = torch.FloatTensor(batch_x).to(device)\n",
    "        labels = torch.LongTensor(batch_y).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = news_net(articles)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 4 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %\n",
    "                  (epoch + 1, num_epochs, i + 1, len(train_df) / batch_size, loss.data))\n",
    "\n",
    "# Test the Model\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Calculate the total number of batches\n",
    "    total_batches = int(np.ceil(len(test_df) / batch_size))\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        # Calculate the start and end indices for the current batch\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(test_df))\n",
    "\n",
    "        # Get the current batch\n",
    "        batch_x_test, batch_y_test = get_batch(test_df, i, batch_size)\n",
    "\n",
    "        articles = torch.FloatTensor(batch_x_test).to(device)\n",
    "        labels = torch.LongTensor(batch_y_test).to(device)\n",
    "        # Forward pass\n",
    "        outputs = news_net(articles)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Collect predicted and true labels for all batches\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predicted = np.array(all_predicted)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(map(str, np.unique(all_labels)))\n",
    "\n",
    "# Calculate and print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predicted, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fynnhagen/anaconda3/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch [1/300], Step [3/112], Loss: 1.2078\n",
      "Epoch [1/300], Step [7/112], Loss: 1.1118\n",
      "Epoch [1/300], Step [11/112], Loss: 1.0523\n",
      "Epoch [1/300], Step [15/112], Loss: 0.9697\n",
      "Epoch [1/300], Step [19/112], Loss: 0.9404\n",
      "Epoch [1/300], Step [23/112], Loss: 0.9228\n",
      "Epoch [1/300], Step [27/112], Loss: 0.8742\n",
      "Epoch [1/300], Step [31/112], Loss: 0.9135\n",
      "Epoch [1/300], Step [35/112], Loss: 0.8719\n",
      "Epoch [1/300], Step [39/112], Loss: 0.8720\n",
      "Epoch [1/300], Step [43/112], Loss: 0.8565\n",
      "Epoch [1/300], Step [47/112], Loss: 0.8560\n",
      "Epoch [1/300], Step [51/112], Loss: 0.8486\n",
      "Epoch [1/300], Step [55/112], Loss: 0.8608\n",
      "Epoch [1/300], Step [59/112], Loss: 0.8443\n",
      "Epoch [1/300], Step [63/112], Loss: 0.8580\n",
      "Epoch [1/300], Step [67/112], Loss: 0.8562\n",
      "Epoch [1/300], Step [71/112], Loss: 0.8401\n",
      "Epoch [1/300], Step [75/112], Loss: 0.8529\n",
      "Epoch [1/300], Step [79/112], Loss: 0.8399\n",
      "Epoch [1/300], Step [83/112], Loss: 0.8607\n",
      "Epoch [1/300], Step [87/112], Loss: 0.8339\n",
      "Epoch [1/300], Step [91/112], Loss: 0.8891\n",
      "Epoch [1/300], Step [95/112], Loss: 0.8477\n",
      "Epoch [1/300], Step [99/112], Loss: 0.8497\n",
      "Epoch [1/300], Step [103/112], Loss: 0.8569\n",
      "Epoch [1/300], Step [107/112], Loss: 0.8310\n",
      "Epoch [1/300], Step [111/112], Loss: 0.8125\n",
      "Epoch [2/300], Step [3/112], Loss: 0.8177\n",
      "Epoch [2/300], Step [7/112], Loss: 0.8403\n",
      "Epoch [2/300], Step [11/112], Loss: 0.8274\n",
      "Epoch [2/300], Step [15/112], Loss: 0.8260\n",
      "Epoch [2/300], Step [19/112], Loss: 0.8234\n",
      "Epoch [2/300], Step [23/112], Loss: 0.8208\n",
      "Epoch [2/300], Step [27/112], Loss: 0.8009\n",
      "Epoch [2/300], Step [31/112], Loss: 0.8412\n",
      "Epoch [2/300], Step [35/112], Loss: 0.7996\n",
      "Epoch [2/300], Step [39/112], Loss: 0.7916\n",
      "Epoch [2/300], Step [43/112], Loss: 0.8010\n",
      "Epoch [2/300], Step [47/112], Loss: 0.8005\n",
      "Epoch [2/300], Step [51/112], Loss: 0.8102\n",
      "Epoch [2/300], Step [55/112], Loss: 0.8140\n",
      "Epoch [2/300], Step [59/112], Loss: 0.8071\n",
      "Epoch [2/300], Step [63/112], Loss: 0.8226\n",
      "Epoch [2/300], Step [67/112], Loss: 0.8092\n",
      "Epoch [2/300], Step [71/112], Loss: 0.8046\n",
      "Epoch [2/300], Step [75/112], Loss: 0.8119\n",
      "Epoch [2/300], Step [79/112], Loss: 0.7980\n",
      "Epoch [2/300], Step [83/112], Loss: 0.8171\n",
      "Epoch [2/300], Step [87/112], Loss: 0.7954\n",
      "Epoch [2/300], Step [91/112], Loss: 0.8437\n",
      "Epoch [2/300], Step [95/112], Loss: 0.8136\n",
      "Epoch [2/300], Step [99/112], Loss: 0.8174\n",
      "Epoch [2/300], Step [103/112], Loss: 0.8233\n",
      "Epoch [2/300], Step [107/112], Loss: 0.8037\n",
      "Epoch [2/300], Step [111/112], Loss: 0.7821\n",
      "Epoch [3/300], Step [3/112], Loss: 0.7844\n",
      "Epoch [3/300], Step [7/112], Loss: 0.8073\n",
      "Epoch [3/300], Step [11/112], Loss: 0.8005\n",
      "Epoch [3/300], Step [15/112], Loss: 0.7899\n",
      "Epoch [3/300], Step [19/112], Loss: 0.7899\n",
      "Epoch [3/300], Step [23/112], Loss: 0.7987\n",
      "Epoch [3/300], Step [27/112], Loss: 0.7722\n",
      "Epoch [3/300], Step [31/112], Loss: 0.8209\n",
      "Epoch [3/300], Step [35/112], Loss: 0.7783\n",
      "Epoch [3/300], Step [39/112], Loss: 0.7689\n",
      "Epoch [3/300], Step [43/112], Loss: 0.7827\n",
      "Epoch [3/300], Step [47/112], Loss: 0.7707\n",
      "Epoch [3/300], Step [51/112], Loss: 0.7788\n",
      "Epoch [3/300], Step [55/112], Loss: 0.7838\n",
      "Epoch [3/300], Step [59/112], Loss: 0.7723\n",
      "Epoch [3/300], Step [63/112], Loss: 0.7954\n",
      "Epoch [3/300], Step [67/112], Loss: 0.7871\n",
      "Epoch [3/300], Step [71/112], Loss: 0.7830\n",
      "Epoch [3/300], Step [75/112], Loss: 0.7820\n",
      "Epoch [3/300], Step [79/112], Loss: 0.7731\n",
      "Epoch [3/300], Step [83/112], Loss: 0.7952\n",
      "Epoch [3/300], Step [87/112], Loss: 0.7712\n",
      "Epoch [3/300], Step [91/112], Loss: 0.8231\n",
      "Epoch [3/300], Step [95/112], Loss: 0.7883\n",
      "Epoch [3/300], Step [99/112], Loss: 0.7939\n",
      "Epoch [3/300], Step [103/112], Loss: 0.7920\n",
      "Epoch [3/300], Step [107/112], Loss: 0.7843\n",
      "Epoch [3/300], Step [111/112], Loss: 0.7657\n",
      "Epoch [4/300], Step [3/112], Loss: 0.7737\n",
      "Epoch [4/300], Step [7/112], Loss: 0.7886\n",
      "Epoch [4/300], Step [11/112], Loss: 0.7828\n",
      "Epoch [4/300], Step [15/112], Loss: 0.7690\n",
      "Epoch [4/300], Step [19/112], Loss: 0.7730\n",
      "Epoch [4/300], Step [23/112], Loss: 0.7859\n",
      "Epoch [4/300], Step [27/112], Loss: 0.7600\n",
      "Epoch [4/300], Step [31/112], Loss: 0.8040\n",
      "Epoch [4/300], Step [35/112], Loss: 0.7592\n",
      "Epoch [4/300], Step [39/112], Loss: 0.7559\n",
      "Epoch [4/300], Step [43/112], Loss: 0.7726\n",
      "Epoch [4/300], Step [47/112], Loss: 0.7543\n",
      "Epoch [4/300], Step [51/112], Loss: 0.7574\n",
      "Epoch [4/300], Step [55/112], Loss: 0.7621\n",
      "Epoch [4/300], Step [59/112], Loss: 0.7611\n",
      "Epoch [4/300], Step [63/112], Loss: 0.7769\n",
      "Epoch [4/300], Step [67/112], Loss: 0.7722\n",
      "Epoch [4/300], Step [71/112], Loss: 0.7694\n",
      "Epoch [4/300], Step [75/112], Loss: 0.7597\n",
      "Epoch [4/300], Step [79/112], Loss: 0.7569\n",
      "Epoch [4/300], Step [83/112], Loss: 0.7821\n",
      "Epoch [4/300], Step [87/112], Loss: 0.7526\n",
      "Epoch [4/300], Step [91/112], Loss: 0.8127\n",
      "Epoch [4/300], Step [95/112], Loss: 0.7788\n",
      "Epoch [4/300], Step [99/112], Loss: 0.7798\n",
      "Epoch [4/300], Step [103/112], Loss: 0.7786\n",
      "Epoch [4/300], Step [107/112], Loss: 0.7706\n",
      "Epoch [4/300], Step [111/112], Loss: 0.7571\n",
      "Epoch [5/300], Step [3/112], Loss: 0.7606\n",
      "Epoch [5/300], Step [7/112], Loss: 0.7701\n",
      "Epoch [5/300], Step [11/112], Loss: 0.7760\n",
      "Epoch [5/300], Step [15/112], Loss: 0.7607\n",
      "Epoch [5/300], Step [19/112], Loss: 0.7651\n",
      "Epoch [5/300], Step [23/112], Loss: 0.7711\n",
      "Epoch [5/300], Step [27/112], Loss: 0.7444\n",
      "Epoch [5/300], Step [31/112], Loss: 0.7866\n",
      "Epoch [5/300], Step [35/112], Loss: 0.7387\n",
      "Epoch [5/300], Step [39/112], Loss: 0.7416\n",
      "Epoch [5/300], Step [43/112], Loss: 0.7571\n",
      "Epoch [5/300], Step [47/112], Loss: 0.7412\n",
      "Epoch [5/300], Step [51/112], Loss: 0.7444\n",
      "Epoch [5/300], Step [55/112], Loss: 0.7469\n",
      "Epoch [5/300], Step [59/112], Loss: 0.7496\n",
      "Epoch [5/300], Step [63/112], Loss: 0.7553\n",
      "Epoch [5/300], Step [67/112], Loss: 0.7595\n",
      "Epoch [5/300], Step [71/112], Loss: 0.7578\n",
      "Epoch [5/300], Step [75/112], Loss: 0.7533\n",
      "Epoch [5/300], Step [79/112], Loss: 0.7448\n",
      "Epoch [5/300], Step [83/112], Loss: 0.7693\n",
      "Epoch [5/300], Step [87/112], Loss: 0.7463\n",
      "Epoch [5/300], Step [91/112], Loss: 0.7929\n",
      "Epoch [5/300], Step [95/112], Loss: 0.7719\n",
      "Epoch [5/300], Step [99/112], Loss: 0.7689\n",
      "Epoch [5/300], Step [103/112], Loss: 0.7677\n",
      "Epoch [5/300], Step [107/112], Loss: 0.7614\n",
      "Epoch [5/300], Step [111/112], Loss: 0.7335\n",
      "Epoch [6/300], Step [3/112], Loss: 0.7476\n",
      "Epoch [6/300], Step [7/112], Loss: 0.7563\n",
      "Epoch [6/300], Step [11/112], Loss: 0.7670\n",
      "Epoch [6/300], Step [15/112], Loss: 0.7556\n",
      "Epoch [6/300], Step [19/112], Loss: 0.7492\n",
      "Epoch [6/300], Step [23/112], Loss: 0.7619\n",
      "Epoch [6/300], Step [27/112], Loss: 0.7217\n",
      "Epoch [6/300], Step [31/112], Loss: 0.7838\n",
      "Epoch [6/300], Step [35/112], Loss: 0.7245\n",
      "Epoch [6/300], Step [39/112], Loss: 0.7274\n",
      "Epoch [6/300], Step [43/112], Loss: 0.7519\n",
      "Epoch [6/300], Step [47/112], Loss: 0.7313\n",
      "Epoch [6/300], Step [51/112], Loss: 0.7298\n",
      "Epoch [6/300], Step [55/112], Loss: 0.7384\n",
      "Epoch [6/300], Step [59/112], Loss: 0.7436\n",
      "Epoch [6/300], Step [63/112], Loss: 0.7435\n",
      "Epoch [6/300], Step [67/112], Loss: 0.7552\n",
      "Epoch [6/300], Step [71/112], Loss: 0.7458\n",
      "Epoch [6/300], Step [75/112], Loss: 0.7477\n",
      "Epoch [6/300], Step [79/112], Loss: 0.7349\n",
      "Epoch [6/300], Step [83/112], Loss: 0.7540\n",
      "Epoch [6/300], Step [87/112], Loss: 0.7364\n",
      "Epoch [6/300], Step [91/112], Loss: 0.7780\n",
      "Epoch [6/300], Step [95/112], Loss: 0.7637\n",
      "Epoch [6/300], Step [99/112], Loss: 0.7583\n",
      "Epoch [6/300], Step [103/112], Loss: 0.7628\n",
      "Epoch [6/300], Step [107/112], Loss: 0.7555\n",
      "Epoch [6/300], Step [111/112], Loss: 0.7181\n",
      "Epoch [7/300], Step [3/112], Loss: 0.7384\n",
      "Epoch [7/300], Step [7/112], Loss: 0.7414\n",
      "Epoch [7/300], Step [11/112], Loss: 0.7629\n",
      "Epoch [7/300], Step [15/112], Loss: 0.7422\n",
      "Epoch [7/300], Step [19/112], Loss: 0.7344\n",
      "Epoch [7/300], Step [23/112], Loss: 0.7541\n",
      "Epoch [7/300], Step [27/112], Loss: 0.7157\n",
      "Epoch [7/300], Step [31/112], Loss: 0.7686\n",
      "Epoch [7/300], Step [35/112], Loss: 0.7210\n",
      "Epoch [7/300], Step [39/112], Loss: 0.7202\n",
      "Epoch [7/300], Step [43/112], Loss: 0.7413\n",
      "Epoch [7/300], Step [47/112], Loss: 0.7252\n",
      "Epoch [7/300], Step [51/112], Loss: 0.7233\n",
      "Epoch [7/300], Step [55/112], Loss: 0.7356\n",
      "Epoch [7/300], Step [59/112], Loss: 0.7276\n",
      "Epoch [7/300], Step [63/112], Loss: 0.7342\n",
      "Epoch [7/300], Step [67/112], Loss: 0.7483\n",
      "Epoch [7/300], Step [71/112], Loss: 0.7386\n",
      "Epoch [7/300], Step [75/112], Loss: 0.7388\n",
      "Epoch [7/300], Step [79/112], Loss: 0.7320\n",
      "Epoch [7/300], Step [83/112], Loss: 0.7459\n",
      "Epoch [7/300], Step [87/112], Loss: 0.7248\n",
      "Epoch [7/300], Step [91/112], Loss: 0.7725\n",
      "Epoch [7/300], Step [95/112], Loss: 0.7540\n",
      "Epoch [7/300], Step [99/112], Loss: 0.7515\n",
      "Epoch [7/300], Step [103/112], Loss: 0.7566\n",
      "Epoch [7/300], Step [107/112], Loss: 0.7525\n",
      "Epoch [7/300], Step [111/112], Loss: 0.7051\n",
      "Epoch [8/300], Step [3/112], Loss: 0.7294\n",
      "Epoch [8/300], Step [7/112], Loss: 0.7375\n",
      "Epoch [8/300], Step [11/112], Loss: 0.7539\n",
      "Epoch [8/300], Step [15/112], Loss: 0.7384\n",
      "Epoch [8/300], Step [19/112], Loss: 0.7312\n",
      "Epoch [8/300], Step [23/112], Loss: 0.7440\n",
      "Epoch [8/300], Step [27/112], Loss: 0.7024\n",
      "Epoch [8/300], Step [31/112], Loss: 0.7572\n",
      "Epoch [8/300], Step [35/112], Loss: 0.7146\n",
      "Epoch [8/300], Step [39/112], Loss: 0.7111\n",
      "Epoch [8/300], Step [43/112], Loss: 0.7355\n",
      "Epoch [8/300], Step [47/112], Loss: 0.7143\n",
      "Epoch [8/300], Step [51/112], Loss: 0.7167\n",
      "Epoch [8/300], Step [55/112], Loss: 0.7236\n",
      "Epoch [8/300], Step [59/112], Loss: 0.7220\n",
      "Epoch [8/300], Step [63/112], Loss: 0.7320\n",
      "Epoch [8/300], Step [67/112], Loss: 0.7393\n",
      "Epoch [8/300], Step [71/112], Loss: 0.7302\n",
      "Epoch [8/300], Step [75/112], Loss: 0.7299\n",
      "Epoch [8/300], Step [79/112], Loss: 0.7183\n",
      "Epoch [8/300], Step [83/112], Loss: 0.7380\n",
      "Epoch [8/300], Step [87/112], Loss: 0.7223\n",
      "Epoch [8/300], Step [91/112], Loss: 0.7639\n",
      "Epoch [8/300], Step [95/112], Loss: 0.7436\n",
      "Epoch [8/300], Step [99/112], Loss: 0.7463\n",
      "Epoch [8/300], Step [103/112], Loss: 0.7452\n",
      "Epoch [8/300], Step [107/112], Loss: 0.7449\n",
      "Epoch [8/300], Step [111/112], Loss: 0.6991\n",
      "Epoch [9/300], Step [3/112], Loss: 0.7208\n",
      "Epoch [9/300], Step [7/112], Loss: 0.7314\n",
      "Epoch [9/300], Step [11/112], Loss: 0.7483\n",
      "Epoch [9/300], Step [15/112], Loss: 0.7305\n",
      "Epoch [9/300], Step [19/112], Loss: 0.7226\n",
      "Epoch [9/300], Step [23/112], Loss: 0.7419\n",
      "Epoch [9/300], Step [27/112], Loss: 0.6999\n",
      "Epoch [9/300], Step [31/112], Loss: 0.7538\n",
      "Epoch [9/300], Step [35/112], Loss: 0.7048\n",
      "Epoch [9/300], Step [39/112], Loss: 0.7057\n",
      "Epoch [9/300], Step [43/112], Loss: 0.7277\n",
      "Epoch [9/300], Step [47/112], Loss: 0.7030\n",
      "Epoch [9/300], Step [51/112], Loss: 0.7138\n",
      "Epoch [9/300], Step [55/112], Loss: 0.7169\n",
      "Epoch [9/300], Step [59/112], Loss: 0.7114\n",
      "Epoch [9/300], Step [63/112], Loss: 0.7253\n",
      "Epoch [9/300], Step [67/112], Loss: 0.7312\n",
      "Epoch [9/300], Step [71/112], Loss: 0.7261\n",
      "Epoch [9/300], Step [75/112], Loss: 0.7302\n",
      "Epoch [9/300], Step [79/112], Loss: 0.7141\n",
      "Epoch [9/300], Step [83/112], Loss: 0.7336\n",
      "Epoch [9/300], Step [87/112], Loss: 0.7147\n",
      "Epoch [9/300], Step [91/112], Loss: 0.7586\n",
      "Epoch [9/300], Step [95/112], Loss: 0.7372\n",
      "Epoch [9/300], Step [99/112], Loss: 0.7454\n",
      "Epoch [9/300], Step [103/112], Loss: 0.7386\n",
      "Epoch [9/300], Step [107/112], Loss: 0.7368\n",
      "Epoch [9/300], Step [111/112], Loss: 0.6997\n",
      "Epoch [10/300], Step [3/112], Loss: 0.7201\n",
      "Epoch [10/300], Step [7/112], Loss: 0.7332\n",
      "Epoch [10/300], Step [11/112], Loss: 0.7456\n",
      "Epoch [10/300], Step [15/112], Loss: 0.7229\n",
      "Epoch [10/300], Step [19/112], Loss: 0.7251\n",
      "Epoch [10/300], Step [23/112], Loss: 0.7498\n",
      "Epoch [10/300], Step [27/112], Loss: 0.6922\n",
      "Epoch [10/300], Step [31/112], Loss: 0.7479\n",
      "Epoch [10/300], Step [35/112], Loss: 0.7017\n",
      "Epoch [10/300], Step [39/112], Loss: 0.7022\n",
      "Epoch [10/300], Step [43/112], Loss: 0.7238\n",
      "Epoch [10/300], Step [47/112], Loss: 0.7002\n",
      "Epoch [10/300], Step [51/112], Loss: 0.7081\n",
      "Epoch [10/300], Step [55/112], Loss: 0.7116\n",
      "Epoch [10/300], Step [59/112], Loss: 0.7047\n",
      "Epoch [10/300], Step [63/112], Loss: 0.7229\n",
      "Epoch [10/300], Step [67/112], Loss: 0.7258\n",
      "Epoch [10/300], Step [71/112], Loss: 0.7254\n",
      "Epoch [10/300], Step [75/112], Loss: 0.7318\n",
      "Epoch [10/300], Step [79/112], Loss: 0.7080\n",
      "Epoch [10/300], Step [83/112], Loss: 0.7317\n",
      "Epoch [10/300], Step [87/112], Loss: 0.7165\n",
      "Epoch [10/300], Step [91/112], Loss: 0.7502\n",
      "Epoch [10/300], Step [95/112], Loss: 0.7334\n",
      "Epoch [10/300], Step [99/112], Loss: 0.7343\n",
      "Epoch [10/300], Step [103/112], Loss: 0.7391\n",
      "Epoch [10/300], Step [107/112], Loss: 0.7277\n",
      "Epoch [10/300], Step [111/112], Loss: 0.6919\n",
      "Epoch [11/300], Step [3/112], Loss: 0.7153\n",
      "Epoch [11/300], Step [7/112], Loss: 0.7240\n",
      "Epoch [11/300], Step [11/112], Loss: 0.7335\n",
      "Epoch [11/300], Step [15/112], Loss: 0.7193\n",
      "Epoch [11/300], Step [19/112], Loss: 0.7218\n",
      "Epoch [11/300], Step [23/112], Loss: 0.7364\n",
      "Epoch [11/300], Step [27/112], Loss: 0.6945\n",
      "Epoch [11/300], Step [31/112], Loss: 0.7447\n",
      "Epoch [11/300], Step [35/112], Loss: 0.6964\n",
      "Epoch [11/300], Step [39/112], Loss: 0.7020\n",
      "Epoch [11/300], Step [43/112], Loss: 0.7135\n",
      "Epoch [11/300], Step [47/112], Loss: 0.7004\n",
      "Epoch [11/300], Step [51/112], Loss: 0.7013\n",
      "Epoch [11/300], Step [55/112], Loss: 0.7044\n",
      "Epoch [11/300], Step [59/112], Loss: 0.7011\n",
      "Epoch [11/300], Step [63/112], Loss: 0.7198\n",
      "Epoch [11/300], Step [67/112], Loss: 0.7261\n",
      "Epoch [11/300], Step [71/112], Loss: 0.7210\n",
      "Epoch [11/300], Step [75/112], Loss: 0.7218\n",
      "Epoch [11/300], Step [79/112], Loss: 0.7031\n",
      "Epoch [11/300], Step [83/112], Loss: 0.7245\n",
      "Epoch [11/300], Step [87/112], Loss: 0.7136\n",
      "Epoch [11/300], Step [91/112], Loss: 0.7402\n",
      "Epoch [11/300], Step [95/112], Loss: 0.7230\n",
      "Epoch [11/300], Step [99/112], Loss: 0.7298\n",
      "Epoch [11/300], Step [103/112], Loss: 0.7284\n",
      "Epoch [11/300], Step [107/112], Loss: 0.7246\n",
      "Epoch [11/300], Step [111/112], Loss: 0.6898\n",
      "Epoch [12/300], Step [3/112], Loss: 0.7162\n",
      "Epoch [12/300], Step [7/112], Loss: 0.7235\n",
      "Epoch [12/300], Step [11/112], Loss: 0.7365\n",
      "Epoch [12/300], Step [15/112], Loss: 0.7194\n",
      "Epoch [12/300], Step [19/112], Loss: 0.7174\n",
      "Epoch [12/300], Step [23/112], Loss: 0.7332\n",
      "Epoch [12/300], Step [27/112], Loss: 0.6906\n",
      "Epoch [12/300], Step [31/112], Loss: 0.7427\n",
      "Epoch [12/300], Step [35/112], Loss: 0.6958\n",
      "Epoch [12/300], Step [39/112], Loss: 0.6987\n",
      "Epoch [12/300], Step [43/112], Loss: 0.7053\n",
      "Epoch [12/300], Step [47/112], Loss: 0.6940\n",
      "Epoch [12/300], Step [51/112], Loss: 0.6954\n",
      "Epoch [12/300], Step [55/112], Loss: 0.6966\n",
      "Epoch [12/300], Step [59/112], Loss: 0.6981\n",
      "Epoch [12/300], Step [63/112], Loss: 0.7179\n",
      "Epoch [12/300], Step [67/112], Loss: 0.7187\n",
      "Epoch [12/300], Step [71/112], Loss: 0.7145\n",
      "Epoch [12/300], Step [75/112], Loss: 0.7199\n",
      "Epoch [12/300], Step [79/112], Loss: 0.6990\n",
      "Epoch [12/300], Step [83/112], Loss: 0.7223\n",
      "Epoch [12/300], Step [87/112], Loss: 0.7146\n",
      "Epoch [12/300], Step [91/112], Loss: 0.7369\n",
      "Epoch [12/300], Step [95/112], Loss: 0.7260\n",
      "Epoch [12/300], Step [99/112], Loss: 0.7175\n",
      "Epoch [12/300], Step [103/112], Loss: 0.7260\n",
      "Epoch [12/300], Step [107/112], Loss: 0.7221\n",
      "Epoch [12/300], Step [111/112], Loss: 0.6895\n",
      "Epoch [13/300], Step [3/112], Loss: 0.7151\n",
      "Epoch [13/300], Step [7/112], Loss: 0.7237\n",
      "Epoch [13/300], Step [11/112], Loss: 0.7306\n",
      "Epoch [13/300], Step [15/112], Loss: 0.7132\n",
      "Epoch [13/300], Step [19/112], Loss: 0.7103\n",
      "Epoch [13/300], Step [23/112], Loss: 0.7293\n",
      "Epoch [13/300], Step [27/112], Loss: 0.6932\n",
      "Epoch [13/300], Step [31/112], Loss: 0.7352\n",
      "Epoch [13/300], Step [35/112], Loss: 0.6889\n",
      "Epoch [13/300], Step [39/112], Loss: 0.7022\n",
      "Epoch [13/300], Step [43/112], Loss: 0.7011\n",
      "Epoch [13/300], Step [47/112], Loss: 0.6868\n",
      "Epoch [13/300], Step [51/112], Loss: 0.6892\n",
      "Epoch [13/300], Step [55/112], Loss: 0.6987\n",
      "Epoch [13/300], Step [59/112], Loss: 0.6911\n",
      "Epoch [13/300], Step [63/112], Loss: 0.7158\n",
      "Epoch [13/300], Step [67/112], Loss: 0.7105\n",
      "Epoch [13/300], Step [71/112], Loss: 0.7136\n",
      "Epoch [13/300], Step [75/112], Loss: 0.7196\n",
      "Epoch [13/300], Step [79/112], Loss: 0.6948\n",
      "Epoch [13/300], Step [83/112], Loss: 0.7230\n",
      "Epoch [13/300], Step [87/112], Loss: 0.7114\n",
      "Epoch [13/300], Step [91/112], Loss: 0.7342\n",
      "Epoch [13/300], Step [95/112], Loss: 0.7261\n",
      "Epoch [13/300], Step [99/112], Loss: 0.7142\n",
      "Epoch [13/300], Step [103/112], Loss: 0.7195\n",
      "Epoch [13/300], Step [107/112], Loss: 0.7134\n",
      "Epoch [13/300], Step [111/112], Loss: 0.6819\n",
      "Epoch [14/300], Step [3/112], Loss: 0.7130\n",
      "Epoch [14/300], Step [7/112], Loss: 0.7186\n",
      "Epoch [14/300], Step [11/112], Loss: 0.7316\n",
      "Epoch [14/300], Step [15/112], Loss: 0.7107\n",
      "Epoch [14/300], Step [19/112], Loss: 0.7085\n",
      "Epoch [14/300], Step [23/112], Loss: 0.7269\n",
      "Epoch [14/300], Step [27/112], Loss: 0.6917\n",
      "Epoch [14/300], Step [31/112], Loss: 0.7367\n",
      "Epoch [14/300], Step [35/112], Loss: 0.6870\n",
      "Epoch [14/300], Step [39/112], Loss: 0.6905\n",
      "Epoch [14/300], Step [43/112], Loss: 0.7014\n",
      "Epoch [14/300], Step [47/112], Loss: 0.6833\n",
      "Epoch [14/300], Step [51/112], Loss: 0.6828\n",
      "Epoch [14/300], Step [55/112], Loss: 0.6974\n",
      "Epoch [14/300], Step [59/112], Loss: 0.6906\n",
      "Epoch [14/300], Step [63/112], Loss: 0.7103\n",
      "Epoch [14/300], Step [67/112], Loss: 0.7083\n",
      "Epoch [14/300], Step [71/112], Loss: 0.7130\n",
      "Epoch [14/300], Step [75/112], Loss: 0.7228\n",
      "Epoch [14/300], Step [79/112], Loss: 0.6952\n",
      "Epoch [14/300], Step [83/112], Loss: 0.7216\n",
      "Epoch [14/300], Step [87/112], Loss: 0.7074\n",
      "Epoch [14/300], Step [91/112], Loss: 0.7325\n",
      "Epoch [14/300], Step [95/112], Loss: 0.7192\n",
      "Epoch [14/300], Step [99/112], Loss: 0.7148\n",
      "Epoch [14/300], Step [103/112], Loss: 0.7181\n",
      "Epoch [14/300], Step [107/112], Loss: 0.7114\n",
      "Epoch [14/300], Step [111/112], Loss: 0.6766\n",
      "Epoch [15/300], Step [3/112], Loss: 0.7040\n",
      "Epoch [15/300], Step [7/112], Loss: 0.7049\n",
      "Epoch [15/300], Step [11/112], Loss: 0.7205\n",
      "Epoch [15/300], Step [15/112], Loss: 0.7021\n",
      "Epoch [15/300], Step [19/112], Loss: 0.7042\n",
      "Epoch [15/300], Step [23/112], Loss: 0.7240\n",
      "Epoch [15/300], Step [27/112], Loss: 0.6900\n",
      "Epoch [15/300], Step [31/112], Loss: 0.7307\n",
      "Epoch [15/300], Step [35/112], Loss: 0.6826\n",
      "Epoch [15/300], Step [39/112], Loss: 0.6915\n",
      "Epoch [15/300], Step [43/112], Loss: 0.6989\n",
      "Epoch [15/300], Step [47/112], Loss: 0.6823\n",
      "Epoch [15/300], Step [51/112], Loss: 0.6833\n",
      "Epoch [15/300], Step [55/112], Loss: 0.6961\n",
      "Epoch [15/300], Step [59/112], Loss: 0.6896\n",
      "Epoch [15/300], Step [63/112], Loss: 0.7036\n",
      "Epoch [15/300], Step [67/112], Loss: 0.7011\n",
      "Epoch [15/300], Step [71/112], Loss: 0.7067\n",
      "Epoch [15/300], Step [75/112], Loss: 0.7193\n",
      "Epoch [15/300], Step [79/112], Loss: 0.6965\n",
      "Epoch [15/300], Step [83/112], Loss: 0.7233\n",
      "Epoch [15/300], Step [87/112], Loss: 0.7027\n",
      "Epoch [15/300], Step [91/112], Loss: 0.7307\n",
      "Epoch [15/300], Step [95/112], Loss: 0.7181\n",
      "Epoch [15/300], Step [99/112], Loss: 0.7088\n",
      "Epoch [15/300], Step [103/112], Loss: 0.7146\n",
      "Epoch [15/300], Step [107/112], Loss: 0.7100\n",
      "Epoch [15/300], Step [111/112], Loss: 0.6781\n",
      "Epoch [16/300], Step [3/112], Loss: 0.7071\n",
      "Epoch [16/300], Step [7/112], Loss: 0.7045\n",
      "Epoch [16/300], Step [11/112], Loss: 0.7188\n",
      "Epoch [16/300], Step [15/112], Loss: 0.7009\n",
      "Epoch [16/300], Step [19/112], Loss: 0.7013\n",
      "Epoch [16/300], Step [23/112], Loss: 0.7227\n",
      "Epoch [16/300], Step [27/112], Loss: 0.6855\n",
      "Epoch [16/300], Step [31/112], Loss: 0.7335\n",
      "Epoch [16/300], Step [35/112], Loss: 0.6819\n",
      "Epoch [16/300], Step [39/112], Loss: 0.6871\n",
      "Epoch [16/300], Step [43/112], Loss: 0.6966\n",
      "Epoch [16/300], Step [47/112], Loss: 0.6815\n",
      "Epoch [16/300], Step [51/112], Loss: 0.6782\n",
      "Epoch [16/300], Step [55/112], Loss: 0.6883\n",
      "Epoch [16/300], Step [59/112], Loss: 0.6836\n",
      "Epoch [16/300], Step [63/112], Loss: 0.7043\n",
      "Epoch [16/300], Step [67/112], Loss: 0.7018\n",
      "Epoch [16/300], Step [71/112], Loss: 0.7070\n",
      "Epoch [16/300], Step [75/112], Loss: 0.7192\n",
      "Epoch [16/300], Step [79/112], Loss: 0.6929\n",
      "Epoch [16/300], Step [83/112], Loss: 0.7190\n",
      "Epoch [16/300], Step [87/112], Loss: 0.6974\n",
      "Epoch [16/300], Step [91/112], Loss: 0.7269\n",
      "Epoch [16/300], Step [95/112], Loss: 0.7196\n",
      "Epoch [16/300], Step [99/112], Loss: 0.7086\n",
      "Epoch [16/300], Step [103/112], Loss: 0.7137\n",
      "Epoch [16/300], Step [107/112], Loss: 0.7103\n",
      "Epoch [16/300], Step [111/112], Loss: 0.6754\n",
      "Epoch [17/300], Step [3/112], Loss: 0.6989\n",
      "Epoch [17/300], Step [7/112], Loss: 0.7017\n",
      "Epoch [17/300], Step [11/112], Loss: 0.7208\n",
      "Epoch [17/300], Step [15/112], Loss: 0.6982\n",
      "Epoch [17/300], Step [19/112], Loss: 0.6976\n",
      "Epoch [17/300], Step [23/112], Loss: 0.7198\n",
      "Epoch [17/300], Step [27/112], Loss: 0.6736\n",
      "Epoch [17/300], Step [31/112], Loss: 0.7247\n",
      "Epoch [17/300], Step [35/112], Loss: 0.6772\n",
      "Epoch [17/300], Step [39/112], Loss: 0.6830\n",
      "Epoch [17/300], Step [43/112], Loss: 0.6894\n",
      "Epoch [17/300], Step [47/112], Loss: 0.6747\n",
      "Epoch [17/300], Step [51/112], Loss: 0.6782\n",
      "Epoch [17/300], Step [55/112], Loss: 0.6857\n",
      "Epoch [17/300], Step [59/112], Loss: 0.6806\n",
      "Epoch [17/300], Step [63/112], Loss: 0.7021\n",
      "Epoch [17/300], Step [67/112], Loss: 0.7024\n",
      "Epoch [17/300], Step [71/112], Loss: 0.7057\n",
      "Epoch [17/300], Step [75/112], Loss: 0.7162\n",
      "Epoch [17/300], Step [79/112], Loss: 0.6898\n",
      "Epoch [17/300], Step [83/112], Loss: 0.7151\n",
      "Epoch [17/300], Step [87/112], Loss: 0.6998\n",
      "Epoch [17/300], Step [91/112], Loss: 0.7271\n",
      "Epoch [17/300], Step [95/112], Loss: 0.7166\n",
      "Epoch [17/300], Step [99/112], Loss: 0.7072\n",
      "Epoch [17/300], Step [103/112], Loss: 0.7027\n",
      "Epoch [17/300], Step [107/112], Loss: 0.7073\n",
      "Epoch [17/300], Step [111/112], Loss: 0.6724\n",
      "Epoch [18/300], Step [3/112], Loss: 0.6960\n",
      "Epoch [18/300], Step [7/112], Loss: 0.7060\n",
      "Epoch [18/300], Step [11/112], Loss: 0.7143\n",
      "Epoch [18/300], Step [15/112], Loss: 0.6913\n",
      "Epoch [18/300], Step [19/112], Loss: 0.6968\n",
      "Epoch [18/300], Step [23/112], Loss: 0.7136\n",
      "Epoch [18/300], Step [27/112], Loss: 0.6788\n",
      "Epoch [18/300], Step [31/112], Loss: 0.7234\n",
      "Epoch [18/300], Step [35/112], Loss: 0.6768\n",
      "Epoch [18/300], Step [39/112], Loss: 0.6793\n",
      "Epoch [18/300], Step [43/112], Loss: 0.6851\n",
      "Epoch [18/300], Step [47/112], Loss: 0.6764\n",
      "Epoch [18/300], Step [51/112], Loss: 0.6785\n",
      "Epoch [18/300], Step [55/112], Loss: 0.6831\n",
      "Epoch [18/300], Step [59/112], Loss: 0.6800\n",
      "Epoch [18/300], Step [63/112], Loss: 0.6977\n",
      "Epoch [18/300], Step [67/112], Loss: 0.7028\n",
      "Epoch [18/300], Step [71/112], Loss: 0.6987\n",
      "Epoch [18/300], Step [75/112], Loss: 0.7171\n",
      "Epoch [18/300], Step [79/112], Loss: 0.6889\n",
      "Epoch [18/300], Step [83/112], Loss: 0.7193\n",
      "Epoch [18/300], Step [87/112], Loss: 0.6944\n",
      "Epoch [18/300], Step [91/112], Loss: 0.7213\n",
      "Epoch [18/300], Step [95/112], Loss: 0.7195\n",
      "Epoch [18/300], Step [99/112], Loss: 0.7063\n",
      "Epoch [18/300], Step [103/112], Loss: 0.7128\n",
      "Epoch [18/300], Step [107/112], Loss: 0.7039\n",
      "Epoch [18/300], Step [111/112], Loss: 0.6683\n",
      "Epoch [19/300], Step [3/112], Loss: 0.6970\n",
      "Epoch [19/300], Step [7/112], Loss: 0.6993\n",
      "Epoch [19/300], Step [11/112], Loss: 0.7109\n",
      "Epoch [19/300], Step [15/112], Loss: 0.6858\n",
      "Epoch [19/300], Step [19/112], Loss: 0.6945\n",
      "Epoch [19/300], Step [23/112], Loss: 0.7115\n",
      "Epoch [19/300], Step [27/112], Loss: 0.6733\n",
      "Epoch [19/300], Step [31/112], Loss: 0.7158\n",
      "Epoch [19/300], Step [35/112], Loss: 0.6751\n",
      "Epoch [19/300], Step [39/112], Loss: 0.6790\n",
      "Epoch [19/300], Step [43/112], Loss: 0.6851\n",
      "Epoch [19/300], Step [47/112], Loss: 0.6751\n",
      "Epoch [19/300], Step [51/112], Loss: 0.6734\n",
      "Epoch [19/300], Step [55/112], Loss: 0.6849\n",
      "Epoch [19/300], Step [59/112], Loss: 0.6692\n",
      "Epoch [19/300], Step [63/112], Loss: 0.6999\n",
      "Epoch [19/300], Step [67/112], Loss: 0.6944\n",
      "Epoch [19/300], Step [71/112], Loss: 0.6980\n",
      "Epoch [19/300], Step [75/112], Loss: 0.7164\n",
      "Epoch [19/300], Step [79/112], Loss: 0.6892\n",
      "Epoch [19/300], Step [83/112], Loss: 0.7212\n",
      "Epoch [19/300], Step [87/112], Loss: 0.6897\n",
      "Epoch [19/300], Step [91/112], Loss: 0.7191\n",
      "Epoch [19/300], Step [95/112], Loss: 0.7190\n",
      "Epoch [19/300], Step [99/112], Loss: 0.7067\n",
      "Epoch [19/300], Step [103/112], Loss: 0.7074\n",
      "Epoch [19/300], Step [107/112], Loss: 0.7014\n",
      "Epoch [19/300], Step [111/112], Loss: 0.6630\n",
      "Epoch [20/300], Step [3/112], Loss: 0.6905\n",
      "Epoch [20/300], Step [7/112], Loss: 0.7003\n",
      "Epoch [20/300], Step [11/112], Loss: 0.7105\n",
      "Epoch [20/300], Step [15/112], Loss: 0.6906\n",
      "Epoch [20/300], Step [19/112], Loss: 0.6906\n",
      "Epoch [20/300], Step [23/112], Loss: 0.7138\n",
      "Epoch [20/300], Step [27/112], Loss: 0.6713\n",
      "Epoch [20/300], Step [31/112], Loss: 0.7189\n",
      "Epoch [20/300], Step [35/112], Loss: 0.6757\n",
      "Epoch [20/300], Step [39/112], Loss: 0.6848\n",
      "Epoch [20/300], Step [43/112], Loss: 0.6795\n",
      "Epoch [20/300], Step [47/112], Loss: 0.6743\n",
      "Epoch [20/300], Step [51/112], Loss: 0.6787\n",
      "Epoch [20/300], Step [55/112], Loss: 0.6827\n",
      "Epoch [20/300], Step [59/112], Loss: 0.6694\n",
      "Epoch [20/300], Step [63/112], Loss: 0.6944\n",
      "Epoch [20/300], Step [67/112], Loss: 0.6933\n",
      "Epoch [20/300], Step [71/112], Loss: 0.6968\n",
      "Epoch [20/300], Step [75/112], Loss: 0.7060\n",
      "Epoch [20/300], Step [79/112], Loss: 0.6893\n",
      "Epoch [20/300], Step [83/112], Loss: 0.7178\n",
      "Epoch [20/300], Step [87/112], Loss: 0.6920\n",
      "Epoch [20/300], Step [91/112], Loss: 0.7241\n",
      "Epoch [20/300], Step [95/112], Loss: 0.7164\n",
      "Epoch [20/300], Step [99/112], Loss: 0.7062\n",
      "Epoch [20/300], Step [103/112], Loss: 0.7060\n",
      "Epoch [20/300], Step [107/112], Loss: 0.6948\n",
      "Epoch [20/300], Step [111/112], Loss: 0.6570\n",
      "Epoch [21/300], Step [3/112], Loss: 0.6898\n",
      "Epoch [21/300], Step [7/112], Loss: 0.7054\n",
      "Epoch [21/300], Step [11/112], Loss: 0.7059\n",
      "Epoch [21/300], Step [15/112], Loss: 0.6916\n",
      "Epoch [21/300], Step [19/112], Loss: 0.6876\n",
      "Epoch [21/300], Step [23/112], Loss: 0.7054\n",
      "Epoch [21/300], Step [27/112], Loss: 0.6694\n",
      "Epoch [21/300], Step [31/112], Loss: 0.7165\n",
      "Epoch [21/300], Step [35/112], Loss: 0.6730\n",
      "Epoch [21/300], Step [39/112], Loss: 0.6797\n",
      "Epoch [21/300], Step [43/112], Loss: 0.6877\n",
      "Epoch [21/300], Step [47/112], Loss: 0.6728\n",
      "Epoch [21/300], Step [51/112], Loss: 0.6730\n",
      "Epoch [21/300], Step [55/112], Loss: 0.6781\n",
      "Epoch [21/300], Step [59/112], Loss: 0.6671\n",
      "Epoch [21/300], Step [63/112], Loss: 0.6927\n",
      "Epoch [21/300], Step [67/112], Loss: 0.6963\n",
      "Epoch [21/300], Step [71/112], Loss: 0.6898\n",
      "Epoch [21/300], Step [75/112], Loss: 0.7120\n",
      "Epoch [21/300], Step [79/112], Loss: 0.6911\n",
      "Epoch [21/300], Step [83/112], Loss: 0.7144\n",
      "Epoch [21/300], Step [87/112], Loss: 0.6956\n",
      "Epoch [21/300], Step [91/112], Loss: 0.7186\n",
      "Epoch [21/300], Step [95/112], Loss: 0.7068\n",
      "Epoch [21/300], Step [99/112], Loss: 0.7021\n",
      "Epoch [21/300], Step [103/112], Loss: 0.7007\n",
      "Epoch [21/300], Step [107/112], Loss: 0.6931\n",
      "Epoch [21/300], Step [111/112], Loss: 0.6537\n",
      "Epoch [22/300], Step [3/112], Loss: 0.6849\n",
      "Epoch [22/300], Step [7/112], Loss: 0.6988\n",
      "Epoch [22/300], Step [11/112], Loss: 0.7025\n",
      "Epoch [22/300], Step [15/112], Loss: 0.6870\n",
      "Epoch [22/300], Step [19/112], Loss: 0.6890\n",
      "Epoch [22/300], Step [23/112], Loss: 0.7012\n",
      "Epoch [22/300], Step [27/112], Loss: 0.6654\n",
      "Epoch [22/300], Step [31/112], Loss: 0.7111\n",
      "Epoch [22/300], Step [35/112], Loss: 0.6659\n",
      "Epoch [22/300], Step [39/112], Loss: 0.6788\n",
      "Epoch [22/300], Step [43/112], Loss: 0.6858\n",
      "Epoch [22/300], Step [47/112], Loss: 0.6728\n",
      "Epoch [22/300], Step [51/112], Loss: 0.6748\n",
      "Epoch [22/300], Step [55/112], Loss: 0.6778\n",
      "Epoch [22/300], Step [59/112], Loss: 0.6617\n",
      "Epoch [22/300], Step [63/112], Loss: 0.6874\n",
      "Epoch [22/300], Step [67/112], Loss: 0.6863\n",
      "Epoch [22/300], Step [71/112], Loss: 0.6972\n",
      "Epoch [22/300], Step [75/112], Loss: 0.7046\n",
      "Epoch [22/300], Step [79/112], Loss: 0.6817\n",
      "Epoch [22/300], Step [83/112], Loss: 0.7148\n",
      "Epoch [22/300], Step [87/112], Loss: 0.6898\n",
      "Epoch [22/300], Step [91/112], Loss: 0.7092\n",
      "Epoch [22/300], Step [95/112], Loss: 0.7132\n",
      "Epoch [22/300], Step [99/112], Loss: 0.7048\n",
      "Epoch [22/300], Step [103/112], Loss: 0.6951\n",
      "Epoch [22/300], Step [107/112], Loss: 0.6915\n",
      "Epoch [22/300], Step [111/112], Loss: 0.6544\n",
      "Epoch [23/300], Step [3/112], Loss: 0.6763\n",
      "Epoch [23/300], Step [7/112], Loss: 0.7001\n",
      "Epoch [23/300], Step [11/112], Loss: 0.7047\n",
      "Epoch [23/300], Step [15/112], Loss: 0.6853\n",
      "Epoch [23/300], Step [19/112], Loss: 0.6855\n",
      "Epoch [23/300], Step [23/112], Loss: 0.6955\n",
      "Epoch [23/300], Step [27/112], Loss: 0.6604\n",
      "Epoch [23/300], Step [31/112], Loss: 0.7107\n",
      "Epoch [23/300], Step [35/112], Loss: 0.6680\n",
      "Epoch [23/300], Step [39/112], Loss: 0.6698\n",
      "Epoch [23/300], Step [43/112], Loss: 0.6811\n",
      "Epoch [23/300], Step [47/112], Loss: 0.6691\n",
      "Epoch [23/300], Step [51/112], Loss: 0.6684\n",
      "Epoch [23/300], Step [55/112], Loss: 0.6752\n",
      "Epoch [23/300], Step [59/112], Loss: 0.6639\n",
      "Epoch [23/300], Step [63/112], Loss: 0.6858\n",
      "Epoch [23/300], Step [67/112], Loss: 0.6907\n",
      "Epoch [23/300], Step [71/112], Loss: 0.6924\n",
      "Epoch [23/300], Step [75/112], Loss: 0.7020\n",
      "Epoch [23/300], Step [79/112], Loss: 0.6730\n",
      "Epoch [23/300], Step [83/112], Loss: 0.7067\n",
      "Epoch [23/300], Step [87/112], Loss: 0.6864\n",
      "Epoch [23/300], Step [91/112], Loss: 0.7080\n",
      "Epoch [23/300], Step [95/112], Loss: 0.7136\n",
      "Epoch [23/300], Step [99/112], Loss: 0.6985\n",
      "Epoch [23/300], Step [103/112], Loss: 0.6930\n",
      "Epoch [23/300], Step [107/112], Loss: 0.6890\n",
      "Epoch [23/300], Step [111/112], Loss: 0.6612\n",
      "Epoch [24/300], Step [3/112], Loss: 0.6751\n",
      "Epoch [24/300], Step [7/112], Loss: 0.6979\n",
      "Epoch [24/300], Step [11/112], Loss: 0.7033\n",
      "Epoch [24/300], Step [15/112], Loss: 0.6902\n",
      "Epoch [24/300], Step [19/112], Loss: 0.6814\n",
      "Epoch [24/300], Step [23/112], Loss: 0.6895\n",
      "Epoch [24/300], Step [27/112], Loss: 0.6601\n",
      "Epoch [24/300], Step [31/112], Loss: 0.7106\n",
      "Epoch [24/300], Step [35/112], Loss: 0.6665\n",
      "Epoch [24/300], Step [39/112], Loss: 0.6744\n",
      "Epoch [24/300], Step [43/112], Loss: 0.6867\n",
      "Epoch [24/300], Step [47/112], Loss: 0.6732\n",
      "Epoch [24/300], Step [51/112], Loss: 0.6712\n",
      "Epoch [24/300], Step [55/112], Loss: 0.6770\n",
      "Epoch [24/300], Step [59/112], Loss: 0.6691\n",
      "Epoch [24/300], Step [63/112], Loss: 0.6898\n",
      "Epoch [24/300], Step [67/112], Loss: 0.6916\n",
      "Epoch [24/300], Step [71/112], Loss: 0.6921\n",
      "Epoch [24/300], Step [75/112], Loss: 0.7010\n",
      "Epoch [24/300], Step [79/112], Loss: 0.6740\n",
      "Epoch [24/300], Step [83/112], Loss: 0.7065\n",
      "Epoch [24/300], Step [87/112], Loss: 0.6884\n",
      "Epoch [24/300], Step [91/112], Loss: 0.7090\n",
      "Epoch [24/300], Step [95/112], Loss: 0.7100\n",
      "Epoch [24/300], Step [99/112], Loss: 0.6982\n",
      "Epoch [24/300], Step [103/112], Loss: 0.6939\n",
      "Epoch [24/300], Step [107/112], Loss: 0.6869\n",
      "Epoch [24/300], Step [111/112], Loss: 0.6502\n",
      "Epoch [25/300], Step [3/112], Loss: 0.6710\n",
      "Epoch [25/300], Step [7/112], Loss: 0.6921\n",
      "Epoch [25/300], Step [11/112], Loss: 0.7031\n",
      "Epoch [25/300], Step [15/112], Loss: 0.6847\n",
      "Epoch [25/300], Step [19/112], Loss: 0.6815\n",
      "Epoch [25/300], Step [23/112], Loss: 0.6968\n",
      "Epoch [25/300], Step [27/112], Loss: 0.6581\n",
      "Epoch [25/300], Step [31/112], Loss: 0.7087\n",
      "Epoch [25/300], Step [35/112], Loss: 0.6617\n",
      "Epoch [25/300], Step [39/112], Loss: 0.6716\n",
      "Epoch [25/300], Step [43/112], Loss: 0.6862\n",
      "Epoch [25/300], Step [47/112], Loss: 0.6777\n",
      "Epoch [25/300], Step [51/112], Loss: 0.6680\n",
      "Epoch [25/300], Step [55/112], Loss: 0.6814\n",
      "Epoch [25/300], Step [59/112], Loss: 0.6671\n",
      "Epoch [25/300], Step [63/112], Loss: 0.6892\n",
      "Epoch [25/300], Step [67/112], Loss: 0.6966\n",
      "Epoch [25/300], Step [71/112], Loss: 0.6850\n",
      "Epoch [25/300], Step [75/112], Loss: 0.7023\n",
      "Epoch [25/300], Step [79/112], Loss: 0.6715\n",
      "Epoch [25/300], Step [83/112], Loss: 0.7069\n",
      "Epoch [25/300], Step [87/112], Loss: 0.6817\n",
      "Epoch [25/300], Step [91/112], Loss: 0.7113\n",
      "Epoch [25/300], Step [95/112], Loss: 0.7081\n",
      "Epoch [25/300], Step [99/112], Loss: 0.6896\n",
      "Epoch [25/300], Step [103/112], Loss: 0.6974\n",
      "Epoch [25/300], Step [107/112], Loss: 0.6891\n",
      "Epoch [25/300], Step [111/112], Loss: 0.6552\n",
      "Epoch [26/300], Step [3/112], Loss: 0.6694\n",
      "Epoch [26/300], Step [7/112], Loss: 0.6900\n",
      "Epoch [26/300], Step [11/112], Loss: 0.7056\n",
      "Epoch [26/300], Step [15/112], Loss: 0.6842\n",
      "Epoch [26/300], Step [19/112], Loss: 0.6795\n",
      "Epoch [26/300], Step [23/112], Loss: 0.6878\n",
      "Epoch [26/300], Step [27/112], Loss: 0.6593\n",
      "Epoch [26/300], Step [31/112], Loss: 0.7066\n",
      "Epoch [26/300], Step [35/112], Loss: 0.6661\n",
      "Epoch [26/300], Step [39/112], Loss: 0.6688\n",
      "Epoch [26/300], Step [43/112], Loss: 0.6909\n",
      "Epoch [26/300], Step [47/112], Loss: 0.6781\n",
      "Epoch [26/300], Step [51/112], Loss: 0.6650\n",
      "Epoch [26/300], Step [55/112], Loss: 0.6776\n",
      "Epoch [26/300], Step [59/112], Loss: 0.6588\n",
      "Epoch [26/300], Step [63/112], Loss: 0.6862\n",
      "Epoch [26/300], Step [67/112], Loss: 0.6946\n",
      "Epoch [26/300], Step [71/112], Loss: 0.6916\n",
      "Epoch [26/300], Step [75/112], Loss: 0.7094\n",
      "Epoch [26/300], Step [79/112], Loss: 0.6740\n",
      "Epoch [26/300], Step [83/112], Loss: 0.7036\n",
      "Epoch [26/300], Step [87/112], Loss: 0.6887\n",
      "Epoch [26/300], Step [91/112], Loss: 0.7092\n",
      "Epoch [26/300], Step [95/112], Loss: 0.7091\n",
      "Epoch [26/300], Step [99/112], Loss: 0.6942\n",
      "Epoch [26/300], Step [103/112], Loss: 0.6876\n",
      "Epoch [26/300], Step [107/112], Loss: 0.6859\n",
      "Epoch [26/300], Step [111/112], Loss: 0.6502\n",
      "Epoch [27/300], Step [3/112], Loss: 0.6721\n",
      "Epoch [27/300], Step [7/112], Loss: 0.6894\n",
      "Epoch [27/300], Step [11/112], Loss: 0.7041\n",
      "Epoch [27/300], Step [15/112], Loss: 0.6808\n",
      "Epoch [27/300], Step [19/112], Loss: 0.6760\n",
      "Epoch [27/300], Step [23/112], Loss: 0.6832\n",
      "Epoch [27/300], Step [27/112], Loss: 0.6596\n",
      "Epoch [27/300], Step [31/112], Loss: 0.7031\n",
      "Epoch [27/300], Step [35/112], Loss: 0.6642\n",
      "Epoch [27/300], Step [39/112], Loss: 0.6650\n",
      "Epoch [27/300], Step [43/112], Loss: 0.6862\n",
      "Epoch [27/300], Step [47/112], Loss: 0.6725\n",
      "Epoch [27/300], Step [51/112], Loss: 0.6641\n",
      "Epoch [27/300], Step [55/112], Loss: 0.6824\n",
      "Epoch [27/300], Step [59/112], Loss: 0.6534\n",
      "Epoch [27/300], Step [63/112], Loss: 0.6833\n",
      "Epoch [27/300], Step [67/112], Loss: 0.6942\n",
      "Epoch [27/300], Step [71/112], Loss: 0.6911\n",
      "Epoch [27/300], Step [75/112], Loss: 0.7098\n",
      "Epoch [27/300], Step [79/112], Loss: 0.6725\n",
      "Epoch [27/300], Step [83/112], Loss: 0.7043\n",
      "Epoch [27/300], Step [87/112], Loss: 0.6849\n",
      "Epoch [27/300], Step [91/112], Loss: 0.7032\n",
      "Epoch [27/300], Step [95/112], Loss: 0.7004\n",
      "Epoch [27/300], Step [99/112], Loss: 0.6896\n",
      "Epoch [27/300], Step [103/112], Loss: 0.6861\n",
      "Epoch [27/300], Step [107/112], Loss: 0.6850\n",
      "Epoch [27/300], Step [111/112], Loss: 0.6547\n",
      "Epoch [28/300], Step [3/112], Loss: 0.6681\n",
      "Epoch [28/300], Step [7/112], Loss: 0.6897\n",
      "Epoch [28/300], Step [11/112], Loss: 0.6952\n",
      "Epoch [28/300], Step [15/112], Loss: 0.6782\n",
      "Epoch [28/300], Step [19/112], Loss: 0.6751\n",
      "Epoch [28/300], Step [23/112], Loss: 0.6842\n",
      "Epoch [28/300], Step [27/112], Loss: 0.6571\n",
      "Epoch [28/300], Step [31/112], Loss: 0.7111\n",
      "Epoch [28/300], Step [35/112], Loss: 0.6609\n",
      "Epoch [28/300], Step [39/112], Loss: 0.6707\n",
      "Epoch [28/300], Step [43/112], Loss: 0.6848\n",
      "Epoch [28/300], Step [47/112], Loss: 0.6673\n",
      "Epoch [28/300], Step [51/112], Loss: 0.6645\n",
      "Epoch [28/300], Step [55/112], Loss: 0.6766\n",
      "Epoch [28/300], Step [59/112], Loss: 0.6525\n",
      "Epoch [28/300], Step [63/112], Loss: 0.6756\n",
      "Epoch [28/300], Step [67/112], Loss: 0.6895\n",
      "Epoch [28/300], Step [71/112], Loss: 0.6867\n",
      "Epoch [28/300], Step [75/112], Loss: 0.7053\n",
      "Epoch [28/300], Step [79/112], Loss: 0.6684\n",
      "Epoch [28/300], Step [83/112], Loss: 0.6994\n",
      "Epoch [28/300], Step [87/112], Loss: 0.6819\n",
      "Epoch [28/300], Step [91/112], Loss: 0.7071\n",
      "Epoch [28/300], Step [95/112], Loss: 0.7039\n",
      "Epoch [28/300], Step [99/112], Loss: 0.6929\n",
      "Epoch [28/300], Step [103/112], Loss: 0.6893\n",
      "Epoch [28/300], Step [107/112], Loss: 0.6848\n",
      "Epoch [28/300], Step [111/112], Loss: 0.6485\n",
      "Epoch [29/300], Step [3/112], Loss: 0.6663\n",
      "Epoch [29/300], Step [7/112], Loss: 0.6850\n",
      "Epoch [29/300], Step [11/112], Loss: 0.6987\n",
      "Epoch [29/300], Step [15/112], Loss: 0.6768\n",
      "Epoch [29/300], Step [19/112], Loss: 0.6695\n",
      "Epoch [29/300], Step [23/112], Loss: 0.6769\n",
      "Epoch [29/300], Step [27/112], Loss: 0.6604\n",
      "Epoch [29/300], Step [31/112], Loss: 0.7105\n",
      "Epoch [29/300], Step [35/112], Loss: 0.6596\n",
      "Epoch [29/300], Step [39/112], Loss: 0.6655\n",
      "Epoch [29/300], Step [43/112], Loss: 0.6816\n",
      "Epoch [29/300], Step [47/112], Loss: 0.6681\n",
      "Epoch [29/300], Step [51/112], Loss: 0.6652\n",
      "Epoch [29/300], Step [55/112], Loss: 0.6788\n",
      "Epoch [29/300], Step [59/112], Loss: 0.6473\n",
      "Epoch [29/300], Step [63/112], Loss: 0.6719\n",
      "Epoch [29/300], Step [67/112], Loss: 0.6854\n",
      "Epoch [29/300], Step [71/112], Loss: 0.6816\n",
      "Epoch [29/300], Step [75/112], Loss: 0.6997\n",
      "Epoch [29/300], Step [79/112], Loss: 0.6638\n",
      "Epoch [29/300], Step [83/112], Loss: 0.7028\n",
      "Epoch [29/300], Step [87/112], Loss: 0.6759\n",
      "Epoch [29/300], Step [91/112], Loss: 0.7065\n",
      "Epoch [29/300], Step [95/112], Loss: 0.7027\n",
      "Epoch [29/300], Step [99/112], Loss: 0.6905\n",
      "Epoch [29/300], Step [103/112], Loss: 0.6888\n",
      "Epoch [29/300], Step [107/112], Loss: 0.6833\n",
      "Epoch [29/300], Step [111/112], Loss: 0.6458\n",
      "Epoch [30/300], Step [3/112], Loss: 0.6677\n",
      "Epoch [30/300], Step [7/112], Loss: 0.6914\n",
      "Epoch [30/300], Step [11/112], Loss: 0.6993\n",
      "Epoch [30/300], Step [15/112], Loss: 0.6781\n",
      "Epoch [30/300], Step [19/112], Loss: 0.6726\n",
      "Epoch [30/300], Step [23/112], Loss: 0.6798\n",
      "Epoch [30/300], Step [27/112], Loss: 0.6540\n",
      "Epoch [30/300], Step [31/112], Loss: 0.7020\n",
      "Epoch [30/300], Step [35/112], Loss: 0.6595\n",
      "Epoch [30/300], Step [39/112], Loss: 0.6656\n",
      "Epoch [30/300], Step [43/112], Loss: 0.6767\n",
      "Epoch [30/300], Step [47/112], Loss: 0.6660\n",
      "Epoch [30/300], Step [51/112], Loss: 0.6607\n",
      "Epoch [30/300], Step [55/112], Loss: 0.6739\n",
      "Epoch [30/300], Step [59/112], Loss: 0.6517\n",
      "Epoch [30/300], Step [63/112], Loss: 0.6803\n",
      "Epoch [30/300], Step [67/112], Loss: 0.6872\n",
      "Epoch [30/300], Step [71/112], Loss: 0.6828\n",
      "Epoch [30/300], Step [75/112], Loss: 0.6950\n",
      "Epoch [30/300], Step [79/112], Loss: 0.6656\n",
      "Epoch [30/300], Step [83/112], Loss: 0.6945\n",
      "Epoch [30/300], Step [87/112], Loss: 0.6762\n",
      "Epoch [30/300], Step [91/112], Loss: 0.7081\n",
      "Epoch [30/300], Step [95/112], Loss: 0.6964\n",
      "Epoch [30/300], Step [99/112], Loss: 0.6878\n",
      "Epoch [30/300], Step [103/112], Loss: 0.6889\n",
      "Epoch [30/300], Step [107/112], Loss: 0.6865\n",
      "Epoch [30/300], Step [111/112], Loss: 0.6455\n",
      "Epoch [31/300], Step [3/112], Loss: 0.6656\n",
      "Epoch [31/300], Step [7/112], Loss: 0.6843\n",
      "Epoch [31/300], Step [11/112], Loss: 0.6964\n",
      "Epoch [31/300], Step [15/112], Loss: 0.6794\n",
      "Epoch [31/300], Step [19/112], Loss: 0.6717\n",
      "Epoch [31/300], Step [23/112], Loss: 0.6760\n",
      "Epoch [31/300], Step [27/112], Loss: 0.6570\n",
      "Epoch [31/300], Step [31/112], Loss: 0.6977\n",
      "Epoch [31/300], Step [35/112], Loss: 0.6599\n",
      "Epoch [31/300], Step [39/112], Loss: 0.6632\n",
      "Epoch [31/300], Step [43/112], Loss: 0.6717\n",
      "Epoch [31/300], Step [47/112], Loss: 0.6667\n",
      "Epoch [31/300], Step [51/112], Loss: 0.6613\n",
      "Epoch [31/300], Step [55/112], Loss: 0.6744\n",
      "Epoch [31/300], Step [59/112], Loss: 0.6459\n",
      "Epoch [31/300], Step [63/112], Loss: 0.6717\n",
      "Epoch [31/300], Step [67/112], Loss: 0.6883\n",
      "Epoch [31/300], Step [71/112], Loss: 0.6855\n",
      "Epoch [31/300], Step [75/112], Loss: 0.6975\n",
      "Epoch [31/300], Step [79/112], Loss: 0.6641\n",
      "Epoch [31/300], Step [83/112], Loss: 0.6955\n",
      "Epoch [31/300], Step [87/112], Loss: 0.6754\n",
      "Epoch [31/300], Step [91/112], Loss: 0.7022\n",
      "Epoch [31/300], Step [95/112], Loss: 0.6950\n",
      "Epoch [31/300], Step [99/112], Loss: 0.6914\n",
      "Epoch [31/300], Step [103/112], Loss: 0.6843\n",
      "Epoch [31/300], Step [107/112], Loss: 0.6821\n",
      "Epoch [31/300], Step [111/112], Loss: 0.6452\n",
      "Epoch [32/300], Step [3/112], Loss: 0.6652\n",
      "Epoch [32/300], Step [7/112], Loss: 0.6833\n",
      "Epoch [32/300], Step [11/112], Loss: 0.6974\n",
      "Epoch [32/300], Step [15/112], Loss: 0.6707\n",
      "Epoch [32/300], Step [19/112], Loss: 0.6709\n",
      "Epoch [32/300], Step [23/112], Loss: 0.6742\n",
      "Epoch [32/300], Step [27/112], Loss: 0.6529\n",
      "Epoch [32/300], Step [31/112], Loss: 0.6986\n",
      "Epoch [32/300], Step [35/112], Loss: 0.6551\n",
      "Epoch [32/300], Step [39/112], Loss: 0.6613\n",
      "Epoch [32/300], Step [43/112], Loss: 0.6789\n",
      "Epoch [32/300], Step [47/112], Loss: 0.6634\n",
      "Epoch [32/300], Step [51/112], Loss: 0.6589\n",
      "Epoch [32/300], Step [55/112], Loss: 0.6724\n",
      "Epoch [32/300], Step [59/112], Loss: 0.6444\n",
      "Epoch [32/300], Step [63/112], Loss: 0.6695\n",
      "Epoch [32/300], Step [67/112], Loss: 0.6867\n",
      "Epoch [32/300], Step [71/112], Loss: 0.6842\n",
      "Epoch [32/300], Step [75/112], Loss: 0.6964\n",
      "Epoch [32/300], Step [79/112], Loss: 0.6657\n",
      "Epoch [32/300], Step [83/112], Loss: 0.6930\n",
      "Epoch [32/300], Step [87/112], Loss: 0.6761\n",
      "Epoch [32/300], Step [91/112], Loss: 0.6985\n",
      "Epoch [32/300], Step [95/112], Loss: 0.6944\n",
      "Epoch [32/300], Step [99/112], Loss: 0.6835\n",
      "Epoch [32/300], Step [103/112], Loss: 0.6802\n",
      "Epoch [32/300], Step [107/112], Loss: 0.6800\n",
      "Epoch [32/300], Step [111/112], Loss: 0.6382\n",
      "Epoch [33/300], Step [3/112], Loss: 0.6616\n",
      "Epoch [33/300], Step [7/112], Loss: 0.6850\n",
      "Epoch [33/300], Step [11/112], Loss: 0.6990\n",
      "Epoch [33/300], Step [15/112], Loss: 0.6795\n",
      "Epoch [33/300], Step [19/112], Loss: 0.6704\n",
      "Epoch [33/300], Step [23/112], Loss: 0.6757\n",
      "Epoch [33/300], Step [27/112], Loss: 0.6546\n",
      "Epoch [33/300], Step [31/112], Loss: 0.7020\n",
      "Epoch [33/300], Step [35/112], Loss: 0.6528\n",
      "Epoch [33/300], Step [39/112], Loss: 0.6624\n",
      "Epoch [33/300], Step [43/112], Loss: 0.6763\n",
      "Epoch [33/300], Step [47/112], Loss: 0.6661\n",
      "Epoch [33/300], Step [51/112], Loss: 0.6582\n",
      "Epoch [33/300], Step [55/112], Loss: 0.6667\n",
      "Epoch [33/300], Step [59/112], Loss: 0.6464\n",
      "Epoch [33/300], Step [63/112], Loss: 0.6710\n",
      "Epoch [33/300], Step [67/112], Loss: 0.6842\n",
      "Epoch [33/300], Step [71/112], Loss: 0.6808\n",
      "Epoch [33/300], Step [75/112], Loss: 0.7035\n",
      "Epoch [33/300], Step [79/112], Loss: 0.6656\n",
      "Epoch [33/300], Step [83/112], Loss: 0.6944\n",
      "Epoch [33/300], Step [87/112], Loss: 0.6719\n",
      "Epoch [33/300], Step [91/112], Loss: 0.7032\n",
      "Epoch [33/300], Step [95/112], Loss: 0.6970\n",
      "Epoch [33/300], Step [99/112], Loss: 0.6857\n",
      "Epoch [33/300], Step [103/112], Loss: 0.6828\n",
      "Epoch [33/300], Step [107/112], Loss: 0.6783\n",
      "Epoch [33/300], Step [111/112], Loss: 0.6412\n",
      "Epoch [34/300], Step [3/112], Loss: 0.6675\n",
      "Epoch [34/300], Step [7/112], Loss: 0.6782\n",
      "Epoch [34/300], Step [11/112], Loss: 0.6888\n",
      "Epoch [34/300], Step [15/112], Loss: 0.6753\n",
      "Epoch [34/300], Step [19/112], Loss: 0.6693\n",
      "Epoch [34/300], Step [23/112], Loss: 0.6735\n",
      "Epoch [34/300], Step [27/112], Loss: 0.6485\n",
      "Epoch [34/300], Step [31/112], Loss: 0.7004\n",
      "Epoch [34/300], Step [35/112], Loss: 0.6533\n",
      "Epoch [34/300], Step [39/112], Loss: 0.6541\n",
      "Epoch [34/300], Step [43/112], Loss: 0.6756\n",
      "Epoch [34/300], Step [47/112], Loss: 0.6629\n",
      "Epoch [34/300], Step [51/112], Loss: 0.6615\n",
      "Epoch [34/300], Step [55/112], Loss: 0.6697\n",
      "Epoch [34/300], Step [59/112], Loss: 0.6509\n",
      "Epoch [34/300], Step [63/112], Loss: 0.6753\n",
      "Epoch [34/300], Step [67/112], Loss: 0.6850\n",
      "Epoch [34/300], Step [71/112], Loss: 0.6763\n",
      "Epoch [34/300], Step [75/112], Loss: 0.6944\n",
      "Epoch [34/300], Step [79/112], Loss: 0.6630\n",
      "Epoch [34/300], Step [83/112], Loss: 0.6935\n",
      "Epoch [34/300], Step [87/112], Loss: 0.6710\n",
      "Epoch [34/300], Step [91/112], Loss: 0.7017\n",
      "Epoch [34/300], Step [95/112], Loss: 0.6984\n",
      "Epoch [34/300], Step [99/112], Loss: 0.6850\n",
      "Epoch [34/300], Step [103/112], Loss: 0.6799\n",
      "Epoch [34/300], Step [107/112], Loss: 0.6762\n",
      "Epoch [34/300], Step [111/112], Loss: 0.6393\n",
      "Epoch [35/300], Step [3/112], Loss: 0.6625\n",
      "Epoch [35/300], Step [7/112], Loss: 0.6763\n",
      "Epoch [35/300], Step [11/112], Loss: 0.6846\n",
      "Epoch [35/300], Step [15/112], Loss: 0.6731\n",
      "Epoch [35/300], Step [19/112], Loss: 0.6651\n",
      "Epoch [35/300], Step [23/112], Loss: 0.6771\n",
      "Epoch [35/300], Step [27/112], Loss: 0.6514\n",
      "Epoch [35/300], Step [31/112], Loss: 0.7008\n",
      "Epoch [35/300], Step [35/112], Loss: 0.6518\n",
      "Epoch [35/300], Step [39/112], Loss: 0.6559\n",
      "Epoch [35/300], Step [43/112], Loss: 0.6844\n",
      "Epoch [35/300], Step [47/112], Loss: 0.6670\n",
      "Epoch [35/300], Step [51/112], Loss: 0.6606\n",
      "Epoch [35/300], Step [55/112], Loss: 0.6703\n",
      "Epoch [35/300], Step [59/112], Loss: 0.6521\n",
      "Epoch [35/300], Step [63/112], Loss: 0.6739\n",
      "Epoch [35/300], Step [67/112], Loss: 0.6853\n",
      "Epoch [35/300], Step [71/112], Loss: 0.6721\n",
      "Epoch [35/300], Step [75/112], Loss: 0.6885\n",
      "Epoch [35/300], Step [79/112], Loss: 0.6579\n",
      "Epoch [35/300], Step [83/112], Loss: 0.6820\n",
      "Epoch [35/300], Step [87/112], Loss: 0.6673\n",
      "Epoch [35/300], Step [91/112], Loss: 0.6987\n",
      "Epoch [35/300], Step [95/112], Loss: 0.6934\n",
      "Epoch [35/300], Step [99/112], Loss: 0.6890\n",
      "Epoch [35/300], Step [103/112], Loss: 0.6819\n",
      "Epoch [35/300], Step [107/112], Loss: 0.6771\n",
      "Epoch [35/300], Step [111/112], Loss: 0.6460\n",
      "Epoch [36/300], Step [3/112], Loss: 0.6602\n",
      "Epoch [36/300], Step [7/112], Loss: 0.6675\n",
      "Epoch [36/300], Step [11/112], Loss: 0.6856\n",
      "Epoch [36/300], Step [15/112], Loss: 0.6699\n",
      "Epoch [36/300], Step [19/112], Loss: 0.6678\n",
      "Epoch [36/300], Step [23/112], Loss: 0.6725\n",
      "Epoch [36/300], Step [27/112], Loss: 0.6445\n",
      "Epoch [36/300], Step [31/112], Loss: 0.6993\n",
      "Epoch [36/300], Step [35/112], Loss: 0.6520\n",
      "Epoch [36/300], Step [39/112], Loss: 0.6563\n",
      "Epoch [36/300], Step [43/112], Loss: 0.6775\n",
      "Epoch [36/300], Step [47/112], Loss: 0.6636\n",
      "Epoch [36/300], Step [51/112], Loss: 0.6552\n",
      "Epoch [36/300], Step [55/112], Loss: 0.6689\n",
      "Epoch [36/300], Step [59/112], Loss: 0.6473\n",
      "Epoch [36/300], Step [63/112], Loss: 0.6702\n",
      "Epoch [36/300], Step [67/112], Loss: 0.6794\n",
      "Epoch [36/300], Step [71/112], Loss: 0.6760\n",
      "Epoch [36/300], Step [75/112], Loss: 0.6874\n",
      "Epoch [36/300], Step [79/112], Loss: 0.6569\n",
      "Epoch [36/300], Step [83/112], Loss: 0.6837\n",
      "Epoch [36/300], Step [87/112], Loss: 0.6678\n",
      "Epoch [36/300], Step [91/112], Loss: 0.6980\n",
      "Epoch [36/300], Step [95/112], Loss: 0.6967\n",
      "Epoch [36/300], Step [99/112], Loss: 0.6854\n",
      "Epoch [36/300], Step [103/112], Loss: 0.6784\n",
      "Epoch [36/300], Step [107/112], Loss: 0.6753\n",
      "Epoch [36/300], Step [111/112], Loss: 0.6395\n",
      "Epoch [37/300], Step [3/112], Loss: 0.6627\n",
      "Epoch [37/300], Step [7/112], Loss: 0.6758\n",
      "Epoch [37/300], Step [11/112], Loss: 0.6856\n",
      "Epoch [37/300], Step [15/112], Loss: 0.6791\n",
      "Epoch [37/300], Step [19/112], Loss: 0.6611\n",
      "Epoch [37/300], Step [23/112], Loss: 0.6680\n",
      "Epoch [37/300], Step [27/112], Loss: 0.6458\n",
      "Epoch [37/300], Step [31/112], Loss: 0.6997\n",
      "Epoch [37/300], Step [35/112], Loss: 0.6572\n",
      "Epoch [37/300], Step [39/112], Loss: 0.6548\n",
      "Epoch [37/300], Step [43/112], Loss: 0.6733\n",
      "Epoch [37/300], Step [47/112], Loss: 0.6585\n",
      "Epoch [37/300], Step [51/112], Loss: 0.6601\n",
      "Epoch [37/300], Step [55/112], Loss: 0.6695\n",
      "Epoch [37/300], Step [59/112], Loss: 0.6456\n",
      "Epoch [37/300], Step [63/112], Loss: 0.6717\n",
      "Epoch [37/300], Step [67/112], Loss: 0.6784\n",
      "Epoch [37/300], Step [71/112], Loss: 0.6684\n",
      "Epoch [37/300], Step [75/112], Loss: 0.6916\n",
      "Epoch [37/300], Step [79/112], Loss: 0.6559\n",
      "Epoch [37/300], Step [83/112], Loss: 0.6803\n",
      "Epoch [37/300], Step [87/112], Loss: 0.6732\n",
      "Epoch [37/300], Step [91/112], Loss: 0.6959\n",
      "Epoch [37/300], Step [95/112], Loss: 0.6937\n",
      "Epoch [37/300], Step [99/112], Loss: 0.6828\n",
      "Epoch [37/300], Step [103/112], Loss: 0.6718\n",
      "Epoch [37/300], Step [107/112], Loss: 0.6796\n",
      "Epoch [37/300], Step [111/112], Loss: 0.6410\n",
      "Epoch [38/300], Step [3/112], Loss: 0.6656\n",
      "Epoch [38/300], Step [7/112], Loss: 0.6761\n",
      "Epoch [38/300], Step [11/112], Loss: 0.6838\n",
      "Epoch [38/300], Step [15/112], Loss: 0.6660\n",
      "Epoch [38/300], Step [19/112], Loss: 0.6654\n",
      "Epoch [38/300], Step [23/112], Loss: 0.6728\n",
      "Epoch [38/300], Step [27/112], Loss: 0.6522\n",
      "Epoch [38/300], Step [31/112], Loss: 0.7020\n",
      "Epoch [38/300], Step [35/112], Loss: 0.6582\n",
      "Epoch [38/300], Step [39/112], Loss: 0.6638\n",
      "Epoch [38/300], Step [43/112], Loss: 0.6765\n",
      "Epoch [38/300], Step [47/112], Loss: 0.6622\n",
      "Epoch [38/300], Step [51/112], Loss: 0.6594\n",
      "Epoch [38/300], Step [55/112], Loss: 0.6725\n",
      "Epoch [38/300], Step [59/112], Loss: 0.6442\n",
      "Epoch [38/300], Step [63/112], Loss: 0.6763\n",
      "Epoch [38/300], Step [67/112], Loss: 0.6780\n",
      "Epoch [38/300], Step [71/112], Loss: 0.6692\n",
      "Epoch [38/300], Step [75/112], Loss: 0.6988\n",
      "Epoch [38/300], Step [79/112], Loss: 0.6581\n",
      "Epoch [38/300], Step [83/112], Loss: 0.6800\n",
      "Epoch [38/300], Step [87/112], Loss: 0.6662\n",
      "Epoch [38/300], Step [91/112], Loss: 0.6959\n",
      "Epoch [38/300], Step [95/112], Loss: 0.6907\n",
      "Epoch [38/300], Step [99/112], Loss: 0.6819\n",
      "Epoch [38/300], Step [103/112], Loss: 0.6755\n",
      "Epoch [38/300], Step [107/112], Loss: 0.6753\n",
      "Epoch [38/300], Step [111/112], Loss: 0.6389\n",
      "Epoch [39/300], Step [3/112], Loss: 0.6652\n",
      "Epoch [39/300], Step [7/112], Loss: 0.6720\n",
      "Epoch [39/300], Step [11/112], Loss: 0.6845\n",
      "Epoch [39/300], Step [15/112], Loss: 0.6599\n",
      "Epoch [39/300], Step [19/112], Loss: 0.6667\n",
      "Epoch [39/300], Step [23/112], Loss: 0.6738\n",
      "Epoch [39/300], Step [27/112], Loss: 0.6545\n",
      "Epoch [39/300], Step [31/112], Loss: 0.7018\n",
      "Epoch [39/300], Step [35/112], Loss: 0.6568\n",
      "Epoch [39/300], Step [39/112], Loss: 0.6541\n",
      "Epoch [39/300], Step [43/112], Loss: 0.6732\n",
      "Epoch [39/300], Step [47/112], Loss: 0.6574\n",
      "Epoch [39/300], Step [51/112], Loss: 0.6578\n",
      "Epoch [39/300], Step [55/112], Loss: 0.6687\n",
      "Epoch [39/300], Step [59/112], Loss: 0.6417\n",
      "Epoch [39/300], Step [63/112], Loss: 0.6714\n",
      "Epoch [39/300], Step [67/112], Loss: 0.6780\n",
      "Epoch [39/300], Step [71/112], Loss: 0.6734\n",
      "Epoch [39/300], Step [75/112], Loss: 0.6899\n",
      "Epoch [39/300], Step [79/112], Loss: 0.6573\n",
      "Epoch [39/300], Step [83/112], Loss: 0.6780\n",
      "Epoch [39/300], Step [87/112], Loss: 0.6713\n",
      "Epoch [39/300], Step [91/112], Loss: 0.6938\n",
      "Epoch [39/300], Step [95/112], Loss: 0.6944\n",
      "Epoch [39/300], Step [99/112], Loss: 0.6827\n",
      "Epoch [39/300], Step [103/112], Loss: 0.6792\n",
      "Epoch [39/300], Step [107/112], Loss: 0.6794\n",
      "Epoch [39/300], Step [111/112], Loss: 0.6310\n",
      "Epoch [40/300], Step [3/112], Loss: 0.6605\n",
      "Epoch [40/300], Step [7/112], Loss: 0.6703\n",
      "Epoch [40/300], Step [11/112], Loss: 0.6843\n",
      "Epoch [40/300], Step [15/112], Loss: 0.6685\n",
      "Epoch [40/300], Step [19/112], Loss: 0.6718\n",
      "Epoch [40/300], Step [23/112], Loss: 0.6702\n",
      "Epoch [40/300], Step [27/112], Loss: 0.6536\n",
      "Epoch [40/300], Step [31/112], Loss: 0.7001\n",
      "Epoch [40/300], Step [35/112], Loss: 0.6561\n",
      "Epoch [40/300], Step [39/112], Loss: 0.6584\n",
      "Epoch [40/300], Step [43/112], Loss: 0.6742\n",
      "Epoch [40/300], Step [47/112], Loss: 0.6604\n",
      "Epoch [40/300], Step [51/112], Loss: 0.6585\n",
      "Epoch [40/300], Step [55/112], Loss: 0.6734\n",
      "Epoch [40/300], Step [59/112], Loss: 0.6403\n",
      "Epoch [40/300], Step [63/112], Loss: 0.6737\n",
      "Epoch [40/300], Step [67/112], Loss: 0.6775\n",
      "Epoch [40/300], Step [71/112], Loss: 0.6747\n",
      "Epoch [40/300], Step [75/112], Loss: 0.6819\n",
      "Epoch [40/300], Step [79/112], Loss: 0.6567\n",
      "Epoch [40/300], Step [83/112], Loss: 0.6864\n",
      "Epoch [40/300], Step [87/112], Loss: 0.6653\n",
      "Epoch [40/300], Step [91/112], Loss: 0.6941\n",
      "Epoch [40/300], Step [95/112], Loss: 0.6900\n",
      "Epoch [40/300], Step [99/112], Loss: 0.6822\n",
      "Epoch [40/300], Step [103/112], Loss: 0.6809\n",
      "Epoch [40/300], Step [107/112], Loss: 0.6786\n",
      "Epoch [40/300], Step [111/112], Loss: 0.6364\n",
      "Epoch [41/300], Step [3/112], Loss: 0.6614\n",
      "Epoch [41/300], Step [7/112], Loss: 0.6777\n",
      "Epoch [41/300], Step [11/112], Loss: 0.6809\n",
      "Epoch [41/300], Step [15/112], Loss: 0.6627\n",
      "Epoch [41/300], Step [19/112], Loss: 0.6654\n",
      "Epoch [41/300], Step [23/112], Loss: 0.6650\n",
      "Epoch [41/300], Step [27/112], Loss: 0.6496\n",
      "Epoch [41/300], Step [31/112], Loss: 0.6962\n",
      "Epoch [41/300], Step [35/112], Loss: 0.6564\n",
      "Epoch [41/300], Step [39/112], Loss: 0.6611\n",
      "Epoch [41/300], Step [43/112], Loss: 0.6702\n",
      "Epoch [41/300], Step [47/112], Loss: 0.6565\n",
      "Epoch [41/300], Step [51/112], Loss: 0.6552\n",
      "Epoch [41/300], Step [55/112], Loss: 0.6717\n",
      "Epoch [41/300], Step [59/112], Loss: 0.6450\n",
      "Epoch [41/300], Step [63/112], Loss: 0.6772\n",
      "Epoch [41/300], Step [67/112], Loss: 0.6796\n",
      "Epoch [41/300], Step [71/112], Loss: 0.6736\n",
      "Epoch [41/300], Step [75/112], Loss: 0.6896\n",
      "Epoch [41/300], Step [79/112], Loss: 0.6524\n",
      "Epoch [41/300], Step [83/112], Loss: 0.6829\n",
      "Epoch [41/300], Step [87/112], Loss: 0.6655\n",
      "Epoch [41/300], Step [91/112], Loss: 0.6935\n",
      "Epoch [41/300], Step [95/112], Loss: 0.6889\n",
      "Epoch [41/300], Step [99/112], Loss: 0.6791\n",
      "Epoch [41/300], Step [103/112], Loss: 0.6699\n",
      "Epoch [41/300], Step [107/112], Loss: 0.6742\n",
      "Epoch [41/300], Step [111/112], Loss: 0.6302\n",
      "Epoch [42/300], Step [3/112], Loss: 0.6567\n",
      "Epoch [42/300], Step [7/112], Loss: 0.6702\n",
      "Epoch [42/300], Step [11/112], Loss: 0.6782\n",
      "Epoch [42/300], Step [15/112], Loss: 0.6650\n",
      "Epoch [42/300], Step [19/112], Loss: 0.6659\n",
      "Epoch [42/300], Step [23/112], Loss: 0.6712\n",
      "Epoch [42/300], Step [27/112], Loss: 0.6453\n",
      "Epoch [42/300], Step [31/112], Loss: 0.6972\n",
      "Epoch [42/300], Step [35/112], Loss: 0.6501\n",
      "Epoch [42/300], Step [39/112], Loss: 0.6576\n",
      "Epoch [42/300], Step [43/112], Loss: 0.6717\n",
      "Epoch [42/300], Step [47/112], Loss: 0.6548\n",
      "Epoch [42/300], Step [51/112], Loss: 0.6631\n",
      "Epoch [42/300], Step [55/112], Loss: 0.6715\n",
      "Epoch [42/300], Step [59/112], Loss: 0.6421\n",
      "Epoch [42/300], Step [63/112], Loss: 0.6725\n",
      "Epoch [42/300], Step [67/112], Loss: 0.6837\n",
      "Epoch [42/300], Step [71/112], Loss: 0.6676\n",
      "Epoch [42/300], Step [75/112], Loss: 0.6849\n",
      "Epoch [42/300], Step [79/112], Loss: 0.6529\n",
      "Epoch [42/300], Step [83/112], Loss: 0.6764\n",
      "Epoch [42/300], Step [87/112], Loss: 0.6701\n",
      "Epoch [42/300], Step [91/112], Loss: 0.6956\n",
      "Epoch [42/300], Step [95/112], Loss: 0.6811\n",
      "Epoch [42/300], Step [99/112], Loss: 0.6800\n",
      "Epoch [42/300], Step [103/112], Loss: 0.6713\n",
      "Epoch [42/300], Step [107/112], Loss: 0.6712\n",
      "Epoch [42/300], Step [111/112], Loss: 0.6327\n",
      "Epoch [43/300], Step [3/112], Loss: 0.6610\n",
      "Epoch [43/300], Step [7/112], Loss: 0.6682\n",
      "Epoch [43/300], Step [11/112], Loss: 0.6804\n",
      "Epoch [43/300], Step [15/112], Loss: 0.6599\n",
      "Epoch [43/300], Step [19/112], Loss: 0.6665\n",
      "Epoch [43/300], Step [23/112], Loss: 0.6661\n",
      "Epoch [43/300], Step [27/112], Loss: 0.6446\n",
      "Epoch [43/300], Step [31/112], Loss: 0.6991\n",
      "Epoch [43/300], Step [35/112], Loss: 0.6579\n",
      "Epoch [43/300], Step [39/112], Loss: 0.6583\n",
      "Epoch [43/300], Step [43/112], Loss: 0.6730\n",
      "Epoch [43/300], Step [47/112], Loss: 0.6569\n",
      "Epoch [43/300], Step [51/112], Loss: 0.6594\n",
      "Epoch [43/300], Step [55/112], Loss: 0.6655\n",
      "Epoch [43/300], Step [59/112], Loss: 0.6410\n",
      "Epoch [43/300], Step [63/112], Loss: 0.6704\n",
      "Epoch [43/300], Step [67/112], Loss: 0.6794\n",
      "Epoch [43/300], Step [71/112], Loss: 0.6677\n",
      "Epoch [43/300], Step [75/112], Loss: 0.6846\n",
      "Epoch [43/300], Step [79/112], Loss: 0.6509\n",
      "Epoch [43/300], Step [83/112], Loss: 0.6783\n",
      "Epoch [43/300], Step [87/112], Loss: 0.6672\n",
      "Epoch [43/300], Step [91/112], Loss: 0.6943\n",
      "Epoch [43/300], Step [95/112], Loss: 0.6880\n",
      "Epoch [43/300], Step [99/112], Loss: 0.6768\n",
      "Epoch [43/300], Step [103/112], Loss: 0.6702\n",
      "Epoch [43/300], Step [107/112], Loss: 0.6690\n",
      "Epoch [43/300], Step [111/112], Loss: 0.6288\n",
      "Epoch [44/300], Step [3/112], Loss: 0.6641\n",
      "Epoch [44/300], Step [7/112], Loss: 0.6689\n",
      "Epoch [44/300], Step [11/112], Loss: 0.6739\n",
      "Epoch [44/300], Step [15/112], Loss: 0.6592\n",
      "Epoch [44/300], Step [19/112], Loss: 0.6654\n",
      "Epoch [44/300], Step [23/112], Loss: 0.6680\n",
      "Epoch [44/300], Step [27/112], Loss: 0.6467\n",
      "Epoch [44/300], Step [31/112], Loss: 0.6905\n",
      "Epoch [44/300], Step [35/112], Loss: 0.6534\n",
      "Epoch [44/300], Step [39/112], Loss: 0.6609\n",
      "Epoch [44/300], Step [43/112], Loss: 0.6703\n",
      "Epoch [44/300], Step [47/112], Loss: 0.6529\n",
      "Epoch [44/300], Step [51/112], Loss: 0.6552\n",
      "Epoch [44/300], Step [55/112], Loss: 0.6707\n",
      "Epoch [44/300], Step [59/112], Loss: 0.6442\n",
      "Epoch [44/300], Step [63/112], Loss: 0.6692\n",
      "Epoch [44/300], Step [67/112], Loss: 0.6734\n",
      "Epoch [44/300], Step [71/112], Loss: 0.6705\n",
      "Epoch [44/300], Step [75/112], Loss: 0.6834\n",
      "Epoch [44/300], Step [79/112], Loss: 0.6586\n",
      "Epoch [44/300], Step [83/112], Loss: 0.6803\n",
      "Epoch [44/300], Step [87/112], Loss: 0.6605\n",
      "Epoch [44/300], Step [91/112], Loss: 0.6930\n",
      "Epoch [44/300], Step [95/112], Loss: 0.6753\n",
      "Epoch [44/300], Step [99/112], Loss: 0.6762\n",
      "Epoch [44/300], Step [103/112], Loss: 0.6665\n",
      "Epoch [44/300], Step [107/112], Loss: 0.6713\n",
      "Epoch [44/300], Step [111/112], Loss: 0.6310\n",
      "Epoch [45/300], Step [3/112], Loss: 0.6593\n",
      "Epoch [45/300], Step [7/112], Loss: 0.6690\n",
      "Epoch [45/300], Step [11/112], Loss: 0.6750\n",
      "Epoch [45/300], Step [15/112], Loss: 0.6631\n",
      "Epoch [45/300], Step [19/112], Loss: 0.6612\n",
      "Epoch [45/300], Step [23/112], Loss: 0.6672\n",
      "Epoch [45/300], Step [27/112], Loss: 0.6391\n",
      "Epoch [45/300], Step [31/112], Loss: 0.6920\n",
      "Epoch [45/300], Step [35/112], Loss: 0.6464\n",
      "Epoch [45/300], Step [39/112], Loss: 0.6522\n",
      "Epoch [45/300], Step [43/112], Loss: 0.6666\n",
      "Epoch [45/300], Step [47/112], Loss: 0.6534\n",
      "Epoch [45/300], Step [51/112], Loss: 0.6517\n",
      "Epoch [45/300], Step [55/112], Loss: 0.6643\n",
      "Epoch [45/300], Step [59/112], Loss: 0.6426\n",
      "Epoch [45/300], Step [63/112], Loss: 0.6681\n",
      "Epoch [45/300], Step [67/112], Loss: 0.6737\n",
      "Epoch [45/300], Step [71/112], Loss: 0.6683\n",
      "Epoch [45/300], Step [75/112], Loss: 0.6823\n",
      "Epoch [45/300], Step [79/112], Loss: 0.6545\n",
      "Epoch [45/300], Step [83/112], Loss: 0.6787\n",
      "Epoch [45/300], Step [87/112], Loss: 0.6676\n",
      "Epoch [45/300], Step [91/112], Loss: 0.6916\n",
      "Epoch [45/300], Step [95/112], Loss: 0.6811\n",
      "Epoch [45/300], Step [99/112], Loss: 0.6739\n",
      "Epoch [45/300], Step [103/112], Loss: 0.6671\n",
      "Epoch [45/300], Step [107/112], Loss: 0.6713\n",
      "Epoch [45/300], Step [111/112], Loss: 0.6279\n",
      "Epoch [46/300], Step [3/112], Loss: 0.6579\n",
      "Epoch [46/300], Step [7/112], Loss: 0.6671\n",
      "Epoch [46/300], Step [11/112], Loss: 0.6793\n",
      "Epoch [46/300], Step [15/112], Loss: 0.6560\n",
      "Epoch [46/300], Step [19/112], Loss: 0.6710\n",
      "Epoch [46/300], Step [23/112], Loss: 0.6707\n",
      "Epoch [46/300], Step [27/112], Loss: 0.6396\n",
      "Epoch [46/300], Step [31/112], Loss: 0.6902\n",
      "Epoch [46/300], Step [35/112], Loss: 0.6498\n",
      "Epoch [46/300], Step [39/112], Loss: 0.6518\n",
      "Epoch [46/300], Step [43/112], Loss: 0.6644\n",
      "Epoch [46/300], Step [47/112], Loss: 0.6549\n",
      "Epoch [46/300], Step [51/112], Loss: 0.6538\n",
      "Epoch [46/300], Step [55/112], Loss: 0.6653\n",
      "Epoch [46/300], Step [59/112], Loss: 0.6454\n",
      "Epoch [46/300], Step [63/112], Loss: 0.6634\n",
      "Epoch [46/300], Step [67/112], Loss: 0.6687\n",
      "Epoch [46/300], Step [71/112], Loss: 0.6671\n",
      "Epoch [46/300], Step [75/112], Loss: 0.6824\n",
      "Epoch [46/300], Step [79/112], Loss: 0.6524\n",
      "Epoch [46/300], Step [83/112], Loss: 0.6803\n",
      "Epoch [46/300], Step [87/112], Loss: 0.6584\n",
      "Epoch [46/300], Step [91/112], Loss: 0.6852\n",
      "Epoch [46/300], Step [95/112], Loss: 0.6763\n",
      "Epoch [46/300], Step [99/112], Loss: 0.6707\n",
      "Epoch [46/300], Step [103/112], Loss: 0.6669\n",
      "Epoch [46/300], Step [107/112], Loss: 0.6711\n",
      "Epoch [46/300], Step [111/112], Loss: 0.6295\n",
      "Epoch [47/300], Step [3/112], Loss: 0.6557\n",
      "Epoch [47/300], Step [7/112], Loss: 0.6657\n",
      "Epoch [47/300], Step [11/112], Loss: 0.6751\n",
      "Epoch [47/300], Step [15/112], Loss: 0.6578\n",
      "Epoch [47/300], Step [19/112], Loss: 0.6650\n",
      "Epoch [47/300], Step [23/112], Loss: 0.6680\n",
      "Epoch [47/300], Step [27/112], Loss: 0.6439\n",
      "Epoch [47/300], Step [31/112], Loss: 0.6907\n",
      "Epoch [47/300], Step [35/112], Loss: 0.6472\n",
      "Epoch [47/300], Step [39/112], Loss: 0.6522\n",
      "Epoch [47/300], Step [43/112], Loss: 0.6640\n",
      "Epoch [47/300], Step [47/112], Loss: 0.6524\n",
      "Epoch [47/300], Step [51/112], Loss: 0.6540\n",
      "Epoch [47/300], Step [55/112], Loss: 0.6635\n",
      "Epoch [47/300], Step [59/112], Loss: 0.6371\n",
      "Epoch [47/300], Step [63/112], Loss: 0.6661\n",
      "Epoch [47/300], Step [67/112], Loss: 0.6673\n",
      "Epoch [47/300], Step [71/112], Loss: 0.6645\n",
      "Epoch [47/300], Step [75/112], Loss: 0.6763\n",
      "Epoch [47/300], Step [79/112], Loss: 0.6518\n",
      "Epoch [47/300], Step [83/112], Loss: 0.6790\n",
      "Epoch [47/300], Step [87/112], Loss: 0.6635\n",
      "Epoch [47/300], Step [91/112], Loss: 0.6817\n",
      "Epoch [47/300], Step [95/112], Loss: 0.6762\n",
      "Epoch [47/300], Step [99/112], Loss: 0.6744\n",
      "Epoch [47/300], Step [103/112], Loss: 0.6683\n",
      "Epoch [47/300], Step [107/112], Loss: 0.6679\n",
      "Epoch [47/300], Step [111/112], Loss: 0.6284\n",
      "Epoch [48/300], Step [3/112], Loss: 0.6543\n",
      "Epoch [48/300], Step [7/112], Loss: 0.6640\n",
      "Epoch [48/300], Step [11/112], Loss: 0.6713\n",
      "Epoch [48/300], Step [15/112], Loss: 0.6645\n",
      "Epoch [48/300], Step [19/112], Loss: 0.6675\n",
      "Epoch [48/300], Step [23/112], Loss: 0.6644\n",
      "Epoch [48/300], Step [27/112], Loss: 0.6389\n",
      "Epoch [48/300], Step [31/112], Loss: 0.6848\n",
      "Epoch [48/300], Step [35/112], Loss: 0.6505\n",
      "Epoch [48/300], Step [39/112], Loss: 0.6547\n",
      "Epoch [48/300], Step [43/112], Loss: 0.6576\n",
      "Epoch [48/300], Step [47/112], Loss: 0.6510\n",
      "Epoch [48/300], Step [51/112], Loss: 0.6549\n",
      "Epoch [48/300], Step [55/112], Loss: 0.6630\n",
      "Epoch [48/300], Step [59/112], Loss: 0.6397\n",
      "Epoch [48/300], Step [63/112], Loss: 0.6594\n",
      "Epoch [48/300], Step [67/112], Loss: 0.6651\n",
      "Epoch [48/300], Step [71/112], Loss: 0.6668\n",
      "Epoch [48/300], Step [75/112], Loss: 0.6783\n",
      "Epoch [48/300], Step [79/112], Loss: 0.6484\n",
      "Epoch [48/300], Step [83/112], Loss: 0.6769\n",
      "Epoch [48/300], Step [87/112], Loss: 0.6633\n",
      "Epoch [48/300], Step [91/112], Loss: 0.7004\n",
      "Epoch [48/300], Step [95/112], Loss: 0.6774\n",
      "Epoch [48/300], Step [99/112], Loss: 0.6722\n",
      "Epoch [48/300], Step [103/112], Loss: 0.6712\n",
      "Epoch [48/300], Step [107/112], Loss: 0.6702\n",
      "Epoch [48/300], Step [111/112], Loss: 0.6309\n",
      "Epoch [49/300], Step [3/112], Loss: 0.6582\n",
      "Epoch [49/300], Step [7/112], Loss: 0.6647\n",
      "Epoch [49/300], Step [11/112], Loss: 0.6732\n",
      "Epoch [49/300], Step [15/112], Loss: 0.6648\n",
      "Epoch [49/300], Step [19/112], Loss: 0.6625\n",
      "Epoch [49/300], Step [23/112], Loss: 0.6651\n",
      "Epoch [49/300], Step [27/112], Loss: 0.6427\n",
      "Epoch [49/300], Step [31/112], Loss: 0.6896\n",
      "Epoch [49/300], Step [35/112], Loss: 0.6502\n",
      "Epoch [49/300], Step [39/112], Loss: 0.6523\n",
      "Epoch [49/300], Step [43/112], Loss: 0.6659\n",
      "Epoch [49/300], Step [47/112], Loss: 0.6561\n",
      "Epoch [49/300], Step [51/112], Loss: 0.6560\n",
      "Epoch [49/300], Step [55/112], Loss: 0.6625\n",
      "Epoch [49/300], Step [59/112], Loss: 0.6476\n",
      "Epoch [49/300], Step [63/112], Loss: 0.6637\n",
      "Epoch [49/300], Step [67/112], Loss: 0.6701\n",
      "Epoch [49/300], Step [71/112], Loss: 0.6691\n",
      "Epoch [49/300], Step [75/112], Loss: 0.6796\n",
      "Epoch [49/300], Step [79/112], Loss: 0.6538\n",
      "Epoch [49/300], Step [83/112], Loss: 0.6750\n",
      "Epoch [49/300], Step [87/112], Loss: 0.6680\n",
      "Epoch [49/300], Step [91/112], Loss: 0.6873\n",
      "Epoch [49/300], Step [95/112], Loss: 0.6757\n",
      "Epoch [49/300], Step [99/112], Loss: 0.6738\n",
      "Epoch [49/300], Step [103/112], Loss: 0.6675\n",
      "Epoch [49/300], Step [107/112], Loss: 0.6733\n",
      "Epoch [49/300], Step [111/112], Loss: 0.6263\n",
      "Epoch [50/300], Step [3/112], Loss: 0.6501\n",
      "Epoch [50/300], Step [7/112], Loss: 0.6641\n",
      "Epoch [50/300], Step [11/112], Loss: 0.6751\n",
      "Epoch [50/300], Step [15/112], Loss: 0.6644\n",
      "Epoch [50/300], Step [19/112], Loss: 0.6619\n",
      "Epoch [50/300], Step [23/112], Loss: 0.6689\n",
      "Epoch [50/300], Step [27/112], Loss: 0.6463\n",
      "Epoch [50/300], Step [31/112], Loss: 0.6911\n",
      "Epoch [50/300], Step [35/112], Loss: 0.6492\n",
      "Epoch [50/300], Step [39/112], Loss: 0.6562\n",
      "Epoch [50/300], Step [43/112], Loss: 0.6636\n",
      "Epoch [50/300], Step [47/112], Loss: 0.6574\n",
      "Epoch [50/300], Step [51/112], Loss: 0.6562\n",
      "Epoch [50/300], Step [55/112], Loss: 0.6636\n",
      "Epoch [50/300], Step [59/112], Loss: 0.6465\n",
      "Epoch [50/300], Step [63/112], Loss: 0.6698\n",
      "Epoch [50/300], Step [67/112], Loss: 0.6685\n",
      "Epoch [50/300], Step [71/112], Loss: 0.6627\n",
      "Epoch [50/300], Step [75/112], Loss: 0.6749\n",
      "Epoch [50/300], Step [79/112], Loss: 0.6531\n",
      "Epoch [50/300], Step [83/112], Loss: 0.6772\n",
      "Epoch [50/300], Step [87/112], Loss: 0.6590\n",
      "Epoch [50/300], Step [91/112], Loss: 0.6819\n",
      "Epoch [50/300], Step [95/112], Loss: 0.6802\n",
      "Epoch [50/300], Step [99/112], Loss: 0.6762\n",
      "Epoch [50/300], Step [103/112], Loss: 0.6667\n",
      "Epoch [50/300], Step [107/112], Loss: 0.6709\n",
      "Epoch [50/300], Step [111/112], Loss: 0.6259\n",
      "Epoch [51/300], Step [3/112], Loss: 0.6524\n",
      "Epoch [51/300], Step [7/112], Loss: 0.6649\n",
      "Epoch [51/300], Step [11/112], Loss: 0.6716\n",
      "Epoch [51/300], Step [15/112], Loss: 0.6580\n",
      "Epoch [51/300], Step [19/112], Loss: 0.6668\n",
      "Epoch [51/300], Step [23/112], Loss: 0.6696\n",
      "Epoch [51/300], Step [27/112], Loss: 0.6359\n",
      "Epoch [51/300], Step [31/112], Loss: 0.6932\n",
      "Epoch [51/300], Step [35/112], Loss: 0.6524\n",
      "Epoch [51/300], Step [39/112], Loss: 0.6523\n",
      "Epoch [51/300], Step [43/112], Loss: 0.6570\n",
      "Epoch [51/300], Step [47/112], Loss: 0.6483\n",
      "Epoch [51/300], Step [51/112], Loss: 0.6538\n",
      "Epoch [51/300], Step [55/112], Loss: 0.6603\n",
      "Epoch [51/300], Step [59/112], Loss: 0.6420\n",
      "Epoch [51/300], Step [63/112], Loss: 0.6662\n",
      "Epoch [51/300], Step [67/112], Loss: 0.6651\n",
      "Epoch [51/300], Step [71/112], Loss: 0.6631\n",
      "Epoch [51/300], Step [75/112], Loss: 0.6715\n",
      "Epoch [51/300], Step [79/112], Loss: 0.6549\n",
      "Epoch [51/300], Step [83/112], Loss: 0.6734\n",
      "Epoch [51/300], Step [87/112], Loss: 0.6558\n",
      "Epoch [51/300], Step [91/112], Loss: 0.6861\n",
      "Epoch [51/300], Step [95/112], Loss: 0.6745\n",
      "Epoch [51/300], Step [99/112], Loss: 0.6765\n",
      "Epoch [51/300], Step [103/112], Loss: 0.6666\n",
      "Epoch [51/300], Step [107/112], Loss: 0.6644\n",
      "Epoch [51/300], Step [111/112], Loss: 0.6321\n",
      "Epoch [52/300], Step [3/112], Loss: 0.6538\n",
      "Epoch [52/300], Step [7/112], Loss: 0.6623\n",
      "Epoch [52/300], Step [11/112], Loss: 0.6717\n",
      "Epoch [52/300], Step [15/112], Loss: 0.6615\n",
      "Epoch [52/300], Step [19/112], Loss: 0.6613\n",
      "Epoch [52/300], Step [23/112], Loss: 0.6647\n",
      "Epoch [52/300], Step [27/112], Loss: 0.6361\n",
      "Epoch [52/300], Step [31/112], Loss: 0.6817\n",
      "Epoch [52/300], Step [35/112], Loss: 0.6504\n",
      "Epoch [52/300], Step [39/112], Loss: 0.6494\n",
      "Epoch [52/300], Step [43/112], Loss: 0.6585\n",
      "Epoch [52/300], Step [47/112], Loss: 0.6574\n",
      "Epoch [52/300], Step [51/112], Loss: 0.6538\n",
      "Epoch [52/300], Step [55/112], Loss: 0.6627\n",
      "Epoch [52/300], Step [59/112], Loss: 0.6432\n",
      "Epoch [52/300], Step [63/112], Loss: 0.6698\n",
      "Epoch [52/300], Step [67/112], Loss: 0.6574\n",
      "Epoch [52/300], Step [71/112], Loss: 0.6632\n",
      "Epoch [52/300], Step [75/112], Loss: 0.6746\n",
      "Epoch [52/300], Step [79/112], Loss: 0.6521\n",
      "Epoch [52/300], Step [83/112], Loss: 0.6738\n",
      "Epoch [52/300], Step [87/112], Loss: 0.6720\n",
      "Epoch [52/300], Step [91/112], Loss: 0.6817\n",
      "Epoch [52/300], Step [95/112], Loss: 0.6711\n",
      "Epoch [52/300], Step [99/112], Loss: 0.6728\n",
      "Epoch [52/300], Step [103/112], Loss: 0.6645\n",
      "Epoch [52/300], Step [107/112], Loss: 0.6688\n",
      "Epoch [52/300], Step [111/112], Loss: 0.6303\n",
      "Epoch [53/300], Step [3/112], Loss: 0.6530\n",
      "Epoch [53/300], Step [7/112], Loss: 0.6653\n",
      "Epoch [53/300], Step [11/112], Loss: 0.6695\n",
      "Epoch [53/300], Step [15/112], Loss: 0.6592\n",
      "Epoch [53/300], Step [19/112], Loss: 0.6596\n",
      "Epoch [53/300], Step [23/112], Loss: 0.6622\n",
      "Epoch [53/300], Step [27/112], Loss: 0.6382\n",
      "Epoch [53/300], Step [31/112], Loss: 0.6843\n",
      "Epoch [53/300], Step [35/112], Loss: 0.6455\n",
      "Epoch [53/300], Step [39/112], Loss: 0.6495\n",
      "Epoch [53/300], Step [43/112], Loss: 0.6520\n",
      "Epoch [53/300], Step [47/112], Loss: 0.6497\n",
      "Epoch [53/300], Step [51/112], Loss: 0.6533\n",
      "Epoch [53/300], Step [55/112], Loss: 0.6581\n",
      "Epoch [53/300], Step [59/112], Loss: 0.6445\n",
      "Epoch [53/300], Step [63/112], Loss: 0.6640\n",
      "Epoch [53/300], Step [67/112], Loss: 0.6593\n",
      "Epoch [53/300], Step [71/112], Loss: 0.6630\n",
      "Epoch [53/300], Step [75/112], Loss: 0.6727\n",
      "Epoch [53/300], Step [79/112], Loss: 0.6503\n",
      "Epoch [53/300], Step [83/112], Loss: 0.6723\n",
      "Epoch [53/300], Step [87/112], Loss: 0.6624\n",
      "Epoch [53/300], Step [91/112], Loss: 0.6803\n",
      "Epoch [53/300], Step [95/112], Loss: 0.6700\n",
      "Epoch [53/300], Step [99/112], Loss: 0.6700\n",
      "Epoch [53/300], Step [103/112], Loss: 0.6680\n",
      "Epoch [53/300], Step [107/112], Loss: 0.6599\n",
      "Epoch [53/300], Step [111/112], Loss: 0.6296\n",
      "Epoch [54/300], Step [3/112], Loss: 0.6511\n",
      "Epoch [54/300], Step [7/112], Loss: 0.6629\n",
      "Epoch [54/300], Step [11/112], Loss: 0.6664\n",
      "Epoch [54/300], Step [15/112], Loss: 0.6610\n",
      "Epoch [54/300], Step [19/112], Loss: 0.6593\n",
      "Epoch [54/300], Step [23/112], Loss: 0.6675\n",
      "Epoch [54/300], Step [27/112], Loss: 0.6412\n",
      "Epoch [54/300], Step [31/112], Loss: 0.6851\n",
      "Epoch [54/300], Step [35/112], Loss: 0.6507\n",
      "Epoch [54/300], Step [39/112], Loss: 0.6571\n",
      "Epoch [54/300], Step [43/112], Loss: 0.6625\n",
      "Epoch [54/300], Step [47/112], Loss: 0.6504\n",
      "Epoch [54/300], Step [51/112], Loss: 0.6529\n",
      "Epoch [54/300], Step [55/112], Loss: 0.6611\n",
      "Epoch [54/300], Step [59/112], Loss: 0.6467\n",
      "Epoch [54/300], Step [63/112], Loss: 0.6603\n",
      "Epoch [54/300], Step [67/112], Loss: 0.6631\n",
      "Epoch [54/300], Step [71/112], Loss: 0.6641\n",
      "Epoch [54/300], Step [75/112], Loss: 0.6772\n",
      "Epoch [54/300], Step [79/112], Loss: 0.6572\n",
      "Epoch [54/300], Step [83/112], Loss: 0.6731\n",
      "Epoch [54/300], Step [87/112], Loss: 0.6598\n",
      "Epoch [54/300], Step [91/112], Loss: 0.6842\n",
      "Epoch [54/300], Step [95/112], Loss: 0.6724\n",
      "Epoch [54/300], Step [99/112], Loss: 0.6703\n",
      "Epoch [54/300], Step [103/112], Loss: 0.6622\n",
      "Epoch [54/300], Step [107/112], Loss: 0.6637\n",
      "Epoch [54/300], Step [111/112], Loss: 0.6321\n",
      "Epoch [55/300], Step [3/112], Loss: 0.6561\n",
      "Epoch [55/300], Step [7/112], Loss: 0.6670\n",
      "Epoch [55/300], Step [11/112], Loss: 0.6690\n",
      "Epoch [55/300], Step [15/112], Loss: 0.6619\n",
      "Epoch [55/300], Step [19/112], Loss: 0.6611\n",
      "Epoch [55/300], Step [23/112], Loss: 0.6606\n",
      "Epoch [55/300], Step [27/112], Loss: 0.6348\n",
      "Epoch [55/300], Step [31/112], Loss: 0.6784\n",
      "Epoch [55/300], Step [35/112], Loss: 0.6494\n",
      "Epoch [55/300], Step [39/112], Loss: 0.6488\n",
      "Epoch [55/300], Step [43/112], Loss: 0.6584\n",
      "Epoch [55/300], Step [47/112], Loss: 0.6515\n",
      "Epoch [55/300], Step [51/112], Loss: 0.6534\n",
      "Epoch [55/300], Step [55/112], Loss: 0.6606\n",
      "Epoch [55/300], Step [59/112], Loss: 0.6456\n",
      "Epoch [55/300], Step [63/112], Loss: 0.6695\n",
      "Epoch [55/300], Step [67/112], Loss: 0.6614\n",
      "Epoch [55/300], Step [71/112], Loss: 0.6599\n",
      "Epoch [55/300], Step [75/112], Loss: 0.6859\n",
      "Epoch [55/300], Step [79/112], Loss: 0.6647\n",
      "Epoch [55/300], Step [83/112], Loss: 0.6766\n",
      "Epoch [55/300], Step [87/112], Loss: 0.6652\n",
      "Epoch [55/300], Step [91/112], Loss: 0.6819\n",
      "Epoch [55/300], Step [95/112], Loss: 0.6710\n",
      "Epoch [55/300], Step [99/112], Loss: 0.6710\n",
      "Epoch [55/300], Step [103/112], Loss: 0.6646\n",
      "Epoch [55/300], Step [107/112], Loss: 0.6718\n",
      "Epoch [55/300], Step [111/112], Loss: 0.6308\n",
      "Epoch [56/300], Step [3/112], Loss: 0.6603\n",
      "Epoch [56/300], Step [7/112], Loss: 0.6775\n",
      "Epoch [56/300], Step [11/112], Loss: 0.6714\n",
      "Epoch [56/300], Step [15/112], Loss: 0.6569\n",
      "Epoch [56/300], Step [19/112], Loss: 0.6661\n",
      "Epoch [56/300], Step [23/112], Loss: 0.6726\n",
      "Epoch [56/300], Step [27/112], Loss: 0.6450\n",
      "Epoch [56/300], Step [31/112], Loss: 0.6828\n",
      "Epoch [56/300], Step [35/112], Loss: 0.6536\n",
      "Epoch [56/300], Step [39/112], Loss: 0.6510\n",
      "Epoch [56/300], Step [43/112], Loss: 0.6616\n",
      "Epoch [56/300], Step [47/112], Loss: 0.6525\n",
      "Epoch [56/300], Step [51/112], Loss: 0.6539\n",
      "Epoch [56/300], Step [55/112], Loss: 0.6652\n",
      "Epoch [56/300], Step [59/112], Loss: 0.6395\n",
      "Epoch [56/300], Step [63/112], Loss: 0.6658\n",
      "Epoch [56/300], Step [67/112], Loss: 0.6587\n",
      "Epoch [56/300], Step [71/112], Loss: 0.6629\n",
      "Epoch [56/300], Step [75/112], Loss: 0.6731\n",
      "Epoch [56/300], Step [79/112], Loss: 0.6530\n",
      "Epoch [56/300], Step [83/112], Loss: 0.6710\n",
      "Epoch [56/300], Step [87/112], Loss: 0.6667\n",
      "Epoch [56/300], Step [91/112], Loss: 0.6772\n",
      "Epoch [56/300], Step [95/112], Loss: 0.6720\n",
      "Epoch [56/300], Step [99/112], Loss: 0.6771\n",
      "Epoch [56/300], Step [103/112], Loss: 0.6636\n",
      "Epoch [56/300], Step [107/112], Loss: 0.6670\n",
      "Epoch [56/300], Step [111/112], Loss: 0.6310\n",
      "Epoch [57/300], Step [3/112], Loss: 0.6599\n",
      "Epoch [57/300], Step [7/112], Loss: 0.6697\n",
      "Epoch [57/300], Step [11/112], Loss: 0.6650\n",
      "Epoch [57/300], Step [15/112], Loss: 0.6594\n",
      "Epoch [57/300], Step [19/112], Loss: 0.6614\n",
      "Epoch [57/300], Step [23/112], Loss: 0.6722\n",
      "Epoch [57/300], Step [27/112], Loss: 0.6461\n",
      "Epoch [57/300], Step [31/112], Loss: 0.6804\n",
      "Epoch [57/300], Step [35/112], Loss: 0.6536\n",
      "Epoch [57/300], Step [39/112], Loss: 0.6545\n",
      "Epoch [57/300], Step [43/112], Loss: 0.6553\n",
      "Epoch [57/300], Step [47/112], Loss: 0.6530\n",
      "Epoch [57/300], Step [51/112], Loss: 0.6517\n",
      "Epoch [57/300], Step [55/112], Loss: 0.6575\n",
      "Epoch [57/300], Step [59/112], Loss: 0.6490\n",
      "Epoch [57/300], Step [63/112], Loss: 0.6633\n",
      "Epoch [57/300], Step [67/112], Loss: 0.6689\n",
      "Epoch [57/300], Step [71/112], Loss: 0.6664\n",
      "Epoch [57/300], Step [75/112], Loss: 0.6780\n",
      "Epoch [57/300], Step [79/112], Loss: 0.6532\n",
      "Epoch [57/300], Step [83/112], Loss: 0.6694\n",
      "Epoch [57/300], Step [87/112], Loss: 0.6635\n",
      "Epoch [57/300], Step [91/112], Loss: 0.6744\n",
      "Epoch [57/300], Step [95/112], Loss: 0.6748\n",
      "Epoch [57/300], Step [99/112], Loss: 0.6779\n",
      "Epoch [57/300], Step [103/112], Loss: 0.6673\n",
      "Epoch [57/300], Step [107/112], Loss: 0.6700\n",
      "Epoch [57/300], Step [111/112], Loss: 0.6315\n",
      "Epoch [58/300], Step [3/112], Loss: 0.6622\n",
      "Epoch [58/300], Step [7/112], Loss: 0.6708\n",
      "Epoch [58/300], Step [11/112], Loss: 0.6680\n",
      "Epoch [58/300], Step [15/112], Loss: 0.6582\n",
      "Epoch [58/300], Step [19/112], Loss: 0.6598\n",
      "Epoch [58/300], Step [23/112], Loss: 0.6710\n",
      "Epoch [58/300], Step [27/112], Loss: 0.6402\n",
      "Epoch [58/300], Step [31/112], Loss: 0.6816\n",
      "Epoch [58/300], Step [35/112], Loss: 0.6513\n",
      "Epoch [58/300], Step [39/112], Loss: 0.6485\n",
      "Epoch [58/300], Step [43/112], Loss: 0.6519\n",
      "Epoch [58/300], Step [47/112], Loss: 0.6518\n",
      "Epoch [58/300], Step [51/112], Loss: 0.6556\n",
      "Epoch [58/300], Step [55/112], Loss: 0.6557\n",
      "Epoch [58/300], Step [59/112], Loss: 0.6383\n",
      "Epoch [58/300], Step [63/112], Loss: 0.6672\n",
      "Epoch [58/300], Step [67/112], Loss: 0.6635\n",
      "Epoch [58/300], Step [71/112], Loss: 0.6696\n",
      "Epoch [58/300], Step [75/112], Loss: 0.6765\n",
      "Epoch [58/300], Step [79/112], Loss: 0.6550\n",
      "Epoch [58/300], Step [83/112], Loss: 0.6702\n",
      "Epoch [58/300], Step [87/112], Loss: 0.6629\n",
      "Epoch [58/300], Step [91/112], Loss: 0.6792\n",
      "Epoch [58/300], Step [95/112], Loss: 0.6729\n",
      "Epoch [58/300], Step [99/112], Loss: 0.6670\n",
      "Epoch [58/300], Step [103/112], Loss: 0.6613\n",
      "Epoch [58/300], Step [107/112], Loss: 0.6666\n",
      "Epoch [58/300], Step [111/112], Loss: 0.6285\n",
      "Epoch [59/300], Step [3/112], Loss: 0.6621\n",
      "Epoch [59/300], Step [7/112], Loss: 0.6683\n",
      "Epoch [59/300], Step [11/112], Loss: 0.6578\n",
      "Epoch [59/300], Step [15/112], Loss: 0.6543\n",
      "Epoch [59/300], Step [19/112], Loss: 0.6535\n",
      "Epoch [59/300], Step [23/112], Loss: 0.6618\n",
      "Epoch [59/300], Step [27/112], Loss: 0.6418\n",
      "Epoch [59/300], Step [31/112], Loss: 0.6756\n",
      "Epoch [59/300], Step [35/112], Loss: 0.6531\n",
      "Epoch [59/300], Step [39/112], Loss: 0.6546\n",
      "Epoch [59/300], Step [43/112], Loss: 0.6466\n",
      "Epoch [59/300], Step [47/112], Loss: 0.6504\n",
      "Epoch [59/300], Step [51/112], Loss: 0.6528\n",
      "Epoch [59/300], Step [55/112], Loss: 0.6574\n",
      "Epoch [59/300], Step [59/112], Loss: 0.6393\n",
      "Epoch [59/300], Step [63/112], Loss: 0.6622\n",
      "Epoch [59/300], Step [67/112], Loss: 0.6627\n",
      "Epoch [59/300], Step [71/112], Loss: 0.6714\n",
      "Epoch [59/300], Step [75/112], Loss: 0.6757\n",
      "Epoch [59/300], Step [79/112], Loss: 0.6582\n",
      "Epoch [59/300], Step [83/112], Loss: 0.6694\n",
      "Epoch [59/300], Step [87/112], Loss: 0.6572\n",
      "Epoch [59/300], Step [91/112], Loss: 0.6817\n",
      "Epoch [59/300], Step [95/112], Loss: 0.6727\n",
      "Epoch [59/300], Step [99/112], Loss: 0.6740\n",
      "Epoch [59/300], Step [103/112], Loss: 0.6585\n",
      "Epoch [59/300], Step [107/112], Loss: 0.6637\n",
      "Epoch [59/300], Step [111/112], Loss: 0.6229\n",
      "Epoch [60/300], Step [3/112], Loss: 0.6608\n",
      "Epoch [60/300], Step [7/112], Loss: 0.6674\n",
      "Epoch [60/300], Step [11/112], Loss: 0.6640\n",
      "Epoch [60/300], Step [15/112], Loss: 0.6542\n",
      "Epoch [60/300], Step [19/112], Loss: 0.6554\n",
      "Epoch [60/300], Step [23/112], Loss: 0.6597\n",
      "Epoch [60/300], Step [27/112], Loss: 0.6385\n",
      "Epoch [60/300], Step [31/112], Loss: 0.6731\n",
      "Epoch [60/300], Step [35/112], Loss: 0.6510\n",
      "Epoch [60/300], Step [39/112], Loss: 0.6473\n",
      "Epoch [60/300], Step [43/112], Loss: 0.6527\n",
      "Epoch [60/300], Step [47/112], Loss: 0.6592\n",
      "Epoch [60/300], Step [51/112], Loss: 0.6541\n",
      "Epoch [60/300], Step [55/112], Loss: 0.6582\n",
      "Epoch [60/300], Step [59/112], Loss: 0.6343\n",
      "Epoch [60/300], Step [63/112], Loss: 0.6598\n",
      "Epoch [60/300], Step [67/112], Loss: 0.6587\n",
      "Epoch [60/300], Step [71/112], Loss: 0.6644\n",
      "Epoch [60/300], Step [75/112], Loss: 0.6793\n",
      "Epoch [60/300], Step [79/112], Loss: 0.6543\n",
      "Epoch [60/300], Step [83/112], Loss: 0.6754\n",
      "Epoch [60/300], Step [87/112], Loss: 0.6644\n",
      "Epoch [60/300], Step [91/112], Loss: 0.6746\n",
      "Epoch [60/300], Step [95/112], Loss: 0.6744\n",
      "Epoch [60/300], Step [99/112], Loss: 0.6749\n",
      "Epoch [60/300], Step [103/112], Loss: 0.6612\n",
      "Epoch [60/300], Step [107/112], Loss: 0.6643\n",
      "Epoch [60/300], Step [111/112], Loss: 0.6266\n",
      "Epoch [61/300], Step [3/112], Loss: 0.6585\n",
      "Epoch [61/300], Step [7/112], Loss: 0.6659\n",
      "Epoch [61/300], Step [11/112], Loss: 0.6616\n",
      "Epoch [61/300], Step [15/112], Loss: 0.6546\n",
      "Epoch [61/300], Step [19/112], Loss: 0.6546\n",
      "Epoch [61/300], Step [23/112], Loss: 0.6607\n",
      "Epoch [61/300], Step [27/112], Loss: 0.6311\n",
      "Epoch [61/300], Step [31/112], Loss: 0.6737\n",
      "Epoch [61/300], Step [35/112], Loss: 0.6535\n",
      "Epoch [61/300], Step [39/112], Loss: 0.6500\n",
      "Epoch [61/300], Step [43/112], Loss: 0.6466\n",
      "Epoch [61/300], Step [47/112], Loss: 0.6564\n",
      "Epoch [61/300], Step [51/112], Loss: 0.6463\n",
      "Epoch [61/300], Step [55/112], Loss: 0.6620\n",
      "Epoch [61/300], Step [59/112], Loss: 0.6393\n",
      "Epoch [61/300], Step [63/112], Loss: 0.6583\n",
      "Epoch [61/300], Step [67/112], Loss: 0.6566\n",
      "Epoch [61/300], Step [71/112], Loss: 0.6629\n",
      "Epoch [61/300], Step [75/112], Loss: 0.6779\n",
      "Epoch [61/300], Step [79/112], Loss: 0.6528\n",
      "Epoch [61/300], Step [83/112], Loss: 0.6762\n",
      "Epoch [61/300], Step [87/112], Loss: 0.6671\n",
      "Epoch [61/300], Step [91/112], Loss: 0.6770\n",
      "Epoch [61/300], Step [95/112], Loss: 0.6711\n",
      "Epoch [61/300], Step [99/112], Loss: 0.6739\n",
      "Epoch [61/300], Step [103/112], Loss: 0.6641\n",
      "Epoch [61/300], Step [107/112], Loss: 0.6709\n",
      "Epoch [61/300], Step [111/112], Loss: 0.6226\n",
      "Epoch [62/300], Step [3/112], Loss: 0.6570\n",
      "Epoch [62/300], Step [7/112], Loss: 0.6645\n",
      "Epoch [62/300], Step [11/112], Loss: 0.6628\n",
      "Epoch [62/300], Step [15/112], Loss: 0.6558\n",
      "Epoch [62/300], Step [19/112], Loss: 0.6605\n",
      "Epoch [62/300], Step [23/112], Loss: 0.6624\n",
      "Epoch [62/300], Step [27/112], Loss: 0.6346\n",
      "Epoch [62/300], Step [31/112], Loss: 0.6809\n",
      "Epoch [62/300], Step [35/112], Loss: 0.6561\n",
      "Epoch [62/300], Step [39/112], Loss: 0.6514\n",
      "Epoch [62/300], Step [43/112], Loss: 0.6564\n",
      "Epoch [62/300], Step [47/112], Loss: 0.6526\n",
      "Epoch [62/300], Step [51/112], Loss: 0.6490\n",
      "Epoch [62/300], Step [55/112], Loss: 0.6565\n",
      "Epoch [62/300], Step [59/112], Loss: 0.6372\n",
      "Epoch [62/300], Step [63/112], Loss: 0.6568\n",
      "Epoch [62/300], Step [67/112], Loss: 0.6578\n",
      "Epoch [62/300], Step [71/112], Loss: 0.6662\n",
      "Epoch [62/300], Step [75/112], Loss: 0.6777\n",
      "Epoch [62/300], Step [79/112], Loss: 0.6490\n",
      "Epoch [62/300], Step [83/112], Loss: 0.6735\n",
      "Epoch [62/300], Step [87/112], Loss: 0.6600\n",
      "Epoch [62/300], Step [91/112], Loss: 0.6767\n",
      "Epoch [62/300], Step [95/112], Loss: 0.6707\n",
      "Epoch [62/300], Step [99/112], Loss: 0.6706\n",
      "Epoch [62/300], Step [103/112], Loss: 0.6586\n",
      "Epoch [62/300], Step [107/112], Loss: 0.6678\n",
      "Epoch [62/300], Step [111/112], Loss: 0.6223\n",
      "Epoch [63/300], Step [3/112], Loss: 0.6604\n",
      "Epoch [63/300], Step [7/112], Loss: 0.6729\n",
      "Epoch [63/300], Step [11/112], Loss: 0.6673\n",
      "Epoch [63/300], Step [15/112], Loss: 0.6477\n",
      "Epoch [63/300], Step [19/112], Loss: 0.6586\n",
      "Epoch [63/300], Step [23/112], Loss: 0.6636\n",
      "Epoch [63/300], Step [27/112], Loss: 0.6360\n",
      "Epoch [63/300], Step [31/112], Loss: 0.6790\n",
      "Epoch [63/300], Step [35/112], Loss: 0.6512\n",
      "Epoch [63/300], Step [39/112], Loss: 0.6476\n",
      "Epoch [63/300], Step [43/112], Loss: 0.6498\n",
      "Epoch [63/300], Step [47/112], Loss: 0.6657\n",
      "Epoch [63/300], Step [51/112], Loss: 0.6510\n",
      "Epoch [63/300], Step [55/112], Loss: 0.6629\n",
      "Epoch [63/300], Step [59/112], Loss: 0.6428\n",
      "Epoch [63/300], Step [63/112], Loss: 0.6641\n",
      "Epoch [63/300], Step [67/112], Loss: 0.6609\n",
      "Epoch [63/300], Step [71/112], Loss: 0.6731\n",
      "Epoch [63/300], Step [75/112], Loss: 0.6771\n",
      "Epoch [63/300], Step [79/112], Loss: 0.6545\n",
      "Epoch [63/300], Step [83/112], Loss: 0.6726\n",
      "Epoch [63/300], Step [87/112], Loss: 0.6687\n",
      "Epoch [63/300], Step [91/112], Loss: 0.6805\n",
      "Epoch [63/300], Step [95/112], Loss: 0.6679\n",
      "Epoch [63/300], Step [99/112], Loss: 0.6659\n",
      "Epoch [63/300], Step [103/112], Loss: 0.6606\n",
      "Epoch [63/300], Step [107/112], Loss: 0.6721\n",
      "Epoch [63/300], Step [111/112], Loss: 0.6215\n",
      "Epoch [64/300], Step [3/112], Loss: 0.6622\n",
      "Epoch [64/300], Step [7/112], Loss: 0.6581\n",
      "Epoch [64/300], Step [11/112], Loss: 0.6627\n",
      "Epoch [64/300], Step [15/112], Loss: 0.6490\n",
      "Epoch [64/300], Step [19/112], Loss: 0.6541\n",
      "Epoch [64/300], Step [23/112], Loss: 0.6648\n",
      "Epoch [64/300], Step [27/112], Loss: 0.6374\n",
      "Epoch [64/300], Step [31/112], Loss: 0.6836\n",
      "Epoch [64/300], Step [35/112], Loss: 0.6648\n",
      "Epoch [64/300], Step [39/112], Loss: 0.6456\n",
      "Epoch [64/300], Step [43/112], Loss: 0.6516\n",
      "Epoch [64/300], Step [47/112], Loss: 0.6649\n",
      "Epoch [64/300], Step [51/112], Loss: 0.6474\n",
      "Epoch [64/300], Step [55/112], Loss: 0.6635\n",
      "Epoch [64/300], Step [59/112], Loss: 0.6424\n",
      "Epoch [64/300], Step [63/112], Loss: 0.6619\n",
      "Epoch [64/300], Step [67/112], Loss: 0.6659\n",
      "Epoch [64/300], Step [71/112], Loss: 0.6711\n",
      "Epoch [64/300], Step [75/112], Loss: 0.6810\n",
      "Epoch [64/300], Step [79/112], Loss: 0.6540\n",
      "Epoch [64/300], Step [83/112], Loss: 0.6687\n",
      "Epoch [64/300], Step [87/112], Loss: 0.6600\n",
      "Epoch [64/300], Step [91/112], Loss: 0.6721\n",
      "Epoch [64/300], Step [95/112], Loss: 0.6723\n",
      "Epoch [64/300], Step [99/112], Loss: 0.6668\n",
      "Epoch [64/300], Step [103/112], Loss: 0.6629\n",
      "Epoch [64/300], Step [107/112], Loss: 0.6668\n",
      "Epoch [64/300], Step [111/112], Loss: 0.6180\n",
      "Epoch [65/300], Step [3/112], Loss: 0.6579\n",
      "Epoch [65/300], Step [7/112], Loss: 0.6596\n",
      "Epoch [65/300], Step [11/112], Loss: 0.6594\n",
      "Epoch [65/300], Step [15/112], Loss: 0.6481\n",
      "Epoch [65/300], Step [19/112], Loss: 0.6531\n",
      "Epoch [65/300], Step [23/112], Loss: 0.6645\n",
      "Epoch [65/300], Step [27/112], Loss: 0.6397\n",
      "Epoch [65/300], Step [31/112], Loss: 0.6777\n",
      "Epoch [65/300], Step [35/112], Loss: 0.6512\n",
      "Epoch [65/300], Step [39/112], Loss: 0.6523\n",
      "Epoch [65/300], Step [43/112], Loss: 0.6525\n",
      "Epoch [65/300], Step [47/112], Loss: 0.6597\n",
      "Epoch [65/300], Step [51/112], Loss: 0.6458\n",
      "Epoch [65/300], Step [55/112], Loss: 0.6610\n",
      "Epoch [65/300], Step [59/112], Loss: 0.6411\n",
      "Epoch [65/300], Step [63/112], Loss: 0.6648\n",
      "Epoch [65/300], Step [67/112], Loss: 0.6599\n",
      "Epoch [65/300], Step [71/112], Loss: 0.6628\n",
      "Epoch [65/300], Step [75/112], Loss: 0.6787\n",
      "Epoch [65/300], Step [79/112], Loss: 0.6489\n",
      "Epoch [65/300], Step [83/112], Loss: 0.6681\n",
      "Epoch [65/300], Step [87/112], Loss: 0.6524\n",
      "Epoch [65/300], Step [91/112], Loss: 0.6722\n",
      "Epoch [65/300], Step [95/112], Loss: 0.6666\n",
      "Epoch [65/300], Step [99/112], Loss: 0.6709\n",
      "Epoch [65/300], Step [103/112], Loss: 0.6631\n",
      "Epoch [65/300], Step [107/112], Loss: 0.6620\n",
      "Epoch [65/300], Step [111/112], Loss: 0.6179\n",
      "Epoch [66/300], Step [3/112], Loss: 0.6508\n",
      "Epoch [66/300], Step [7/112], Loss: 0.6613\n",
      "Epoch [66/300], Step [11/112], Loss: 0.6627\n",
      "Epoch [66/300], Step [15/112], Loss: 0.6488\n",
      "Epoch [66/300], Step [19/112], Loss: 0.6531\n",
      "Epoch [66/300], Step [23/112], Loss: 0.6602\n",
      "Epoch [66/300], Step [27/112], Loss: 0.6322\n",
      "Epoch [66/300], Step [31/112], Loss: 0.6834\n",
      "Epoch [66/300], Step [35/112], Loss: 0.6533\n",
      "Epoch [66/300], Step [39/112], Loss: 0.6574\n",
      "Epoch [66/300], Step [43/112], Loss: 0.6510\n",
      "Epoch [66/300], Step [47/112], Loss: 0.6607\n",
      "Epoch [66/300], Step [51/112], Loss: 0.6522\n",
      "Epoch [66/300], Step [55/112], Loss: 0.6626\n",
      "Epoch [66/300], Step [59/112], Loss: 0.6421\n",
      "Epoch [66/300], Step [63/112], Loss: 0.6597\n",
      "Epoch [66/300], Step [67/112], Loss: 0.6561\n",
      "Epoch [66/300], Step [71/112], Loss: 0.6668\n",
      "Epoch [66/300], Step [75/112], Loss: 0.6781\n",
      "Epoch [66/300], Step [79/112], Loss: 0.6494\n",
      "Epoch [66/300], Step [83/112], Loss: 0.6791\n",
      "Epoch [66/300], Step [87/112], Loss: 0.6628\n",
      "Epoch [66/300], Step [91/112], Loss: 0.6685\n",
      "Epoch [66/300], Step [95/112], Loss: 0.6712\n",
      "Epoch [66/300], Step [99/112], Loss: 0.6684\n",
      "Epoch [66/300], Step [103/112], Loss: 0.6636\n",
      "Epoch [66/300], Step [107/112], Loss: 0.6567\n",
      "Epoch [66/300], Step [111/112], Loss: 0.6146\n",
      "Epoch [67/300], Step [3/112], Loss: 0.6572\n",
      "Epoch [67/300], Step [7/112], Loss: 0.6584\n",
      "Epoch [67/300], Step [11/112], Loss: 0.6636\n",
      "Epoch [67/300], Step [15/112], Loss: 0.6462\n",
      "Epoch [67/300], Step [19/112], Loss: 0.6462\n",
      "Epoch [67/300], Step [23/112], Loss: 0.6602\n",
      "Epoch [67/300], Step [27/112], Loss: 0.6338\n",
      "Epoch [67/300], Step [31/112], Loss: 0.6770\n",
      "Epoch [67/300], Step [35/112], Loss: 0.6505\n",
      "Epoch [67/300], Step [39/112], Loss: 0.6515\n",
      "Epoch [67/300], Step [43/112], Loss: 0.6482\n",
      "Epoch [67/300], Step [47/112], Loss: 0.6552\n",
      "Epoch [67/300], Step [51/112], Loss: 0.6436\n",
      "Epoch [67/300], Step [55/112], Loss: 0.6605\n",
      "Epoch [67/300], Step [59/112], Loss: 0.6428\n",
      "Epoch [67/300], Step [63/112], Loss: 0.6603\n",
      "Epoch [67/300], Step [67/112], Loss: 0.6597\n",
      "Epoch [67/300], Step [71/112], Loss: 0.6669\n",
      "Epoch [67/300], Step [75/112], Loss: 0.6760\n",
      "Epoch [67/300], Step [79/112], Loss: 0.6499\n",
      "Epoch [67/300], Step [83/112], Loss: 0.6761\n",
      "Epoch [67/300], Step [87/112], Loss: 0.6606\n",
      "Epoch [67/300], Step [91/112], Loss: 0.6713\n",
      "Epoch [67/300], Step [95/112], Loss: 0.6733\n",
      "Epoch [67/300], Step [99/112], Loss: 0.6661\n",
      "Epoch [67/300], Step [103/112], Loss: 0.6614\n",
      "Epoch [67/300], Step [107/112], Loss: 0.6651\n",
      "Epoch [67/300], Step [111/112], Loss: 0.6158\n",
      "Epoch [68/300], Step [3/112], Loss: 0.6588\n",
      "Epoch [68/300], Step [7/112], Loss: 0.6604\n",
      "Epoch [68/300], Step [11/112], Loss: 0.6610\n",
      "Epoch [68/300], Step [15/112], Loss: 0.6496\n",
      "Epoch [68/300], Step [19/112], Loss: 0.6546\n",
      "Epoch [68/300], Step [23/112], Loss: 0.6620\n",
      "Epoch [68/300], Step [27/112], Loss: 0.6344\n",
      "Epoch [68/300], Step [31/112], Loss: 0.6767\n",
      "Epoch [68/300], Step [35/112], Loss: 0.6535\n",
      "Epoch [68/300], Step [39/112], Loss: 0.6560\n",
      "Epoch [68/300], Step [43/112], Loss: 0.6500\n",
      "Epoch [68/300], Step [47/112], Loss: 0.6581\n",
      "Epoch [68/300], Step [51/112], Loss: 0.6467\n",
      "Epoch [68/300], Step [55/112], Loss: 0.6605\n",
      "Epoch [68/300], Step [59/112], Loss: 0.6380\n",
      "Epoch [68/300], Step [63/112], Loss: 0.6552\n",
      "Epoch [68/300], Step [67/112], Loss: 0.6604\n",
      "Epoch [68/300], Step [71/112], Loss: 0.6704\n",
      "Epoch [68/300], Step [75/112], Loss: 0.6705\n",
      "Epoch [68/300], Step [79/112], Loss: 0.6524\n",
      "Epoch [68/300], Step [83/112], Loss: 0.6726\n",
      "Epoch [68/300], Step [87/112], Loss: 0.6597\n",
      "Epoch [68/300], Step [91/112], Loss: 0.6743\n",
      "Epoch [68/300], Step [95/112], Loss: 0.6716\n",
      "Epoch [68/300], Step [99/112], Loss: 0.6741\n",
      "Epoch [68/300], Step [103/112], Loss: 0.6644\n",
      "Epoch [68/300], Step [107/112], Loss: 0.6647\n",
      "Epoch [68/300], Step [111/112], Loss: 0.6149\n",
      "Epoch [69/300], Step [3/112], Loss: 0.6531\n",
      "Epoch [69/300], Step [7/112], Loss: 0.6624\n",
      "Epoch [69/300], Step [11/112], Loss: 0.6688\n",
      "Epoch [69/300], Step [15/112], Loss: 0.6591\n",
      "Epoch [69/300], Step [19/112], Loss: 0.6492\n",
      "Epoch [69/300], Step [23/112], Loss: 0.6583\n",
      "Epoch [69/300], Step [27/112], Loss: 0.6375\n",
      "Epoch [69/300], Step [31/112], Loss: 0.6733\n",
      "Epoch [69/300], Step [35/112], Loss: 0.6512\n",
      "Epoch [69/300], Step [39/112], Loss: 0.6607\n",
      "Epoch [69/300], Step [43/112], Loss: 0.6512\n",
      "Epoch [69/300], Step [47/112], Loss: 0.6623\n",
      "Epoch [69/300], Step [51/112], Loss: 0.6433\n",
      "Epoch [69/300], Step [55/112], Loss: 0.6618\n",
      "Epoch [69/300], Step [59/112], Loss: 0.6427\n",
      "Epoch [69/300], Step [63/112], Loss: 0.6510\n",
      "Epoch [69/300], Step [67/112], Loss: 0.6613\n",
      "Epoch [69/300], Step [71/112], Loss: 0.6622\n",
      "Epoch [69/300], Step [75/112], Loss: 0.6792\n",
      "Epoch [69/300], Step [79/112], Loss: 0.6433\n",
      "Epoch [69/300], Step [83/112], Loss: 0.6695\n",
      "Epoch [69/300], Step [87/112], Loss: 0.6618\n",
      "Epoch [69/300], Step [91/112], Loss: 0.6695\n",
      "Epoch [69/300], Step [95/112], Loss: 0.6637\n",
      "Epoch [69/300], Step [99/112], Loss: 0.6669\n",
      "Epoch [69/300], Step [103/112], Loss: 0.6565\n",
      "Epoch [69/300], Step [107/112], Loss: 0.6584\n",
      "Epoch [69/300], Step [111/112], Loss: 0.6120\n",
      "Epoch [70/300], Step [3/112], Loss: 0.6557\n",
      "Epoch [70/300], Step [7/112], Loss: 0.6633\n",
      "Epoch [70/300], Step [11/112], Loss: 0.6677\n",
      "Epoch [70/300], Step [15/112], Loss: 0.6483\n",
      "Epoch [70/300], Step [19/112], Loss: 0.6441\n",
      "Epoch [70/300], Step [23/112], Loss: 0.6556\n",
      "Epoch [70/300], Step [27/112], Loss: 0.6311\n",
      "Epoch [70/300], Step [31/112], Loss: 0.6770\n",
      "Epoch [70/300], Step [35/112], Loss: 0.6453\n",
      "Epoch [70/300], Step [39/112], Loss: 0.6521\n",
      "Epoch [70/300], Step [43/112], Loss: 0.6426\n",
      "Epoch [70/300], Step [47/112], Loss: 0.6552\n",
      "Epoch [70/300], Step [51/112], Loss: 0.6426\n",
      "Epoch [70/300], Step [55/112], Loss: 0.6610\n",
      "Epoch [70/300], Step [59/112], Loss: 0.6373\n",
      "Epoch [70/300], Step [63/112], Loss: 0.6489\n",
      "Epoch [70/300], Step [67/112], Loss: 0.6588\n",
      "Epoch [70/300], Step [71/112], Loss: 0.6689\n",
      "Epoch [70/300], Step [75/112], Loss: 0.6741\n",
      "Epoch [70/300], Step [79/112], Loss: 0.6482\n",
      "Epoch [70/300], Step [83/112], Loss: 0.6694\n",
      "Epoch [70/300], Step [87/112], Loss: 0.6604\n",
      "Epoch [70/300], Step [91/112], Loss: 0.6656\n",
      "Epoch [70/300], Step [95/112], Loss: 0.6665\n",
      "Epoch [70/300], Step [99/112], Loss: 0.6657\n",
      "Epoch [70/300], Step [103/112], Loss: 0.6608\n",
      "Epoch [70/300], Step [107/112], Loss: 0.6633\n",
      "Epoch [70/300], Step [111/112], Loss: 0.6171\n",
      "Epoch [71/300], Step [3/112], Loss: 0.6533\n",
      "Epoch [71/300], Step [7/112], Loss: 0.6584\n",
      "Epoch [71/300], Step [11/112], Loss: 0.6635\n",
      "Epoch [71/300], Step [15/112], Loss: 0.6473\n",
      "Epoch [71/300], Step [19/112], Loss: 0.6514\n",
      "Epoch [71/300], Step [23/112], Loss: 0.6543\n",
      "Epoch [71/300], Step [27/112], Loss: 0.6321\n",
      "Epoch [71/300], Step [31/112], Loss: 0.6785\n",
      "Epoch [71/300], Step [35/112], Loss: 0.6525\n",
      "Epoch [71/300], Step [39/112], Loss: 0.6531\n",
      "Epoch [71/300], Step [43/112], Loss: 0.6455\n",
      "Epoch [71/300], Step [47/112], Loss: 0.6516\n",
      "Epoch [71/300], Step [51/112], Loss: 0.6427\n",
      "Epoch [71/300], Step [55/112], Loss: 0.6602\n",
      "Epoch [71/300], Step [59/112], Loss: 0.6466\n",
      "Epoch [71/300], Step [63/112], Loss: 0.6528\n",
      "Epoch [71/300], Step [67/112], Loss: 0.6629\n",
      "Epoch [71/300], Step [71/112], Loss: 0.6628\n",
      "Epoch [71/300], Step [75/112], Loss: 0.6752\n",
      "Epoch [71/300], Step [79/112], Loss: 0.6496\n",
      "Epoch [71/300], Step [83/112], Loss: 0.6667\n",
      "Epoch [71/300], Step [87/112], Loss: 0.6545\n",
      "Epoch [71/300], Step [91/112], Loss: 0.6626\n",
      "Epoch [71/300], Step [95/112], Loss: 0.6636\n",
      "Epoch [71/300], Step [99/112], Loss: 0.6706\n",
      "Epoch [71/300], Step [103/112], Loss: 0.6596\n",
      "Epoch [71/300], Step [107/112], Loss: 0.6639\n",
      "Epoch [71/300], Step [111/112], Loss: 0.6178\n",
      "Epoch [72/300], Step [3/112], Loss: 0.6586\n",
      "Epoch [72/300], Step [7/112], Loss: 0.6616\n",
      "Epoch [72/300], Step [11/112], Loss: 0.6642\n",
      "Epoch [72/300], Step [15/112], Loss: 0.6531\n",
      "Epoch [72/300], Step [19/112], Loss: 0.6443\n",
      "Epoch [72/300], Step [23/112], Loss: 0.6608\n",
      "Epoch [72/300], Step [27/112], Loss: 0.6338\n",
      "Epoch [72/300], Step [31/112], Loss: 0.6788\n",
      "Epoch [72/300], Step [35/112], Loss: 0.6476\n",
      "Epoch [72/300], Step [39/112], Loss: 0.6522\n",
      "Epoch [72/300], Step [43/112], Loss: 0.6464\n",
      "Epoch [72/300], Step [47/112], Loss: 0.6571\n",
      "Epoch [72/300], Step [51/112], Loss: 0.6504\n",
      "Epoch [72/300], Step [55/112], Loss: 0.6655\n",
      "Epoch [72/300], Step [59/112], Loss: 0.6461\n",
      "Epoch [72/300], Step [63/112], Loss: 0.6594\n",
      "Epoch [72/300], Step [67/112], Loss: 0.6627\n",
      "Epoch [72/300], Step [71/112], Loss: 0.6637\n",
      "Epoch [72/300], Step [75/112], Loss: 0.6780\n",
      "Epoch [72/300], Step [79/112], Loss: 0.6483\n",
      "Epoch [72/300], Step [83/112], Loss: 0.6702\n",
      "Epoch [72/300], Step [87/112], Loss: 0.6534\n",
      "Epoch [72/300], Step [91/112], Loss: 0.6708\n",
      "Epoch [72/300], Step [95/112], Loss: 0.6629\n",
      "Epoch [72/300], Step [99/112], Loss: 0.6712\n",
      "Epoch [72/300], Step [103/112], Loss: 0.6571\n",
      "Epoch [72/300], Step [107/112], Loss: 0.6593\n",
      "Epoch [72/300], Step [111/112], Loss: 0.6150\n",
      "Epoch [73/300], Step [3/112], Loss: 0.6583\n",
      "Epoch [73/300], Step [7/112], Loss: 0.6597\n",
      "Epoch [73/300], Step [11/112], Loss: 0.6597\n",
      "Epoch [73/300], Step [15/112], Loss: 0.6556\n",
      "Epoch [73/300], Step [19/112], Loss: 0.6415\n",
      "Epoch [73/300], Step [23/112], Loss: 0.6563\n",
      "Epoch [73/300], Step [27/112], Loss: 0.6341\n",
      "Epoch [73/300], Step [31/112], Loss: 0.6770\n",
      "Epoch [73/300], Step [35/112], Loss: 0.6523\n",
      "Epoch [73/300], Step [39/112], Loss: 0.6523\n",
      "Epoch [73/300], Step [43/112], Loss: 0.6437\n",
      "Epoch [73/300], Step [47/112], Loss: 0.6573\n",
      "Epoch [73/300], Step [51/112], Loss: 0.6402\n",
      "Epoch [73/300], Step [55/112], Loss: 0.6651\n",
      "Epoch [73/300], Step [59/112], Loss: 0.6412\n",
      "Epoch [73/300], Step [63/112], Loss: 0.6514\n",
      "Epoch [73/300], Step [67/112], Loss: 0.6656\n",
      "Epoch [73/300], Step [71/112], Loss: 0.6638\n",
      "Epoch [73/300], Step [75/112], Loss: 0.6767\n",
      "Epoch [73/300], Step [79/112], Loss: 0.6546\n",
      "Epoch [73/300], Step [83/112], Loss: 0.6727\n",
      "Epoch [73/300], Step [87/112], Loss: 0.6534\n",
      "Epoch [73/300], Step [91/112], Loss: 0.6668\n",
      "Epoch [73/300], Step [95/112], Loss: 0.6610\n",
      "Epoch [73/300], Step [99/112], Loss: 0.6661\n",
      "Epoch [73/300], Step [103/112], Loss: 0.6580\n",
      "Epoch [73/300], Step [107/112], Loss: 0.6659\n",
      "Epoch [73/300], Step [111/112], Loss: 0.6154\n",
      "Epoch [74/300], Step [3/112], Loss: 0.6546\n",
      "Epoch [74/300], Step [7/112], Loss: 0.6596\n",
      "Epoch [74/300], Step [11/112], Loss: 0.6602\n",
      "Epoch [74/300], Step [15/112], Loss: 0.6519\n",
      "Epoch [74/300], Step [19/112], Loss: 0.6451\n",
      "Epoch [74/300], Step [23/112], Loss: 0.6539\n",
      "Epoch [74/300], Step [27/112], Loss: 0.6302\n",
      "Epoch [74/300], Step [31/112], Loss: 0.6768\n",
      "Epoch [74/300], Step [35/112], Loss: 0.6491\n",
      "Epoch [74/300], Step [39/112], Loss: 0.6529\n",
      "Epoch [74/300], Step [43/112], Loss: 0.6407\n",
      "Epoch [74/300], Step [47/112], Loss: 0.6533\n",
      "Epoch [74/300], Step [51/112], Loss: 0.6443\n",
      "Epoch [74/300], Step [55/112], Loss: 0.6635\n",
      "Epoch [74/300], Step [59/112], Loss: 0.6436\n",
      "Epoch [74/300], Step [63/112], Loss: 0.6573\n",
      "Epoch [74/300], Step [67/112], Loss: 0.6617\n",
      "Epoch [74/300], Step [71/112], Loss: 0.6594\n",
      "Epoch [74/300], Step [75/112], Loss: 0.6721\n",
      "Epoch [74/300], Step [79/112], Loss: 0.6461\n",
      "Epoch [74/300], Step [83/112], Loss: 0.6707\n",
      "Epoch [74/300], Step [87/112], Loss: 0.6565\n",
      "Epoch [74/300], Step [91/112], Loss: 0.6647\n",
      "Epoch [74/300], Step [95/112], Loss: 0.6615\n",
      "Epoch [74/300], Step [99/112], Loss: 0.6677\n",
      "Epoch [74/300], Step [103/112], Loss: 0.6608\n",
      "Epoch [74/300], Step [107/112], Loss: 0.6548\n",
      "Epoch [74/300], Step [111/112], Loss: 0.6119\n",
      "Epoch [75/300], Step [3/112], Loss: 0.6591\n",
      "Epoch [75/300], Step [7/112], Loss: 0.6561\n",
      "Epoch [75/300], Step [11/112], Loss: 0.6614\n",
      "Epoch [75/300], Step [15/112], Loss: 0.6486\n",
      "Epoch [75/300], Step [19/112], Loss: 0.6493\n",
      "Epoch [75/300], Step [23/112], Loss: 0.6604\n",
      "Epoch [75/300], Step [27/112], Loss: 0.6283\n",
      "Epoch [75/300], Step [31/112], Loss: 0.6780\n",
      "Epoch [75/300], Step [35/112], Loss: 0.6476\n",
      "Epoch [75/300], Step [39/112], Loss: 0.6497\n",
      "Epoch [75/300], Step [43/112], Loss: 0.6435\n",
      "Epoch [75/300], Step [47/112], Loss: 0.6635\n",
      "Epoch [75/300], Step [51/112], Loss: 0.6447\n",
      "Epoch [75/300], Step [55/112], Loss: 0.6575\n",
      "Epoch [75/300], Step [59/112], Loss: 0.6372\n",
      "Epoch [75/300], Step [63/112], Loss: 0.6582\n",
      "Epoch [75/300], Step [67/112], Loss: 0.6588\n",
      "Epoch [75/300], Step [71/112], Loss: 0.6604\n",
      "Epoch [75/300], Step [75/112], Loss: 0.6698\n",
      "Epoch [75/300], Step [79/112], Loss: 0.6410\n",
      "Epoch [75/300], Step [83/112], Loss: 0.6731\n",
      "Epoch [75/300], Step [87/112], Loss: 0.6479\n",
      "Epoch [75/300], Step [91/112], Loss: 0.6731\n",
      "Epoch [75/300], Step [95/112], Loss: 0.6637\n",
      "Epoch [75/300], Step [99/112], Loss: 0.6652\n",
      "Epoch [75/300], Step [103/112], Loss: 0.6608\n",
      "Epoch [75/300], Step [107/112], Loss: 0.6645\n",
      "Epoch [75/300], Step [111/112], Loss: 0.6081\n",
      "Epoch [76/300], Step [3/112], Loss: 0.6584\n",
      "Epoch [76/300], Step [7/112], Loss: 0.6585\n",
      "Epoch [76/300], Step [11/112], Loss: 0.6548\n",
      "Epoch [76/300], Step [15/112], Loss: 0.6478\n",
      "Epoch [76/300], Step [19/112], Loss: 0.6412\n",
      "Epoch [76/300], Step [23/112], Loss: 0.6546\n",
      "Epoch [76/300], Step [27/112], Loss: 0.6245\n",
      "Epoch [76/300], Step [31/112], Loss: 0.6759\n",
      "Epoch [76/300], Step [35/112], Loss: 0.6475\n",
      "Epoch [76/300], Step [39/112], Loss: 0.6403\n",
      "Epoch [76/300], Step [43/112], Loss: 0.6423\n",
      "Epoch [76/300], Step [47/112], Loss: 0.6555\n",
      "Epoch [76/300], Step [51/112], Loss: 0.6450\n",
      "Epoch [76/300], Step [55/112], Loss: 0.6546\n",
      "Epoch [76/300], Step [59/112], Loss: 0.6342\n",
      "Epoch [76/300], Step [63/112], Loss: 0.6504\n",
      "Epoch [76/300], Step [67/112], Loss: 0.6649\n",
      "Epoch [76/300], Step [71/112], Loss: 0.6652\n",
      "Epoch [76/300], Step [75/112], Loss: 0.6747\n",
      "Epoch [76/300], Step [79/112], Loss: 0.6482\n",
      "Epoch [76/300], Step [83/112], Loss: 0.6733\n",
      "Epoch [76/300], Step [87/112], Loss: 0.6560\n",
      "Epoch [76/300], Step [91/112], Loss: 0.6773\n",
      "Epoch [76/300], Step [95/112], Loss: 0.6661\n",
      "Epoch [76/300], Step [99/112], Loss: 0.6660\n",
      "Epoch [76/300], Step [103/112], Loss: 0.6596\n",
      "Epoch [76/300], Step [107/112], Loss: 0.6542\n",
      "Epoch [76/300], Step [111/112], Loss: 0.6075\n",
      "Epoch [77/300], Step [3/112], Loss: 0.6637\n",
      "Epoch [77/300], Step [7/112], Loss: 0.6571\n",
      "Epoch [77/300], Step [11/112], Loss: 0.6553\n",
      "Epoch [77/300], Step [15/112], Loss: 0.6514\n",
      "Epoch [77/300], Step [19/112], Loss: 0.6370\n",
      "Epoch [77/300], Step [23/112], Loss: 0.6578\n",
      "Epoch [77/300], Step [27/112], Loss: 0.6302\n",
      "Epoch [77/300], Step [31/112], Loss: 0.6748\n",
      "Epoch [77/300], Step [35/112], Loss: 0.6481\n",
      "Epoch [77/300], Step [39/112], Loss: 0.6432\n",
      "Epoch [77/300], Step [43/112], Loss: 0.6422\n",
      "Epoch [77/300], Step [47/112], Loss: 0.6563\n",
      "Epoch [77/300], Step [51/112], Loss: 0.6419\n",
      "Epoch [77/300], Step [55/112], Loss: 0.6565\n",
      "Epoch [77/300], Step [59/112], Loss: 0.6346\n",
      "Epoch [77/300], Step [63/112], Loss: 0.6517\n",
      "Epoch [77/300], Step [67/112], Loss: 0.6571\n",
      "Epoch [77/300], Step [71/112], Loss: 0.6629\n",
      "Epoch [77/300], Step [75/112], Loss: 0.6698\n",
      "Epoch [77/300], Step [79/112], Loss: 0.6477\n",
      "Epoch [77/300], Step [83/112], Loss: 0.6677\n",
      "Epoch [77/300], Step [87/112], Loss: 0.6578\n",
      "Epoch [77/300], Step [91/112], Loss: 0.6673\n",
      "Epoch [77/300], Step [95/112], Loss: 0.6692\n",
      "Epoch [77/300], Step [99/112], Loss: 0.6662\n",
      "Epoch [77/300], Step [103/112], Loss: 0.6582\n",
      "Epoch [77/300], Step [107/112], Loss: 0.6578\n",
      "Epoch [77/300], Step [111/112], Loss: 0.6068\n",
      "Epoch [78/300], Step [3/112], Loss: 0.6526\n",
      "Epoch [78/300], Step [7/112], Loss: 0.6549\n",
      "Epoch [78/300], Step [11/112], Loss: 0.6583\n",
      "Epoch [78/300], Step [15/112], Loss: 0.6493\n",
      "Epoch [78/300], Step [19/112], Loss: 0.6439\n",
      "Epoch [78/300], Step [23/112], Loss: 0.6501\n",
      "Epoch [78/300], Step [27/112], Loss: 0.6314\n",
      "Epoch [78/300], Step [31/112], Loss: 0.6736\n",
      "Epoch [78/300], Step [35/112], Loss: 0.6494\n",
      "Epoch [78/300], Step [39/112], Loss: 0.6442\n",
      "Epoch [78/300], Step [43/112], Loss: 0.6425\n",
      "Epoch [78/300], Step [47/112], Loss: 0.6520\n",
      "Epoch [78/300], Step [51/112], Loss: 0.6391\n",
      "Epoch [78/300], Step [55/112], Loss: 0.6615\n",
      "Epoch [78/300], Step [59/112], Loss: 0.6397\n",
      "Epoch [78/300], Step [63/112], Loss: 0.6519\n",
      "Epoch [78/300], Step [67/112], Loss: 0.6587\n",
      "Epoch [78/300], Step [71/112], Loss: 0.6598\n",
      "Epoch [78/300], Step [75/112], Loss: 0.6743\n",
      "Epoch [78/300], Step [79/112], Loss: 0.6430\n",
      "Epoch [78/300], Step [83/112], Loss: 0.6645\n",
      "Epoch [78/300], Step [87/112], Loss: 0.6501\n",
      "Epoch [78/300], Step [91/112], Loss: 0.6742\n",
      "Epoch [78/300], Step [95/112], Loss: 0.6637\n",
      "Epoch [78/300], Step [99/112], Loss: 0.6629\n",
      "Epoch [78/300], Step [103/112], Loss: 0.6606\n",
      "Epoch [78/300], Step [107/112], Loss: 0.6593\n",
      "Epoch [78/300], Step [111/112], Loss: 0.6102\n",
      "Epoch [79/300], Step [3/112], Loss: 0.6547\n",
      "Epoch [79/300], Step [7/112], Loss: 0.6595\n",
      "Epoch [79/300], Step [11/112], Loss: 0.6602\n",
      "Epoch [79/300], Step [15/112], Loss: 0.6507\n",
      "Epoch [79/300], Step [19/112], Loss: 0.6398\n",
      "Epoch [79/300], Step [23/112], Loss: 0.6488\n",
      "Epoch [79/300], Step [27/112], Loss: 0.6292\n",
      "Epoch [79/300], Step [31/112], Loss: 0.6762\n",
      "Epoch [79/300], Step [35/112], Loss: 0.6413\n",
      "Epoch [79/300], Step [39/112], Loss: 0.6499\n",
      "Epoch [79/300], Step [43/112], Loss: 0.6552\n",
      "Epoch [79/300], Step [47/112], Loss: 0.6567\n",
      "Epoch [79/300], Step [51/112], Loss: 0.6468\n",
      "Epoch [79/300], Step [55/112], Loss: 0.6612\n",
      "Epoch [79/300], Step [59/112], Loss: 0.6452\n",
      "Epoch [79/300], Step [63/112], Loss: 0.6481\n",
      "Epoch [79/300], Step [67/112], Loss: 0.6536\n",
      "Epoch [79/300], Step [71/112], Loss: 0.6646\n",
      "Epoch [79/300], Step [75/112], Loss: 0.6592\n",
      "Epoch [79/300], Step [79/112], Loss: 0.6440\n",
      "Epoch [79/300], Step [83/112], Loss: 0.6708\n",
      "Epoch [79/300], Step [87/112], Loss: 0.6441\n",
      "Epoch [79/300], Step [91/112], Loss: 0.6675\n",
      "Epoch [79/300], Step [95/112], Loss: 0.6633\n",
      "Epoch [79/300], Step [99/112], Loss: 0.6633\n",
      "Epoch [79/300], Step [103/112], Loss: 0.6506\n",
      "Epoch [79/300], Step [107/112], Loss: 0.6538\n",
      "Epoch [79/300], Step [111/112], Loss: 0.6114\n",
      "Epoch [80/300], Step [3/112], Loss: 0.6492\n",
      "Epoch [80/300], Step [7/112], Loss: 0.6528\n",
      "Epoch [80/300], Step [11/112], Loss: 0.6584\n",
      "Epoch [80/300], Step [15/112], Loss: 0.6511\n",
      "Epoch [80/300], Step [19/112], Loss: 0.6401\n",
      "Epoch [80/300], Step [23/112], Loss: 0.6438\n",
      "Epoch [80/300], Step [27/112], Loss: 0.6242\n",
      "Epoch [80/300], Step [31/112], Loss: 0.6745\n",
      "Epoch [80/300], Step [35/112], Loss: 0.6471\n",
      "Epoch [80/300], Step [39/112], Loss: 0.6433\n",
      "Epoch [80/300], Step [43/112], Loss: 0.6440\n",
      "Epoch [80/300], Step [47/112], Loss: 0.6623\n",
      "Epoch [80/300], Step [51/112], Loss: 0.6453\n",
      "Epoch [80/300], Step [55/112], Loss: 0.6628\n",
      "Epoch [80/300], Step [59/112], Loss: 0.6481\n",
      "Epoch [80/300], Step [63/112], Loss: 0.6552\n",
      "Epoch [80/300], Step [67/112], Loss: 0.6541\n",
      "Epoch [80/300], Step [71/112], Loss: 0.6668\n",
      "Epoch [80/300], Step [75/112], Loss: 0.6671\n",
      "Epoch [80/300], Step [79/112], Loss: 0.6471\n",
      "Epoch [80/300], Step [83/112], Loss: 0.6641\n",
      "Epoch [80/300], Step [87/112], Loss: 0.6512\n",
      "Epoch [80/300], Step [91/112], Loss: 0.6624\n",
      "Epoch [80/300], Step [95/112], Loss: 0.6656\n",
      "Epoch [80/300], Step [99/112], Loss: 0.6601\n",
      "Epoch [80/300], Step [103/112], Loss: 0.6518\n",
      "Epoch [80/300], Step [107/112], Loss: 0.6518\n",
      "Epoch [80/300], Step [111/112], Loss: 0.6122\n",
      "Epoch [81/300], Step [3/112], Loss: 0.6533\n",
      "Epoch [81/300], Step [7/112], Loss: 0.6609\n",
      "Epoch [81/300], Step [11/112], Loss: 0.6579\n",
      "Epoch [81/300], Step [15/112], Loss: 0.6468\n",
      "Epoch [81/300], Step [19/112], Loss: 0.6467\n",
      "Epoch [81/300], Step [23/112], Loss: 0.6534\n",
      "Epoch [81/300], Step [27/112], Loss: 0.6317\n",
      "Epoch [81/300], Step [31/112], Loss: 0.6789\n",
      "Epoch [81/300], Step [35/112], Loss: 0.6453\n",
      "Epoch [81/300], Step [39/112], Loss: 0.6382\n",
      "Epoch [81/300], Step [43/112], Loss: 0.6482\n",
      "Epoch [81/300], Step [47/112], Loss: 0.6601\n",
      "Epoch [81/300], Step [51/112], Loss: 0.6446\n",
      "Epoch [81/300], Step [55/112], Loss: 0.6549\n",
      "Epoch [81/300], Step [59/112], Loss: 0.6319\n",
      "Epoch [81/300], Step [63/112], Loss: 0.6551\n",
      "Epoch [81/300], Step [67/112], Loss: 0.6531\n",
      "Epoch [81/300], Step [71/112], Loss: 0.6618\n",
      "Epoch [81/300], Step [75/112], Loss: 0.6708\n",
      "Epoch [81/300], Step [79/112], Loss: 0.6499\n",
      "Epoch [81/300], Step [83/112], Loss: 0.6670\n",
      "Epoch [81/300], Step [87/112], Loss: 0.6522\n",
      "Epoch [81/300], Step [91/112], Loss: 0.6696\n",
      "Epoch [81/300], Step [95/112], Loss: 0.6666\n",
      "Epoch [81/300], Step [99/112], Loss: 0.6659\n",
      "Epoch [81/300], Step [103/112], Loss: 0.6553\n",
      "Epoch [81/300], Step [107/112], Loss: 0.6573\n",
      "Epoch [81/300], Step [111/112], Loss: 0.6120\n",
      "Epoch [82/300], Step [3/112], Loss: 0.6578\n",
      "Epoch [82/300], Step [7/112], Loss: 0.6530\n",
      "Epoch [82/300], Step [11/112], Loss: 0.6609\n",
      "Epoch [82/300], Step [15/112], Loss: 0.6452\n",
      "Epoch [82/300], Step [19/112], Loss: 0.6393\n",
      "Epoch [82/300], Step [23/112], Loss: 0.6482\n",
      "Epoch [82/300], Step [27/112], Loss: 0.6303\n",
      "Epoch [82/300], Step [31/112], Loss: 0.6765\n",
      "Epoch [82/300], Step [35/112], Loss: 0.6483\n",
      "Epoch [82/300], Step [39/112], Loss: 0.6456\n",
      "Epoch [82/300], Step [43/112], Loss: 0.6453\n",
      "Epoch [82/300], Step [47/112], Loss: 0.6577\n",
      "Epoch [82/300], Step [51/112], Loss: 0.6439\n",
      "Epoch [82/300], Step [55/112], Loss: 0.6607\n",
      "Epoch [82/300], Step [59/112], Loss: 0.6355\n",
      "Epoch [82/300], Step [63/112], Loss: 0.6492\n",
      "Epoch [82/300], Step [67/112], Loss: 0.6647\n",
      "Epoch [82/300], Step [71/112], Loss: 0.6652\n",
      "Epoch [82/300], Step [75/112], Loss: 0.6683\n",
      "Epoch [82/300], Step [79/112], Loss: 0.6484\n",
      "Epoch [82/300], Step [83/112], Loss: 0.6615\n",
      "Epoch [82/300], Step [87/112], Loss: 0.6489\n",
      "Epoch [82/300], Step [91/112], Loss: 0.6708\n",
      "Epoch [82/300], Step [95/112], Loss: 0.6684\n",
      "Epoch [82/300], Step [99/112], Loss: 0.6629\n",
      "Epoch [82/300], Step [103/112], Loss: 0.6567\n",
      "Epoch [82/300], Step [107/112], Loss: 0.6537\n",
      "Epoch [82/300], Step [111/112], Loss: 0.6101\n",
      "Epoch [83/300], Step [3/112], Loss: 0.6663\n",
      "Epoch [83/300], Step [7/112], Loss: 0.6609\n",
      "Epoch [83/300], Step [11/112], Loss: 0.6565\n",
      "Epoch [83/300], Step [15/112], Loss: 0.6490\n",
      "Epoch [83/300], Step [19/112], Loss: 0.6365\n",
      "Epoch [83/300], Step [23/112], Loss: 0.6524\n",
      "Epoch [83/300], Step [27/112], Loss: 0.6323\n",
      "Epoch [83/300], Step [31/112], Loss: 0.6671\n",
      "Epoch [83/300], Step [35/112], Loss: 0.6414\n",
      "Epoch [83/300], Step [39/112], Loss: 0.6414\n",
      "Epoch [83/300], Step [43/112], Loss: 0.6434\n",
      "Epoch [83/300], Step [47/112], Loss: 0.6538\n",
      "Epoch [83/300], Step [51/112], Loss: 0.6413\n",
      "Epoch [83/300], Step [55/112], Loss: 0.6577\n",
      "Epoch [83/300], Step [59/112], Loss: 0.6335\n",
      "Epoch [83/300], Step [63/112], Loss: 0.6534\n",
      "Epoch [83/300], Step [67/112], Loss: 0.6588\n",
      "Epoch [83/300], Step [71/112], Loss: 0.6587\n",
      "Epoch [83/300], Step [75/112], Loss: 0.6722\n",
      "Epoch [83/300], Step [79/112], Loss: 0.6466\n",
      "Epoch [83/300], Step [83/112], Loss: 0.6620\n",
      "Epoch [83/300], Step [87/112], Loss: 0.6530\n",
      "Epoch [83/300], Step [91/112], Loss: 0.6711\n",
      "Epoch [83/300], Step [95/112], Loss: 0.6638\n",
      "Epoch [83/300], Step [99/112], Loss: 0.6650\n",
      "Epoch [83/300], Step [103/112], Loss: 0.6576\n",
      "Epoch [83/300], Step [107/112], Loss: 0.6564\n",
      "Epoch [83/300], Step [111/112], Loss: 0.6063\n",
      "Epoch [84/300], Step [3/112], Loss: 0.6541\n",
      "Epoch [84/300], Step [7/112], Loss: 0.6568\n",
      "Epoch [84/300], Step [11/112], Loss: 0.6569\n",
      "Epoch [84/300], Step [15/112], Loss: 0.6468\n",
      "Epoch [84/300], Step [19/112], Loss: 0.6395\n",
      "Epoch [84/300], Step [23/112], Loss: 0.6459\n",
      "Epoch [84/300], Step [27/112], Loss: 0.6303\n",
      "Epoch [84/300], Step [31/112], Loss: 0.6727\n",
      "Epoch [84/300], Step [35/112], Loss: 0.6432\n",
      "Epoch [84/300], Step [39/112], Loss: 0.6475\n",
      "Epoch [84/300], Step [43/112], Loss: 0.6513\n",
      "Epoch [84/300], Step [47/112], Loss: 0.6545\n",
      "Epoch [84/300], Step [51/112], Loss: 0.6464\n",
      "Epoch [84/300], Step [55/112], Loss: 0.6565\n",
      "Epoch [84/300], Step [59/112], Loss: 0.6340\n",
      "Epoch [84/300], Step [63/112], Loss: 0.6565\n",
      "Epoch [84/300], Step [67/112], Loss: 0.6566\n",
      "Epoch [84/300], Step [71/112], Loss: 0.6677\n",
      "Epoch [84/300], Step [75/112], Loss: 0.6724\n",
      "Epoch [84/300], Step [79/112], Loss: 0.6465\n",
      "Epoch [84/300], Step [83/112], Loss: 0.6668\n",
      "Epoch [84/300], Step [87/112], Loss: 0.6487\n",
      "Epoch [84/300], Step [91/112], Loss: 0.6659\n",
      "Epoch [84/300], Step [95/112], Loss: 0.6655\n",
      "Epoch [84/300], Step [99/112], Loss: 0.6624\n",
      "Epoch [84/300], Step [103/112], Loss: 0.6603\n",
      "Epoch [84/300], Step [107/112], Loss: 0.6564\n",
      "Epoch [84/300], Step [111/112], Loss: 0.6067\n",
      "Epoch [85/300], Step [3/112], Loss: 0.6556\n",
      "Epoch [85/300], Step [7/112], Loss: 0.6600\n",
      "Epoch [85/300], Step [11/112], Loss: 0.6601\n",
      "Epoch [85/300], Step [15/112], Loss: 0.6475\n",
      "Epoch [85/300], Step [19/112], Loss: 0.6366\n",
      "Epoch [85/300], Step [23/112], Loss: 0.6509\n",
      "Epoch [85/300], Step [27/112], Loss: 0.6322\n",
      "Epoch [85/300], Step [31/112], Loss: 0.6689\n",
      "Epoch [85/300], Step [35/112], Loss: 0.6421\n",
      "Epoch [85/300], Step [39/112], Loss: 0.6424\n",
      "Epoch [85/300], Step [43/112], Loss: 0.6489\n",
      "Epoch [85/300], Step [47/112], Loss: 0.6605\n",
      "Epoch [85/300], Step [51/112], Loss: 0.6421\n",
      "Epoch [85/300], Step [55/112], Loss: 0.6559\n",
      "Epoch [85/300], Step [59/112], Loss: 0.6378\n",
      "Epoch [85/300], Step [63/112], Loss: 0.6555\n",
      "Epoch [85/300], Step [67/112], Loss: 0.6581\n",
      "Epoch [85/300], Step [71/112], Loss: 0.6661\n",
      "Epoch [85/300], Step [75/112], Loss: 0.6687\n",
      "Epoch [85/300], Step [79/112], Loss: 0.6462\n",
      "Epoch [85/300], Step [83/112], Loss: 0.6618\n",
      "Epoch [85/300], Step [87/112], Loss: 0.6534\n",
      "Epoch [85/300], Step [91/112], Loss: 0.6697\n",
      "Epoch [85/300], Step [95/112], Loss: 0.6628\n",
      "Epoch [85/300], Step [99/112], Loss: 0.6644\n",
      "Epoch [85/300], Step [103/112], Loss: 0.6567\n",
      "Epoch [85/300], Step [107/112], Loss: 0.6507\n",
      "Epoch [85/300], Step [111/112], Loss: 0.6073\n",
      "Epoch [86/300], Step [3/112], Loss: 0.6641\n",
      "Epoch [86/300], Step [7/112], Loss: 0.6627\n",
      "Epoch [86/300], Step [11/112], Loss: 0.6514\n",
      "Epoch [86/300], Step [15/112], Loss: 0.6496\n",
      "Epoch [86/300], Step [19/112], Loss: 0.6294\n",
      "Epoch [86/300], Step [23/112], Loss: 0.6548\n",
      "Epoch [86/300], Step [27/112], Loss: 0.6273\n",
      "Epoch [86/300], Step [31/112], Loss: 0.6736\n",
      "Epoch [86/300], Step [35/112], Loss: 0.6422\n",
      "Epoch [86/300], Step [39/112], Loss: 0.6446\n",
      "Epoch [86/300], Step [43/112], Loss: 0.6468\n",
      "Epoch [86/300], Step [47/112], Loss: 0.6613\n",
      "Epoch [86/300], Step [51/112], Loss: 0.6419\n",
      "Epoch [86/300], Step [55/112], Loss: 0.6571\n",
      "Epoch [86/300], Step [59/112], Loss: 0.6328\n",
      "Epoch [86/300], Step [63/112], Loss: 0.6562\n",
      "Epoch [86/300], Step [67/112], Loss: 0.6564\n",
      "Epoch [86/300], Step [71/112], Loss: 0.6609\n",
      "Epoch [86/300], Step [75/112], Loss: 0.6788\n",
      "Epoch [86/300], Step [79/112], Loss: 0.6466\n",
      "Epoch [86/300], Step [83/112], Loss: 0.6710\n",
      "Epoch [86/300], Step [87/112], Loss: 0.6473\n",
      "Epoch [86/300], Step [91/112], Loss: 0.6723\n",
      "Epoch [86/300], Step [95/112], Loss: 0.6709\n",
      "Epoch [86/300], Step [99/112], Loss: 0.6686\n",
      "Epoch [86/300], Step [103/112], Loss: 0.6557\n",
      "Epoch [86/300], Step [107/112], Loss: 0.6492\n",
      "Epoch [86/300], Step [111/112], Loss: 0.6061\n",
      "Epoch [87/300], Step [3/112], Loss: 0.6557\n",
      "Epoch [87/300], Step [7/112], Loss: 0.6560\n",
      "Epoch [87/300], Step [11/112], Loss: 0.6539\n",
      "Epoch [87/300], Step [15/112], Loss: 0.6496\n",
      "Epoch [87/300], Step [19/112], Loss: 0.6355\n",
      "Epoch [87/300], Step [23/112], Loss: 0.6519\n",
      "Epoch [87/300], Step [27/112], Loss: 0.6308\n",
      "Epoch [87/300], Step [31/112], Loss: 0.6700\n",
      "Epoch [87/300], Step [35/112], Loss: 0.6421\n",
      "Epoch [87/300], Step [39/112], Loss: 0.6473\n",
      "Epoch [87/300], Step [43/112], Loss: 0.6412\n",
      "Epoch [87/300], Step [47/112], Loss: 0.6550\n",
      "Epoch [87/300], Step [51/112], Loss: 0.6435\n",
      "Epoch [87/300], Step [55/112], Loss: 0.6556\n",
      "Epoch [87/300], Step [59/112], Loss: 0.6351\n",
      "Epoch [87/300], Step [63/112], Loss: 0.6442\n",
      "Epoch [87/300], Step [67/112], Loss: 0.6700\n",
      "Epoch [87/300], Step [71/112], Loss: 0.6624\n",
      "Epoch [87/300], Step [75/112], Loss: 0.6778\n",
      "Epoch [87/300], Step [79/112], Loss: 0.6419\n",
      "Epoch [87/300], Step [83/112], Loss: 0.6686\n",
      "Epoch [87/300], Step [87/112], Loss: 0.6522\n",
      "Epoch [87/300], Step [91/112], Loss: 0.6681\n",
      "Epoch [87/300], Step [95/112], Loss: 0.6627\n",
      "Epoch [87/300], Step [99/112], Loss: 0.6616\n",
      "Epoch [87/300], Step [103/112], Loss: 0.6494\n",
      "Epoch [87/300], Step [107/112], Loss: 0.6489\n",
      "Epoch [87/300], Step [111/112], Loss: 0.5992\n",
      "Epoch [88/300], Step [3/112], Loss: 0.6660\n",
      "Epoch [88/300], Step [7/112], Loss: 0.6547\n",
      "Epoch [88/300], Step [11/112], Loss: 0.6558\n",
      "Epoch [88/300], Step [15/112], Loss: 0.6446\n",
      "Epoch [88/300], Step [19/112], Loss: 0.6379\n",
      "Epoch [88/300], Step [23/112], Loss: 0.6486\n",
      "Epoch [88/300], Step [27/112], Loss: 0.6278\n",
      "Epoch [88/300], Step [31/112], Loss: 0.6643\n",
      "Epoch [88/300], Step [35/112], Loss: 0.6474\n",
      "Epoch [88/300], Step [39/112], Loss: 0.6452\n",
      "Epoch [88/300], Step [43/112], Loss: 0.6375\n",
      "Epoch [88/300], Step [47/112], Loss: 0.6512\n",
      "Epoch [88/300], Step [51/112], Loss: 0.6405\n",
      "Epoch [88/300], Step [55/112], Loss: 0.6509\n",
      "Epoch [88/300], Step [59/112], Loss: 0.6329\n",
      "Epoch [88/300], Step [63/112], Loss: 0.6501\n",
      "Epoch [88/300], Step [67/112], Loss: 0.6617\n",
      "Epoch [88/300], Step [71/112], Loss: 0.6673\n",
      "Epoch [88/300], Step [75/112], Loss: 0.6740\n",
      "Epoch [88/300], Step [79/112], Loss: 0.6473\n",
      "Epoch [88/300], Step [83/112], Loss: 0.6699\n",
      "Epoch [88/300], Step [87/112], Loss: 0.6415\n",
      "Epoch [88/300], Step [91/112], Loss: 0.6716\n",
      "Epoch [88/300], Step [95/112], Loss: 0.6690\n",
      "Epoch [88/300], Step [99/112], Loss: 0.6610\n",
      "Epoch [88/300], Step [103/112], Loss: 0.6560\n",
      "Epoch [88/300], Step [107/112], Loss: 0.6547\n",
      "Epoch [88/300], Step [111/112], Loss: 0.6027\n",
      "Epoch [89/300], Step [3/112], Loss: 0.6581\n",
      "Epoch [89/300], Step [7/112], Loss: 0.6628\n",
      "Epoch [89/300], Step [11/112], Loss: 0.6510\n",
      "Epoch [89/300], Step [15/112], Loss: 0.6516\n",
      "Epoch [89/300], Step [19/112], Loss: 0.6369\n",
      "Epoch [89/300], Step [23/112], Loss: 0.6561\n",
      "Epoch [89/300], Step [27/112], Loss: 0.6321\n",
      "Epoch [89/300], Step [31/112], Loss: 0.6665\n",
      "Epoch [89/300], Step [35/112], Loss: 0.6391\n",
      "Epoch [89/300], Step [39/112], Loss: 0.6477\n",
      "Epoch [89/300], Step [43/112], Loss: 0.6461\n",
      "Epoch [89/300], Step [47/112], Loss: 0.6611\n",
      "Epoch [89/300], Step [51/112], Loss: 0.6353\n",
      "Epoch [89/300], Step [55/112], Loss: 0.6513\n",
      "Epoch [89/300], Step [59/112], Loss: 0.6260\n",
      "Epoch [89/300], Step [63/112], Loss: 0.6495\n",
      "Epoch [89/300], Step [67/112], Loss: 0.6555\n",
      "Epoch [89/300], Step [71/112], Loss: 0.6636\n",
      "Epoch [89/300], Step [75/112], Loss: 0.6800\n",
      "Epoch [89/300], Step [79/112], Loss: 0.6469\n",
      "Epoch [89/300], Step [83/112], Loss: 0.6631\n",
      "Epoch [89/300], Step [87/112], Loss: 0.6450\n",
      "Epoch [89/300], Step [91/112], Loss: 0.6637\n",
      "Epoch [89/300], Step [95/112], Loss: 0.6654\n",
      "Epoch [89/300], Step [99/112], Loss: 0.6587\n",
      "Epoch [89/300], Step [103/112], Loss: 0.6551\n",
      "Epoch [89/300], Step [107/112], Loss: 0.6553\n",
      "Epoch [89/300], Step [111/112], Loss: 0.6031\n",
      "Epoch [90/300], Step [3/112], Loss: 0.6554\n",
      "Epoch [90/300], Step [7/112], Loss: 0.6551\n",
      "Epoch [90/300], Step [11/112], Loss: 0.6529\n",
      "Epoch [90/300], Step [15/112], Loss: 0.6397\n",
      "Epoch [90/300], Step [19/112], Loss: 0.6387\n",
      "Epoch [90/300], Step [23/112], Loss: 0.6498\n",
      "Epoch [90/300], Step [27/112], Loss: 0.6282\n",
      "Epoch [90/300], Step [31/112], Loss: 0.6726\n",
      "Epoch [90/300], Step [35/112], Loss: 0.6373\n",
      "Epoch [90/300], Step [39/112], Loss: 0.6409\n",
      "Epoch [90/300], Step [43/112], Loss: 0.6462\n",
      "Epoch [90/300], Step [47/112], Loss: 0.6558\n",
      "Epoch [90/300], Step [51/112], Loss: 0.6416\n",
      "Epoch [90/300], Step [55/112], Loss: 0.6525\n",
      "Epoch [90/300], Step [59/112], Loss: 0.6316\n",
      "Epoch [90/300], Step [63/112], Loss: 0.6515\n",
      "Epoch [90/300], Step [67/112], Loss: 0.6532\n",
      "Epoch [90/300], Step [71/112], Loss: 0.6538\n",
      "Epoch [90/300], Step [75/112], Loss: 0.6727\n",
      "Epoch [90/300], Step [79/112], Loss: 0.6406\n",
      "Epoch [90/300], Step [83/112], Loss: 0.6623\n",
      "Epoch [90/300], Step [87/112], Loss: 0.6446\n",
      "Epoch [90/300], Step [91/112], Loss: 0.6652\n",
      "Epoch [90/300], Step [95/112], Loss: 0.6600\n",
      "Epoch [90/300], Step [99/112], Loss: 0.6674\n",
      "Epoch [90/300], Step [103/112], Loss: 0.6504\n",
      "Epoch [90/300], Step [107/112], Loss: 0.6540\n",
      "Epoch [90/300], Step [111/112], Loss: 0.6063\n",
      "Epoch [91/300], Step [3/112], Loss: 0.6571\n",
      "Epoch [91/300], Step [7/112], Loss: 0.6561\n",
      "Epoch [91/300], Step [11/112], Loss: 0.6534\n",
      "Epoch [91/300], Step [15/112], Loss: 0.6411\n",
      "Epoch [91/300], Step [19/112], Loss: 0.6410\n",
      "Epoch [91/300], Step [23/112], Loss: 0.6513\n",
      "Epoch [91/300], Step [27/112], Loss: 0.6292\n",
      "Epoch [91/300], Step [31/112], Loss: 0.6641\n",
      "Epoch [91/300], Step [35/112], Loss: 0.6393\n",
      "Epoch [91/300], Step [39/112], Loss: 0.6416\n",
      "Epoch [91/300], Step [43/112], Loss: 0.6432\n",
      "Epoch [91/300], Step [47/112], Loss: 0.6468\n",
      "Epoch [91/300], Step [51/112], Loss: 0.6421\n",
      "Epoch [91/300], Step [55/112], Loss: 0.6519\n",
      "Epoch [91/300], Step [59/112], Loss: 0.6292\n",
      "Epoch [91/300], Step [63/112], Loss: 0.6436\n",
      "Epoch [91/300], Step [67/112], Loss: 0.6532\n",
      "Epoch [91/300], Step [71/112], Loss: 0.6618\n",
      "Epoch [91/300], Step [75/112], Loss: 0.6714\n",
      "Epoch [91/300], Step [79/112], Loss: 0.6433\n",
      "Epoch [91/300], Step [83/112], Loss: 0.6612\n",
      "Epoch [91/300], Step [87/112], Loss: 0.6418\n",
      "Epoch [91/300], Step [91/112], Loss: 0.6635\n",
      "Epoch [91/300], Step [95/112], Loss: 0.6600\n",
      "Epoch [91/300], Step [99/112], Loss: 0.6613\n",
      "Epoch [91/300], Step [103/112], Loss: 0.6522\n",
      "Epoch [91/300], Step [107/112], Loss: 0.6537\n",
      "Epoch [91/300], Step [111/112], Loss: 0.6025\n",
      "Epoch [92/300], Step [3/112], Loss: 0.6592\n",
      "Epoch [92/300], Step [7/112], Loss: 0.6551\n",
      "Epoch [92/300], Step [11/112], Loss: 0.6493\n",
      "Epoch [92/300], Step [15/112], Loss: 0.6469\n",
      "Epoch [92/300], Step [19/112], Loss: 0.6419\n",
      "Epoch [92/300], Step [23/112], Loss: 0.6576\n",
      "Epoch [92/300], Step [27/112], Loss: 0.6326\n",
      "Epoch [92/300], Step [31/112], Loss: 0.6707\n",
      "Epoch [92/300], Step [35/112], Loss: 0.6361\n",
      "Epoch [92/300], Step [39/112], Loss: 0.6428\n",
      "Epoch [92/300], Step [43/112], Loss: 0.6431\n",
      "Epoch [92/300], Step [47/112], Loss: 0.6493\n",
      "Epoch [92/300], Step [51/112], Loss: 0.6448\n",
      "Epoch [92/300], Step [55/112], Loss: 0.6529\n",
      "Epoch [92/300], Step [59/112], Loss: 0.6376\n",
      "Epoch [92/300], Step [63/112], Loss: 0.6440\n",
      "Epoch [92/300], Step [67/112], Loss: 0.6524\n",
      "Epoch [92/300], Step [71/112], Loss: 0.6604\n",
      "Epoch [92/300], Step [75/112], Loss: 0.6648\n",
      "Epoch [92/300], Step [79/112], Loss: 0.6434\n",
      "Epoch [92/300], Step [83/112], Loss: 0.6633\n",
      "Epoch [92/300], Step [87/112], Loss: 0.6440\n",
      "Epoch [92/300], Step [91/112], Loss: 0.6651\n",
      "Epoch [92/300], Step [95/112], Loss: 0.6623\n",
      "Epoch [92/300], Step [99/112], Loss: 0.6645\n",
      "Epoch [92/300], Step [103/112], Loss: 0.6578\n",
      "Epoch [92/300], Step [107/112], Loss: 0.6486\n",
      "Epoch [92/300], Step [111/112], Loss: 0.5979\n",
      "Epoch [93/300], Step [3/112], Loss: 0.6574\n",
      "Epoch [93/300], Step [7/112], Loss: 0.6550\n",
      "Epoch [93/300], Step [11/112], Loss: 0.6446\n",
      "Epoch [93/300], Step [15/112], Loss: 0.6453\n",
      "Epoch [93/300], Step [19/112], Loss: 0.6374\n",
      "Epoch [93/300], Step [23/112], Loss: 0.6456\n",
      "Epoch [93/300], Step [27/112], Loss: 0.6229\n",
      "Epoch [93/300], Step [31/112], Loss: 0.6680\n",
      "Epoch [93/300], Step [35/112], Loss: 0.6364\n",
      "Epoch [93/300], Step [39/112], Loss: 0.6418\n",
      "Epoch [93/300], Step [43/112], Loss: 0.6441\n",
      "Epoch [93/300], Step [47/112], Loss: 0.6480\n",
      "Epoch [93/300], Step [51/112], Loss: 0.6398\n",
      "Epoch [93/300], Step [55/112], Loss: 0.6524\n",
      "Epoch [93/300], Step [59/112], Loss: 0.6255\n",
      "Epoch [93/300], Step [63/112], Loss: 0.6444\n",
      "Epoch [93/300], Step [67/112], Loss: 0.6580\n",
      "Epoch [93/300], Step [71/112], Loss: 0.6527\n",
      "Epoch [93/300], Step [75/112], Loss: 0.6679\n",
      "Epoch [93/300], Step [79/112], Loss: 0.6402\n",
      "Epoch [93/300], Step [83/112], Loss: 0.6609\n",
      "Epoch [93/300], Step [87/112], Loss: 0.6370\n",
      "Epoch [93/300], Step [91/112], Loss: 0.6618\n",
      "Epoch [93/300], Step [95/112], Loss: 0.6594\n",
      "Epoch [93/300], Step [99/112], Loss: 0.6622\n",
      "Epoch [93/300], Step [103/112], Loss: 0.6524\n",
      "Epoch [93/300], Step [107/112], Loss: 0.6490\n",
      "Epoch [93/300], Step [111/112], Loss: 0.6024\n",
      "Epoch [94/300], Step [3/112], Loss: 0.6556\n",
      "Epoch [94/300], Step [7/112], Loss: 0.6594\n",
      "Epoch [94/300], Step [11/112], Loss: 0.6454\n",
      "Epoch [94/300], Step [15/112], Loss: 0.6425\n",
      "Epoch [94/300], Step [19/112], Loss: 0.6362\n",
      "Epoch [94/300], Step [23/112], Loss: 0.6504\n",
      "Epoch [94/300], Step [27/112], Loss: 0.6322\n",
      "Epoch [94/300], Step [31/112], Loss: 0.6685\n",
      "Epoch [94/300], Step [35/112], Loss: 0.6387\n",
      "Epoch [94/300], Step [39/112], Loss: 0.6431\n",
      "Epoch [94/300], Step [43/112], Loss: 0.6444\n",
      "Epoch [94/300], Step [47/112], Loss: 0.6481\n",
      "Epoch [94/300], Step [51/112], Loss: 0.6418\n",
      "Epoch [94/300], Step [55/112], Loss: 0.6495\n",
      "Epoch [94/300], Step [59/112], Loss: 0.6337\n",
      "Epoch [94/300], Step [63/112], Loss: 0.6410\n",
      "Epoch [94/300], Step [67/112], Loss: 0.6575\n",
      "Epoch [94/300], Step [71/112], Loss: 0.6546\n",
      "Epoch [94/300], Step [75/112], Loss: 0.6633\n",
      "Epoch [94/300], Step [79/112], Loss: 0.6421\n",
      "Epoch [94/300], Step [83/112], Loss: 0.6647\n",
      "Epoch [94/300], Step [87/112], Loss: 0.6376\n",
      "Epoch [94/300], Step [91/112], Loss: 0.6558\n",
      "Epoch [94/300], Step [95/112], Loss: 0.6663\n",
      "Epoch [94/300], Step [99/112], Loss: 0.6602\n",
      "Epoch [94/300], Step [103/112], Loss: 0.6506\n",
      "Epoch [94/300], Step [107/112], Loss: 0.6476\n",
      "Epoch [94/300], Step [111/112], Loss: 0.6041\n",
      "Epoch [95/300], Step [3/112], Loss: 0.6570\n",
      "Epoch [95/300], Step [7/112], Loss: 0.6576\n",
      "Epoch [95/300], Step [11/112], Loss: 0.6521\n",
      "Epoch [95/300], Step [15/112], Loss: 0.6470\n",
      "Epoch [95/300], Step [19/112], Loss: 0.6400\n",
      "Epoch [95/300], Step [23/112], Loss: 0.6545\n",
      "Epoch [95/300], Step [27/112], Loss: 0.6362\n",
      "Epoch [95/300], Step [31/112], Loss: 0.6757\n",
      "Epoch [95/300], Step [35/112], Loss: 0.6400\n",
      "Epoch [95/300], Step [39/112], Loss: 0.6445\n",
      "Epoch [95/300], Step [43/112], Loss: 0.6491\n",
      "Epoch [95/300], Step [47/112], Loss: 0.6490\n",
      "Epoch [95/300], Step [51/112], Loss: 0.6412\n",
      "Epoch [95/300], Step [55/112], Loss: 0.6573\n",
      "Epoch [95/300], Step [59/112], Loss: 0.6349\n",
      "Epoch [95/300], Step [63/112], Loss: 0.6448\n",
      "Epoch [95/300], Step [67/112], Loss: 0.6564\n",
      "Epoch [95/300], Step [71/112], Loss: 0.6572\n",
      "Epoch [95/300], Step [75/112], Loss: 0.6660\n",
      "Epoch [95/300], Step [79/112], Loss: 0.6472\n",
      "Epoch [95/300], Step [83/112], Loss: 0.6647\n",
      "Epoch [95/300], Step [87/112], Loss: 0.6402\n",
      "Epoch [95/300], Step [91/112], Loss: 0.6593\n",
      "Epoch [95/300], Step [95/112], Loss: 0.6596\n",
      "Epoch [95/300], Step [99/112], Loss: 0.6595\n",
      "Epoch [95/300], Step [103/112], Loss: 0.6547\n",
      "Epoch [95/300], Step [107/112], Loss: 0.6493\n",
      "Epoch [95/300], Step [111/112], Loss: 0.6048\n",
      "Epoch [96/300], Step [3/112], Loss: 0.6685\n",
      "Epoch [96/300], Step [7/112], Loss: 0.6636\n",
      "Epoch [96/300], Step [11/112], Loss: 0.6532\n",
      "Epoch [96/300], Step [15/112], Loss: 0.6463\n",
      "Epoch [96/300], Step [19/112], Loss: 0.6336\n",
      "Epoch [96/300], Step [23/112], Loss: 0.6511\n",
      "Epoch [96/300], Step [27/112], Loss: 0.6308\n",
      "Epoch [96/300], Step [31/112], Loss: 0.6760\n",
      "Epoch [96/300], Step [35/112], Loss: 0.6394\n",
      "Epoch [96/300], Step [39/112], Loss: 0.6429\n",
      "Epoch [96/300], Step [43/112], Loss: 0.6411\n",
      "Epoch [96/300], Step [47/112], Loss: 0.6500\n",
      "Epoch [96/300], Step [51/112], Loss: 0.6392\n",
      "Epoch [96/300], Step [55/112], Loss: 0.6517\n",
      "Epoch [96/300], Step [59/112], Loss: 0.6337\n",
      "Epoch [96/300], Step [63/112], Loss: 0.6368\n",
      "Epoch [96/300], Step [67/112], Loss: 0.6521\n",
      "Epoch [96/300], Step [71/112], Loss: 0.6626\n",
      "Epoch [96/300], Step [75/112], Loss: 0.6661\n",
      "Epoch [96/300], Step [79/112], Loss: 0.6415\n",
      "Epoch [96/300], Step [83/112], Loss: 0.6659\n",
      "Epoch [96/300], Step [87/112], Loss: 0.6396\n",
      "Epoch [96/300], Step [91/112], Loss: 0.6579\n",
      "Epoch [96/300], Step [95/112], Loss: 0.6615\n",
      "Epoch [96/300], Step [99/112], Loss: 0.6641\n",
      "Epoch [96/300], Step [103/112], Loss: 0.6525\n",
      "Epoch [96/300], Step [107/112], Loss: 0.6518\n",
      "Epoch [96/300], Step [111/112], Loss: 0.6006\n",
      "Epoch [97/300], Step [3/112], Loss: 0.6654\n",
      "Epoch [97/300], Step [7/112], Loss: 0.6594\n",
      "Epoch [97/300], Step [11/112], Loss: 0.6539\n",
      "Epoch [97/300], Step [15/112], Loss: 0.6446\n",
      "Epoch [97/300], Step [19/112], Loss: 0.6400\n",
      "Epoch [97/300], Step [23/112], Loss: 0.6458\n",
      "Epoch [97/300], Step [27/112], Loss: 0.6313\n",
      "Epoch [97/300], Step [31/112], Loss: 0.6665\n",
      "Epoch [97/300], Step [35/112], Loss: 0.6369\n",
      "Epoch [97/300], Step [39/112], Loss: 0.6466\n",
      "Epoch [97/300], Step [43/112], Loss: 0.6364\n",
      "Epoch [97/300], Step [47/112], Loss: 0.6545\n",
      "Epoch [97/300], Step [51/112], Loss: 0.6339\n",
      "Epoch [97/300], Step [55/112], Loss: 0.6514\n",
      "Epoch [97/300], Step [59/112], Loss: 0.6315\n",
      "Epoch [97/300], Step [63/112], Loss: 0.6477\n",
      "Epoch [97/300], Step [67/112], Loss: 0.6549\n",
      "Epoch [97/300], Step [71/112], Loss: 0.6663\n",
      "Epoch [97/300], Step [75/112], Loss: 0.6659\n",
      "Epoch [97/300], Step [79/112], Loss: 0.6522\n",
      "Epoch [97/300], Step [83/112], Loss: 0.6583\n",
      "Epoch [97/300], Step [87/112], Loss: 0.6412\n",
      "Epoch [97/300], Step [91/112], Loss: 0.6573\n",
      "Epoch [97/300], Step [95/112], Loss: 0.6586\n",
      "Epoch [97/300], Step [99/112], Loss: 0.6603\n",
      "Epoch [97/300], Step [103/112], Loss: 0.6543\n",
      "Epoch [97/300], Step [107/112], Loss: 0.6476\n",
      "Epoch [97/300], Step [111/112], Loss: 0.6004\n",
      "Epoch [98/300], Step [3/112], Loss: 0.6619\n",
      "Epoch [98/300], Step [7/112], Loss: 0.6570\n",
      "Epoch [98/300], Step [11/112], Loss: 0.6507\n",
      "Epoch [98/300], Step [15/112], Loss: 0.6412\n",
      "Epoch [98/300], Step [19/112], Loss: 0.6412\n",
      "Epoch [98/300], Step [23/112], Loss: 0.6545\n",
      "Epoch [98/300], Step [27/112], Loss: 0.6293\n",
      "Epoch [98/300], Step [31/112], Loss: 0.6825\n",
      "Epoch [98/300], Step [35/112], Loss: 0.6404\n",
      "Epoch [98/300], Step [39/112], Loss: 0.6450\n",
      "Epoch [98/300], Step [43/112], Loss: 0.6380\n",
      "Epoch [98/300], Step [47/112], Loss: 0.6535\n",
      "Epoch [98/300], Step [51/112], Loss: 0.6389\n",
      "Epoch [98/300], Step [55/112], Loss: 0.6489\n",
      "Epoch [98/300], Step [59/112], Loss: 0.6298\n",
      "Epoch [98/300], Step [63/112], Loss: 0.6400\n",
      "Epoch [98/300], Step [67/112], Loss: 0.6558\n",
      "Epoch [98/300], Step [71/112], Loss: 0.6594\n",
      "Epoch [98/300], Step [75/112], Loss: 0.6650\n",
      "Epoch [98/300], Step [79/112], Loss: 0.6461\n",
      "Epoch [98/300], Step [83/112], Loss: 0.6573\n",
      "Epoch [98/300], Step [87/112], Loss: 0.6379\n",
      "Epoch [98/300], Step [91/112], Loss: 0.6576\n",
      "Epoch [98/300], Step [95/112], Loss: 0.6601\n",
      "Epoch [98/300], Step [99/112], Loss: 0.6595\n",
      "Epoch [98/300], Step [103/112], Loss: 0.6509\n",
      "Epoch [98/300], Step [107/112], Loss: 0.6552\n",
      "Epoch [98/300], Step [111/112], Loss: 0.6016\n",
      "Epoch [99/300], Step [3/112], Loss: 0.6514\n",
      "Epoch [99/300], Step [7/112], Loss: 0.6525\n",
      "Epoch [99/300], Step [11/112], Loss: 0.6503\n",
      "Epoch [99/300], Step [15/112], Loss: 0.6420\n",
      "Epoch [99/300], Step [19/112], Loss: 0.6344\n",
      "Epoch [99/300], Step [23/112], Loss: 0.6494\n",
      "Epoch [99/300], Step [27/112], Loss: 0.6345\n",
      "Epoch [99/300], Step [31/112], Loss: 0.6748\n",
      "Epoch [99/300], Step [35/112], Loss: 0.6449\n",
      "Epoch [99/300], Step [39/112], Loss: 0.6502\n",
      "Epoch [99/300], Step [43/112], Loss: 0.6456\n",
      "Epoch [99/300], Step [47/112], Loss: 0.6623\n",
      "Epoch [99/300], Step [51/112], Loss: 0.6464\n",
      "Epoch [99/300], Step [55/112], Loss: 0.6543\n",
      "Epoch [99/300], Step [59/112], Loss: 0.6324\n",
      "Epoch [99/300], Step [63/112], Loss: 0.6432\n",
      "Epoch [99/300], Step [67/112], Loss: 0.6552\n",
      "Epoch [99/300], Step [71/112], Loss: 0.6601\n",
      "Epoch [99/300], Step [75/112], Loss: 0.6642\n",
      "Epoch [99/300], Step [79/112], Loss: 0.6503\n",
      "Epoch [99/300], Step [83/112], Loss: 0.6632\n",
      "Epoch [99/300], Step [87/112], Loss: 0.6400\n",
      "Epoch [99/300], Step [91/112], Loss: 0.6609\n",
      "Epoch [99/300], Step [95/112], Loss: 0.6594\n",
      "Epoch [99/300], Step [99/112], Loss: 0.6602\n",
      "Epoch [99/300], Step [103/112], Loss: 0.6542\n",
      "Epoch [99/300], Step [107/112], Loss: 0.6464\n",
      "Epoch [99/300], Step [111/112], Loss: 0.6027\n",
      "Epoch [100/300], Step [3/112], Loss: 0.6547\n",
      "Epoch [100/300], Step [7/112], Loss: 0.6600\n",
      "Epoch [100/300], Step [11/112], Loss: 0.6500\n",
      "Epoch [100/300], Step [15/112], Loss: 0.6481\n",
      "Epoch [100/300], Step [19/112], Loss: 0.6347\n",
      "Epoch [100/300], Step [23/112], Loss: 0.6460\n",
      "Epoch [100/300], Step [27/112], Loss: 0.6269\n",
      "Epoch [100/300], Step [31/112], Loss: 0.6757\n",
      "Epoch [100/300], Step [35/112], Loss: 0.6371\n",
      "Epoch [100/300], Step [39/112], Loss: 0.6392\n",
      "Epoch [100/300], Step [43/112], Loss: 0.6460\n",
      "Epoch [100/300], Step [47/112], Loss: 0.6547\n",
      "Epoch [100/300], Step [51/112], Loss: 0.6380\n",
      "Epoch [100/300], Step [55/112], Loss: 0.6476\n",
      "Epoch [100/300], Step [59/112], Loss: 0.6282\n",
      "Epoch [100/300], Step [63/112], Loss: 0.6440\n",
      "Epoch [100/300], Step [67/112], Loss: 0.6541\n",
      "Epoch [100/300], Step [71/112], Loss: 0.6615\n",
      "Epoch [100/300], Step [75/112], Loss: 0.6684\n",
      "Epoch [100/300], Step [79/112], Loss: 0.6427\n",
      "Epoch [100/300], Step [83/112], Loss: 0.6562\n",
      "Epoch [100/300], Step [87/112], Loss: 0.6382\n",
      "Epoch [100/300], Step [91/112], Loss: 0.6548\n",
      "Epoch [100/300], Step [95/112], Loss: 0.6563\n",
      "Epoch [100/300], Step [99/112], Loss: 0.6602\n",
      "Epoch [100/300], Step [103/112], Loss: 0.6566\n",
      "Epoch [100/300], Step [107/112], Loss: 0.6447\n",
      "Epoch [100/300], Step [111/112], Loss: 0.6074\n",
      "Epoch [101/300], Step [3/112], Loss: 0.6548\n",
      "Epoch [101/300], Step [7/112], Loss: 0.6553\n",
      "Epoch [101/300], Step [11/112], Loss: 0.6455\n",
      "Epoch [101/300], Step [15/112], Loss: 0.6467\n",
      "Epoch [101/300], Step [19/112], Loss: 0.6358\n",
      "Epoch [101/300], Step [23/112], Loss: 0.6443\n",
      "Epoch [101/300], Step [27/112], Loss: 0.6291\n",
      "Epoch [101/300], Step [31/112], Loss: 0.6713\n",
      "Epoch [101/300], Step [35/112], Loss: 0.6413\n",
      "Epoch [101/300], Step [39/112], Loss: 0.6449\n",
      "Epoch [101/300], Step [43/112], Loss: 0.6457\n",
      "Epoch [101/300], Step [47/112], Loss: 0.6553\n",
      "Epoch [101/300], Step [51/112], Loss: 0.6344\n",
      "Epoch [101/300], Step [55/112], Loss: 0.6505\n",
      "Epoch [101/300], Step [59/112], Loss: 0.6246\n",
      "Epoch [101/300], Step [63/112], Loss: 0.6404\n",
      "Epoch [101/300], Step [67/112], Loss: 0.6505\n",
      "Epoch [101/300], Step [71/112], Loss: 0.6603\n",
      "Epoch [101/300], Step [75/112], Loss: 0.6662\n",
      "Epoch [101/300], Step [79/112], Loss: 0.6399\n",
      "Epoch [101/300], Step [83/112], Loss: 0.6653\n",
      "Epoch [101/300], Step [87/112], Loss: 0.6369\n",
      "Epoch [101/300], Step [91/112], Loss: 0.6638\n",
      "Epoch [101/300], Step [95/112], Loss: 0.6570\n",
      "Epoch [101/300], Step [99/112], Loss: 0.6604\n",
      "Epoch [101/300], Step [103/112], Loss: 0.6574\n",
      "Epoch [101/300], Step [107/112], Loss: 0.6491\n",
      "Epoch [101/300], Step [111/112], Loss: 0.6014\n",
      "Epoch [102/300], Step [3/112], Loss: 0.6616\n",
      "Epoch [102/300], Step [7/112], Loss: 0.6565\n",
      "Epoch [102/300], Step [11/112], Loss: 0.6452\n",
      "Epoch [102/300], Step [15/112], Loss: 0.6411\n",
      "Epoch [102/300], Step [19/112], Loss: 0.6362\n",
      "Epoch [102/300], Step [23/112], Loss: 0.6485\n",
      "Epoch [102/300], Step [27/112], Loss: 0.6320\n",
      "Epoch [102/300], Step [31/112], Loss: 0.6730\n",
      "Epoch [102/300], Step [35/112], Loss: 0.6392\n",
      "Epoch [102/300], Step [39/112], Loss: 0.6427\n",
      "Epoch [102/300], Step [43/112], Loss: 0.6431\n",
      "Epoch [102/300], Step [47/112], Loss: 0.6478\n",
      "Epoch [102/300], Step [51/112], Loss: 0.6428\n",
      "Epoch [102/300], Step [55/112], Loss: 0.6492\n",
      "Epoch [102/300], Step [59/112], Loss: 0.6282\n",
      "Epoch [102/300], Step [63/112], Loss: 0.6453\n",
      "Epoch [102/300], Step [67/112], Loss: 0.6518\n",
      "Epoch [102/300], Step [71/112], Loss: 0.6513\n",
      "Epoch [102/300], Step [75/112], Loss: 0.6585\n",
      "Epoch [102/300], Step [79/112], Loss: 0.6410\n",
      "Epoch [102/300], Step [83/112], Loss: 0.6649\n",
      "Epoch [102/300], Step [87/112], Loss: 0.6315\n",
      "Epoch [102/300], Step [91/112], Loss: 0.6621\n",
      "Epoch [102/300], Step [95/112], Loss: 0.6546\n",
      "Epoch [102/300], Step [99/112], Loss: 0.6569\n",
      "Epoch [102/300], Step [103/112], Loss: 0.6566\n",
      "Epoch [102/300], Step [107/112], Loss: 0.6488\n",
      "Epoch [102/300], Step [111/112], Loss: 0.6033\n",
      "Epoch [103/300], Step [3/112], Loss: 0.6556\n",
      "Epoch [103/300], Step [7/112], Loss: 0.6581\n",
      "Epoch [103/300], Step [11/112], Loss: 0.6479\n",
      "Epoch [103/300], Step [15/112], Loss: 0.6514\n",
      "Epoch [103/300], Step [19/112], Loss: 0.6422\n",
      "Epoch [103/300], Step [23/112], Loss: 0.6505\n",
      "Epoch [103/300], Step [27/112], Loss: 0.6337\n",
      "Epoch [103/300], Step [31/112], Loss: 0.6731\n",
      "Epoch [103/300], Step [35/112], Loss: 0.6416\n",
      "Epoch [103/300], Step [39/112], Loss: 0.6511\n",
      "Epoch [103/300], Step [43/112], Loss: 0.6483\n",
      "Epoch [103/300], Step [47/112], Loss: 0.6516\n",
      "Epoch [103/300], Step [51/112], Loss: 0.6347\n",
      "Epoch [103/300], Step [55/112], Loss: 0.6516\n",
      "Epoch [103/300], Step [59/112], Loss: 0.6294\n",
      "Epoch [103/300], Step [63/112], Loss: 0.6445\n",
      "Epoch [103/300], Step [67/112], Loss: 0.6588\n",
      "Epoch [103/300], Step [71/112], Loss: 0.6550\n",
      "Epoch [103/300], Step [75/112], Loss: 0.6613\n",
      "Epoch [103/300], Step [79/112], Loss: 0.6474\n",
      "Epoch [103/300], Step [83/112], Loss: 0.6650\n",
      "Epoch [103/300], Step [87/112], Loss: 0.6347\n",
      "Epoch [103/300], Step [91/112], Loss: 0.6546\n",
      "Epoch [103/300], Step [95/112], Loss: 0.6581\n",
      "Epoch [103/300], Step [99/112], Loss: 0.6621\n",
      "Epoch [103/300], Step [103/112], Loss: 0.6560\n",
      "Epoch [103/300], Step [107/112], Loss: 0.6491\n",
      "Epoch [103/300], Step [111/112], Loss: 0.6051\n",
      "Epoch [104/300], Step [3/112], Loss: 0.6598\n",
      "Epoch [104/300], Step [7/112], Loss: 0.6565\n",
      "Epoch [104/300], Step [11/112], Loss: 0.6508\n",
      "Epoch [104/300], Step [15/112], Loss: 0.6530\n",
      "Epoch [104/300], Step [19/112], Loss: 0.6353\n",
      "Epoch [104/300], Step [23/112], Loss: 0.6513\n",
      "Epoch [104/300], Step [27/112], Loss: 0.6342\n",
      "Epoch [104/300], Step [31/112], Loss: 0.6770\n",
      "Epoch [104/300], Step [35/112], Loss: 0.6455\n",
      "Epoch [104/300], Step [39/112], Loss: 0.6469\n",
      "Epoch [104/300], Step [43/112], Loss: 0.6446\n",
      "Epoch [104/300], Step [47/112], Loss: 0.6524\n",
      "Epoch [104/300], Step [51/112], Loss: 0.6401\n",
      "Epoch [104/300], Step [55/112], Loss: 0.6546\n",
      "Epoch [104/300], Step [59/112], Loss: 0.6298\n",
      "Epoch [104/300], Step [63/112], Loss: 0.6327\n",
      "Epoch [104/300], Step [67/112], Loss: 0.6447\n",
      "Epoch [104/300], Step [71/112], Loss: 0.6572\n",
      "Epoch [104/300], Step [75/112], Loss: 0.6647\n",
      "Epoch [104/300], Step [79/112], Loss: 0.6473\n",
      "Epoch [104/300], Step [83/112], Loss: 0.6618\n",
      "Epoch [104/300], Step [87/112], Loss: 0.6394\n",
      "Epoch [104/300], Step [91/112], Loss: 0.6626\n",
      "Epoch [104/300], Step [95/112], Loss: 0.6552\n",
      "Epoch [104/300], Step [99/112], Loss: 0.6633\n",
      "Epoch [104/300], Step [103/112], Loss: 0.6582\n",
      "Epoch [104/300], Step [107/112], Loss: 0.6459\n",
      "Epoch [104/300], Step [111/112], Loss: 0.6093\n",
      "Epoch [105/300], Step [3/112], Loss: 0.6520\n",
      "Epoch [105/300], Step [7/112], Loss: 0.6593\n",
      "Epoch [105/300], Step [11/112], Loss: 0.6537\n",
      "Epoch [105/300], Step [15/112], Loss: 0.6422\n",
      "Epoch [105/300], Step [19/112], Loss: 0.6317\n",
      "Epoch [105/300], Step [23/112], Loss: 0.6449\n",
      "Epoch [105/300], Step [27/112], Loss: 0.6267\n",
      "Epoch [105/300], Step [31/112], Loss: 0.6718\n",
      "Epoch [105/300], Step [35/112], Loss: 0.6396\n",
      "Epoch [105/300], Step [39/112], Loss: 0.6369\n",
      "Epoch [105/300], Step [43/112], Loss: 0.6316\n",
      "Epoch [105/300], Step [47/112], Loss: 0.6604\n",
      "Epoch [105/300], Step [51/112], Loss: 0.6393\n",
      "Epoch [105/300], Step [55/112], Loss: 0.6515\n",
      "Epoch [105/300], Step [59/112], Loss: 0.6305\n",
      "Epoch [105/300], Step [63/112], Loss: 0.6362\n",
      "Epoch [105/300], Step [67/112], Loss: 0.6483\n",
      "Epoch [105/300], Step [71/112], Loss: 0.6559\n",
      "Epoch [105/300], Step [75/112], Loss: 0.6605\n",
      "Epoch [105/300], Step [79/112], Loss: 0.6473\n",
      "Epoch [105/300], Step [83/112], Loss: 0.6580\n",
      "Epoch [105/300], Step [87/112], Loss: 0.6343\n",
      "Epoch [105/300], Step [91/112], Loss: 0.6563\n",
      "Epoch [105/300], Step [95/112], Loss: 0.6566\n",
      "Epoch [105/300], Step [99/112], Loss: 0.6573\n",
      "Epoch [105/300], Step [103/112], Loss: 0.6534\n",
      "Epoch [105/300], Step [107/112], Loss: 0.6427\n",
      "Epoch [105/300], Step [111/112], Loss: 0.6113\n",
      "Epoch [106/300], Step [3/112], Loss: 0.6514\n",
      "Epoch [106/300], Step [7/112], Loss: 0.6628\n",
      "Epoch [106/300], Step [11/112], Loss: 0.6502\n",
      "Epoch [106/300], Step [15/112], Loss: 0.6454\n",
      "Epoch [106/300], Step [19/112], Loss: 0.6371\n",
      "Epoch [106/300], Step [23/112], Loss: 0.6461\n",
      "Epoch [106/300], Step [27/112], Loss: 0.6280\n",
      "Epoch [106/300], Step [31/112], Loss: 0.6732\n",
      "Epoch [106/300], Step [35/112], Loss: 0.6354\n",
      "Epoch [106/300], Step [39/112], Loss: 0.6352\n",
      "Epoch [106/300], Step [43/112], Loss: 0.6439\n",
      "Epoch [106/300], Step [47/112], Loss: 0.6544\n",
      "Epoch [106/300], Step [51/112], Loss: 0.6352\n",
      "Epoch [106/300], Step [55/112], Loss: 0.6550\n",
      "Epoch [106/300], Step [59/112], Loss: 0.6350\n",
      "Epoch [106/300], Step [63/112], Loss: 0.6382\n",
      "Epoch [106/300], Step [67/112], Loss: 0.6532\n",
      "Epoch [106/300], Step [71/112], Loss: 0.6530\n",
      "Epoch [106/300], Step [75/112], Loss: 0.6599\n",
      "Epoch [106/300], Step [79/112], Loss: 0.6417\n",
      "Epoch [106/300], Step [83/112], Loss: 0.6635\n",
      "Epoch [106/300], Step [87/112], Loss: 0.6380\n",
      "Epoch [106/300], Step [91/112], Loss: 0.6615\n",
      "Epoch [106/300], Step [95/112], Loss: 0.6547\n",
      "Epoch [106/300], Step [99/112], Loss: 0.6525\n",
      "Epoch [106/300], Step [103/112], Loss: 0.6490\n",
      "Epoch [106/300], Step [107/112], Loss: 0.6467\n",
      "Epoch [106/300], Step [111/112], Loss: 0.6134\n",
      "Epoch [107/300], Step [3/112], Loss: 0.6503\n",
      "Epoch [107/300], Step [7/112], Loss: 0.6649\n",
      "Epoch [107/300], Step [11/112], Loss: 0.6497\n",
      "Epoch [107/300], Step [15/112], Loss: 0.6454\n",
      "Epoch [107/300], Step [19/112], Loss: 0.6437\n",
      "Epoch [107/300], Step [23/112], Loss: 0.6529\n",
      "Epoch [107/300], Step [27/112], Loss: 0.6293\n",
      "Epoch [107/300], Step [31/112], Loss: 0.6785\n",
      "Epoch [107/300], Step [35/112], Loss: 0.6371\n",
      "Epoch [107/300], Step [39/112], Loss: 0.6378\n",
      "Epoch [107/300], Step [43/112], Loss: 0.6474\n",
      "Epoch [107/300], Step [47/112], Loss: 0.6521\n",
      "Epoch [107/300], Step [51/112], Loss: 0.6361\n",
      "Epoch [107/300], Step [55/112], Loss: 0.6483\n",
      "Epoch [107/300], Step [59/112], Loss: 0.6273\n",
      "Epoch [107/300], Step [63/112], Loss: 0.6437\n",
      "Epoch [107/300], Step [67/112], Loss: 0.6497\n",
      "Epoch [107/300], Step [71/112], Loss: 0.6527\n",
      "Epoch [107/300], Step [75/112], Loss: 0.6618\n",
      "Epoch [107/300], Step [79/112], Loss: 0.6416\n",
      "Epoch [107/300], Step [83/112], Loss: 0.6607\n",
      "Epoch [107/300], Step [87/112], Loss: 0.6398\n",
      "Epoch [107/300], Step [91/112], Loss: 0.6597\n",
      "Epoch [107/300], Step [95/112], Loss: 0.6545\n",
      "Epoch [107/300], Step [99/112], Loss: 0.6593\n",
      "Epoch [107/300], Step [103/112], Loss: 0.6502\n",
      "Epoch [107/300], Step [107/112], Loss: 0.6428\n",
      "Epoch [107/300], Step [111/112], Loss: 0.6101\n",
      "Epoch [108/300], Step [3/112], Loss: 0.6512\n",
      "Epoch [108/300], Step [7/112], Loss: 0.6597\n",
      "Epoch [108/300], Step [11/112], Loss: 0.6476\n",
      "Epoch [108/300], Step [15/112], Loss: 0.6445\n",
      "Epoch [108/300], Step [19/112], Loss: 0.6374\n",
      "Epoch [108/300], Step [23/112], Loss: 0.6506\n",
      "Epoch [108/300], Step [27/112], Loss: 0.6297\n",
      "Epoch [108/300], Step [31/112], Loss: 0.6727\n",
      "Epoch [108/300], Step [35/112], Loss: 0.6306\n",
      "Epoch [108/300], Step [39/112], Loss: 0.6345\n",
      "Epoch [108/300], Step [43/112], Loss: 0.6370\n",
      "Epoch [108/300], Step [47/112], Loss: 0.6560\n",
      "Epoch [108/300], Step [51/112], Loss: 0.6323\n",
      "Epoch [108/300], Step [55/112], Loss: 0.6534\n",
      "Epoch [108/300], Step [59/112], Loss: 0.6282\n",
      "Epoch [108/300], Step [63/112], Loss: 0.6396\n",
      "Epoch [108/300], Step [67/112], Loss: 0.6503\n",
      "Epoch [108/300], Step [71/112], Loss: 0.6522\n",
      "Epoch [108/300], Step [75/112], Loss: 0.6646\n",
      "Epoch [108/300], Step [79/112], Loss: 0.6423\n",
      "Epoch [108/300], Step [83/112], Loss: 0.6636\n",
      "Epoch [108/300], Step [87/112], Loss: 0.6350\n",
      "Epoch [108/300], Step [91/112], Loss: 0.6523\n",
      "Epoch [108/300], Step [95/112], Loss: 0.6525\n",
      "Epoch [108/300], Step [99/112], Loss: 0.6544\n",
      "Epoch [108/300], Step [103/112], Loss: 0.6476\n",
      "Epoch [108/300], Step [107/112], Loss: 0.6424\n",
      "Epoch [108/300], Step [111/112], Loss: 0.6079\n",
      "Epoch [109/300], Step [3/112], Loss: 0.6527\n",
      "Epoch [109/300], Step [7/112], Loss: 0.6595\n",
      "Epoch [109/300], Step [11/112], Loss: 0.6443\n",
      "Epoch [109/300], Step [15/112], Loss: 0.6492\n",
      "Epoch [109/300], Step [19/112], Loss: 0.6413\n",
      "Epoch [109/300], Step [23/112], Loss: 0.6457\n",
      "Epoch [109/300], Step [27/112], Loss: 0.6230\n",
      "Epoch [109/300], Step [31/112], Loss: 0.6697\n",
      "Epoch [109/300], Step [35/112], Loss: 0.6382\n",
      "Epoch [109/300], Step [39/112], Loss: 0.6484\n",
      "Epoch [109/300], Step [43/112], Loss: 0.6358\n",
      "Epoch [109/300], Step [47/112], Loss: 0.6571\n",
      "Epoch [109/300], Step [51/112], Loss: 0.6394\n",
      "Epoch [109/300], Step [55/112], Loss: 0.6485\n",
      "Epoch [109/300], Step [59/112], Loss: 0.6312\n",
      "Epoch [109/300], Step [63/112], Loss: 0.6389\n",
      "Epoch [109/300], Step [67/112], Loss: 0.6489\n",
      "Epoch [109/300], Step [71/112], Loss: 0.6571\n",
      "Epoch [109/300], Step [75/112], Loss: 0.6597\n",
      "Epoch [109/300], Step [79/112], Loss: 0.6430\n",
      "Epoch [109/300], Step [83/112], Loss: 0.6602\n",
      "Epoch [109/300], Step [87/112], Loss: 0.6330\n",
      "Epoch [109/300], Step [91/112], Loss: 0.6602\n",
      "Epoch [109/300], Step [95/112], Loss: 0.6561\n",
      "Epoch [109/300], Step [99/112], Loss: 0.6567\n",
      "Epoch [109/300], Step [103/112], Loss: 0.6531\n",
      "Epoch [109/300], Step [107/112], Loss: 0.6457\n",
      "Epoch [109/300], Step [111/112], Loss: 0.6096\n",
      "Epoch [110/300], Step [3/112], Loss: 0.6629\n",
      "Epoch [110/300], Step [7/112], Loss: 0.6535\n",
      "Epoch [110/300], Step [11/112], Loss: 0.6484\n",
      "Epoch [110/300], Step [15/112], Loss: 0.6458\n",
      "Epoch [110/300], Step [19/112], Loss: 0.6380\n",
      "Epoch [110/300], Step [23/112], Loss: 0.6394\n",
      "Epoch [110/300], Step [27/112], Loss: 0.6272\n",
      "Epoch [110/300], Step [31/112], Loss: 0.6733\n",
      "Epoch [110/300], Step [35/112], Loss: 0.6331\n",
      "Epoch [110/300], Step [39/112], Loss: 0.6408\n",
      "Epoch [110/300], Step [43/112], Loss: 0.6336\n",
      "Epoch [110/300], Step [47/112], Loss: 0.6544\n",
      "Epoch [110/300], Step [51/112], Loss: 0.6364\n",
      "Epoch [110/300], Step [55/112], Loss: 0.6448\n",
      "Epoch [110/300], Step [59/112], Loss: 0.6324\n",
      "Epoch [110/300], Step [63/112], Loss: 0.6320\n",
      "Epoch [110/300], Step [67/112], Loss: 0.6426\n",
      "Epoch [110/300], Step [71/112], Loss: 0.6549\n",
      "Epoch [110/300], Step [75/112], Loss: 0.6539\n",
      "Epoch [110/300], Step [79/112], Loss: 0.6410\n",
      "Epoch [110/300], Step [83/112], Loss: 0.6591\n",
      "Epoch [110/300], Step [87/112], Loss: 0.6310\n",
      "Epoch [110/300], Step [91/112], Loss: 0.6540\n",
      "Epoch [110/300], Step [95/112], Loss: 0.6618\n",
      "Epoch [110/300], Step [99/112], Loss: 0.6589\n",
      "Epoch [110/300], Step [103/112], Loss: 0.6526\n",
      "Epoch [110/300], Step [107/112], Loss: 0.6419\n",
      "Epoch [110/300], Step [111/112], Loss: 0.6068\n",
      "Epoch [111/300], Step [3/112], Loss: 0.6498\n",
      "Epoch [111/300], Step [7/112], Loss: 0.6536\n",
      "Epoch [111/300], Step [11/112], Loss: 0.6453\n",
      "Epoch [111/300], Step [15/112], Loss: 0.6469\n",
      "Epoch [111/300], Step [19/112], Loss: 0.6371\n",
      "Epoch [111/300], Step [23/112], Loss: 0.6417\n",
      "Epoch [111/300], Step [27/112], Loss: 0.6326\n",
      "Epoch [111/300], Step [31/112], Loss: 0.6738\n",
      "Epoch [111/300], Step [35/112], Loss: 0.6324\n",
      "Epoch [111/300], Step [39/112], Loss: 0.6368\n",
      "Epoch [111/300], Step [43/112], Loss: 0.6334\n",
      "Epoch [111/300], Step [47/112], Loss: 0.6558\n",
      "Epoch [111/300], Step [51/112], Loss: 0.6405\n",
      "Epoch [111/300], Step [55/112], Loss: 0.6501\n",
      "Epoch [111/300], Step [59/112], Loss: 0.6286\n",
      "Epoch [111/300], Step [63/112], Loss: 0.6401\n",
      "Epoch [111/300], Step [67/112], Loss: 0.6455\n",
      "Epoch [111/300], Step [71/112], Loss: 0.6506\n",
      "Epoch [111/300], Step [75/112], Loss: 0.6605\n",
      "Epoch [111/300], Step [79/112], Loss: 0.6391\n",
      "Epoch [111/300], Step [83/112], Loss: 0.6613\n",
      "Epoch [111/300], Step [87/112], Loss: 0.6301\n",
      "Epoch [111/300], Step [91/112], Loss: 0.6563\n",
      "Epoch [111/300], Step [95/112], Loss: 0.6599\n",
      "Epoch [111/300], Step [99/112], Loss: 0.6607\n",
      "Epoch [111/300], Step [103/112], Loss: 0.6490\n",
      "Epoch [111/300], Step [107/112], Loss: 0.6478\n",
      "Epoch [111/300], Step [111/112], Loss: 0.6121\n",
      "Epoch [112/300], Step [3/112], Loss: 0.6467\n",
      "Epoch [112/300], Step [7/112], Loss: 0.6545\n",
      "Epoch [112/300], Step [11/112], Loss: 0.6424\n",
      "Epoch [112/300], Step [15/112], Loss: 0.6410\n",
      "Epoch [112/300], Step [19/112], Loss: 0.6295\n",
      "Epoch [112/300], Step [23/112], Loss: 0.6382\n",
      "Epoch [112/300], Step [27/112], Loss: 0.6247\n",
      "Epoch [112/300], Step [31/112], Loss: 0.6660\n",
      "Epoch [112/300], Step [35/112], Loss: 0.6382\n",
      "Epoch [112/300], Step [39/112], Loss: 0.6408\n",
      "Epoch [112/300], Step [43/112], Loss: 0.6340\n",
      "Epoch [112/300], Step [47/112], Loss: 0.6465\n",
      "Epoch [112/300], Step [51/112], Loss: 0.6387\n",
      "Epoch [112/300], Step [55/112], Loss: 0.6416\n",
      "Epoch [112/300], Step [59/112], Loss: 0.6297\n",
      "Epoch [112/300], Step [63/112], Loss: 0.6386\n",
      "Epoch [112/300], Step [67/112], Loss: 0.6436\n",
      "Epoch [112/300], Step [71/112], Loss: 0.6546\n",
      "Epoch [112/300], Step [75/112], Loss: 0.6557\n",
      "Epoch [112/300], Step [79/112], Loss: 0.6412\n",
      "Epoch [112/300], Step [83/112], Loss: 0.6615\n",
      "Epoch [112/300], Step [87/112], Loss: 0.6359\n",
      "Epoch [112/300], Step [91/112], Loss: 0.6620\n",
      "Epoch [112/300], Step [95/112], Loss: 0.6548\n",
      "Epoch [112/300], Step [99/112], Loss: 0.6575\n",
      "Epoch [112/300], Step [103/112], Loss: 0.6538\n",
      "Epoch [112/300], Step [107/112], Loss: 0.6377\n",
      "Epoch [112/300], Step [111/112], Loss: 0.6060\n",
      "Epoch [113/300], Step [3/112], Loss: 0.6496\n",
      "Epoch [113/300], Step [7/112], Loss: 0.6604\n",
      "Epoch [113/300], Step [11/112], Loss: 0.6444\n",
      "Epoch [113/300], Step [15/112], Loss: 0.6482\n",
      "Epoch [113/300], Step [19/112], Loss: 0.6363\n",
      "Epoch [113/300], Step [23/112], Loss: 0.6422\n",
      "Epoch [113/300], Step [27/112], Loss: 0.6264\n",
      "Epoch [113/300], Step [31/112], Loss: 0.6691\n",
      "Epoch [113/300], Step [35/112], Loss: 0.6366\n",
      "Epoch [113/300], Step [39/112], Loss: 0.6332\n",
      "Epoch [113/300], Step [43/112], Loss: 0.6319\n",
      "Epoch [113/300], Step [47/112], Loss: 0.6470\n",
      "Epoch [113/300], Step [51/112], Loss: 0.6365\n",
      "Epoch [113/300], Step [55/112], Loss: 0.6540\n",
      "Epoch [113/300], Step [59/112], Loss: 0.6254\n",
      "Epoch [113/300], Step [63/112], Loss: 0.6405\n",
      "Epoch [113/300], Step [67/112], Loss: 0.6530\n",
      "Epoch [113/300], Step [71/112], Loss: 0.6504\n",
      "Epoch [113/300], Step [75/112], Loss: 0.6629\n",
      "Epoch [113/300], Step [79/112], Loss: 0.6475\n",
      "Epoch [113/300], Step [83/112], Loss: 0.6663\n",
      "Epoch [113/300], Step [87/112], Loss: 0.6312\n",
      "Epoch [113/300], Step [91/112], Loss: 0.6577\n",
      "Epoch [113/300], Step [95/112], Loss: 0.6506\n",
      "Epoch [113/300], Step [99/112], Loss: 0.6611\n",
      "Epoch [113/300], Step [103/112], Loss: 0.6491\n",
      "Epoch [113/300], Step [107/112], Loss: 0.6431\n",
      "Epoch [113/300], Step [111/112], Loss: 0.6052\n",
      "Epoch [114/300], Step [3/112], Loss: 0.6515\n",
      "Epoch [114/300], Step [7/112], Loss: 0.6541\n",
      "Epoch [114/300], Step [11/112], Loss: 0.6457\n",
      "Epoch [114/300], Step [15/112], Loss: 0.6413\n",
      "Epoch [114/300], Step [19/112], Loss: 0.6402\n",
      "Epoch [114/300], Step [23/112], Loss: 0.6444\n",
      "Epoch [114/300], Step [27/112], Loss: 0.6224\n",
      "Epoch [114/300], Step [31/112], Loss: 0.6651\n",
      "Epoch [114/300], Step [35/112], Loss: 0.6352\n",
      "Epoch [114/300], Step [39/112], Loss: 0.6412\n",
      "Epoch [114/300], Step [43/112], Loss: 0.6262\n",
      "Epoch [114/300], Step [47/112], Loss: 0.6580\n",
      "Epoch [114/300], Step [51/112], Loss: 0.6368\n",
      "Epoch [114/300], Step [55/112], Loss: 0.6488\n",
      "Epoch [114/300], Step [59/112], Loss: 0.6241\n",
      "Epoch [114/300], Step [63/112], Loss: 0.6384\n",
      "Epoch [114/300], Step [67/112], Loss: 0.6458\n",
      "Epoch [114/300], Step [71/112], Loss: 0.6469\n",
      "Epoch [114/300], Step [75/112], Loss: 0.6609\n",
      "Epoch [114/300], Step [79/112], Loss: 0.6406\n",
      "Epoch [114/300], Step [83/112], Loss: 0.6643\n",
      "Epoch [114/300], Step [87/112], Loss: 0.6402\n",
      "Epoch [114/300], Step [91/112], Loss: 0.6632\n",
      "Epoch [114/300], Step [95/112], Loss: 0.6504\n",
      "Epoch [114/300], Step [99/112], Loss: 0.6660\n",
      "Epoch [114/300], Step [103/112], Loss: 0.6492\n",
      "Epoch [114/300], Step [107/112], Loss: 0.6472\n",
      "Epoch [114/300], Step [111/112], Loss: 0.6095\n",
      "Epoch [115/300], Step [3/112], Loss: 0.6517\n",
      "Epoch [115/300], Step [7/112], Loss: 0.6585\n",
      "Epoch [115/300], Step [11/112], Loss: 0.6540\n",
      "Epoch [115/300], Step [15/112], Loss: 0.6382\n",
      "Epoch [115/300], Step [19/112], Loss: 0.6337\n",
      "Epoch [115/300], Step [23/112], Loss: 0.6428\n",
      "Epoch [115/300], Step [27/112], Loss: 0.6263\n",
      "Epoch [115/300], Step [31/112], Loss: 0.6630\n",
      "Epoch [115/300], Step [35/112], Loss: 0.6338\n",
      "Epoch [115/300], Step [39/112], Loss: 0.6287\n",
      "Epoch [115/300], Step [43/112], Loss: 0.6281\n",
      "Epoch [115/300], Step [47/112], Loss: 0.6496\n",
      "Epoch [115/300], Step [51/112], Loss: 0.6368\n",
      "Epoch [115/300], Step [55/112], Loss: 0.6463\n",
      "Epoch [115/300], Step [59/112], Loss: 0.6253\n",
      "Epoch [115/300], Step [63/112], Loss: 0.6410\n",
      "Epoch [115/300], Step [67/112], Loss: 0.6434\n",
      "Epoch [115/300], Step [71/112], Loss: 0.6524\n",
      "Epoch [115/300], Step [75/112], Loss: 0.6614\n",
      "Epoch [115/300], Step [79/112], Loss: 0.6388\n",
      "Epoch [115/300], Step [83/112], Loss: 0.6636\n",
      "Epoch [115/300], Step [87/112], Loss: 0.6326\n",
      "Epoch [115/300], Step [91/112], Loss: 0.6555\n",
      "Epoch [115/300], Step [95/112], Loss: 0.6470\n",
      "Epoch [115/300], Step [99/112], Loss: 0.6543\n",
      "Epoch [115/300], Step [103/112], Loss: 0.6536\n",
      "Epoch [115/300], Step [107/112], Loss: 0.6438\n",
      "Epoch [115/300], Step [111/112], Loss: 0.6100\n",
      "Epoch [116/300], Step [3/112], Loss: 0.6460\n",
      "Epoch [116/300], Step [7/112], Loss: 0.6540\n",
      "Epoch [116/300], Step [11/112], Loss: 0.6458\n",
      "Epoch [116/300], Step [15/112], Loss: 0.6399\n",
      "Epoch [116/300], Step [19/112], Loss: 0.6321\n",
      "Epoch [116/300], Step [23/112], Loss: 0.6333\n",
      "Epoch [116/300], Step [27/112], Loss: 0.6208\n",
      "Epoch [116/300], Step [31/112], Loss: 0.6649\n",
      "Epoch [116/300], Step [35/112], Loss: 0.6337\n",
      "Epoch [116/300], Step [39/112], Loss: 0.6306\n",
      "Epoch [116/300], Step [43/112], Loss: 0.6352\n",
      "Epoch [116/300], Step [47/112], Loss: 0.6462\n",
      "Epoch [116/300], Step [51/112], Loss: 0.6333\n",
      "Epoch [116/300], Step [55/112], Loss: 0.6450\n",
      "Epoch [116/300], Step [59/112], Loss: 0.6265\n",
      "Epoch [116/300], Step [63/112], Loss: 0.6346\n",
      "Epoch [116/300], Step [67/112], Loss: 0.6387\n",
      "Epoch [116/300], Step [71/112], Loss: 0.6441\n",
      "Epoch [116/300], Step [75/112], Loss: 0.6542\n",
      "Epoch [116/300], Step [79/112], Loss: 0.6420\n",
      "Epoch [116/300], Step [83/112], Loss: 0.6632\n",
      "Epoch [116/300], Step [87/112], Loss: 0.6398\n",
      "Epoch [116/300], Step [91/112], Loss: 0.6590\n",
      "Epoch [116/300], Step [95/112], Loss: 0.6477\n",
      "Epoch [116/300], Step [99/112], Loss: 0.6519\n",
      "Epoch [116/300], Step [103/112], Loss: 0.6452\n",
      "Epoch [116/300], Step [107/112], Loss: 0.6413\n",
      "Epoch [116/300], Step [111/112], Loss: 0.6035\n",
      "Epoch [117/300], Step [3/112], Loss: 0.6463\n",
      "Epoch [117/300], Step [7/112], Loss: 0.6546\n",
      "Epoch [117/300], Step [11/112], Loss: 0.6391\n",
      "Epoch [117/300], Step [15/112], Loss: 0.6453\n",
      "Epoch [117/300], Step [19/112], Loss: 0.6356\n",
      "Epoch [117/300], Step [23/112], Loss: 0.6355\n",
      "Epoch [117/300], Step [27/112], Loss: 0.6158\n",
      "Epoch [117/300], Step [31/112], Loss: 0.6680\n",
      "Epoch [117/300], Step [35/112], Loss: 0.6321\n",
      "Epoch [117/300], Step [39/112], Loss: 0.6323\n",
      "Epoch [117/300], Step [43/112], Loss: 0.6347\n",
      "Epoch [117/300], Step [47/112], Loss: 0.6515\n",
      "Epoch [117/300], Step [51/112], Loss: 0.6316\n",
      "Epoch [117/300], Step [55/112], Loss: 0.6428\n",
      "Epoch [117/300], Step [59/112], Loss: 0.6255\n",
      "Epoch [117/300], Step [63/112], Loss: 0.6353\n",
      "Epoch [117/300], Step [67/112], Loss: 0.6450\n",
      "Epoch [117/300], Step [71/112], Loss: 0.6429\n",
      "Epoch [117/300], Step [75/112], Loss: 0.6554\n",
      "Epoch [117/300], Step [79/112], Loss: 0.6377\n",
      "Epoch [117/300], Step [83/112], Loss: 0.6576\n",
      "Epoch [117/300], Step [87/112], Loss: 0.6301\n",
      "Epoch [117/300], Step [91/112], Loss: 0.6618\n",
      "Epoch [117/300], Step [95/112], Loss: 0.6470\n",
      "Epoch [117/300], Step [99/112], Loss: 0.6551\n",
      "Epoch [117/300], Step [103/112], Loss: 0.6469\n",
      "Epoch [117/300], Step [107/112], Loss: 0.6418\n",
      "Epoch [117/300], Step [111/112], Loss: 0.6031\n",
      "Epoch [118/300], Step [3/112], Loss: 0.6552\n",
      "Epoch [118/300], Step [7/112], Loss: 0.6548\n",
      "Epoch [118/300], Step [11/112], Loss: 0.6420\n",
      "Epoch [118/300], Step [15/112], Loss: 0.6340\n",
      "Epoch [118/300], Step [19/112], Loss: 0.6296\n",
      "Epoch [118/300], Step [23/112], Loss: 0.6360\n",
      "Epoch [118/300], Step [27/112], Loss: 0.6231\n",
      "Epoch [118/300], Step [31/112], Loss: 0.6740\n",
      "Epoch [118/300], Step [35/112], Loss: 0.6398\n",
      "Epoch [118/300], Step [39/112], Loss: 0.6349\n",
      "Epoch [118/300], Step [43/112], Loss: 0.6282\n",
      "Epoch [118/300], Step [47/112], Loss: 0.6399\n",
      "Epoch [118/300], Step [51/112], Loss: 0.6356\n",
      "Epoch [118/300], Step [55/112], Loss: 0.6410\n",
      "Epoch [118/300], Step [59/112], Loss: 0.6220\n",
      "Epoch [118/300], Step [63/112], Loss: 0.6368\n",
      "Epoch [118/300], Step [67/112], Loss: 0.6421\n",
      "Epoch [118/300], Step [71/112], Loss: 0.6431\n",
      "Epoch [118/300], Step [75/112], Loss: 0.6556\n",
      "Epoch [118/300], Step [79/112], Loss: 0.6372\n",
      "Epoch [118/300], Step [83/112], Loss: 0.6616\n",
      "Epoch [118/300], Step [87/112], Loss: 0.6338\n",
      "Epoch [118/300], Step [91/112], Loss: 0.6598\n",
      "Epoch [118/300], Step [95/112], Loss: 0.6555\n",
      "Epoch [118/300], Step [99/112], Loss: 0.6556\n",
      "Epoch [118/300], Step [103/112], Loss: 0.6466\n",
      "Epoch [118/300], Step [107/112], Loss: 0.6380\n",
      "Epoch [118/300], Step [111/112], Loss: 0.6142\n",
      "Epoch [119/300], Step [3/112], Loss: 0.6428\n",
      "Epoch [119/300], Step [7/112], Loss: 0.6510\n",
      "Epoch [119/300], Step [11/112], Loss: 0.6402\n",
      "Epoch [119/300], Step [15/112], Loss: 0.6427\n",
      "Epoch [119/300], Step [19/112], Loss: 0.6352\n",
      "Epoch [119/300], Step [23/112], Loss: 0.6447\n",
      "Epoch [119/300], Step [27/112], Loss: 0.6257\n",
      "Epoch [119/300], Step [31/112], Loss: 0.6701\n",
      "Epoch [119/300], Step [35/112], Loss: 0.6306\n",
      "Epoch [119/300], Step [39/112], Loss: 0.6321\n",
      "Epoch [119/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [119/300], Step [47/112], Loss: 0.6448\n",
      "Epoch [119/300], Step [51/112], Loss: 0.6348\n",
      "Epoch [119/300], Step [55/112], Loss: 0.6437\n",
      "Epoch [119/300], Step [59/112], Loss: 0.6273\n",
      "Epoch [119/300], Step [63/112], Loss: 0.6378\n",
      "Epoch [119/300], Step [67/112], Loss: 0.6411\n",
      "Epoch [119/300], Step [71/112], Loss: 0.6418\n",
      "Epoch [119/300], Step [75/112], Loss: 0.6591\n",
      "Epoch [119/300], Step [79/112], Loss: 0.6417\n",
      "Epoch [119/300], Step [83/112], Loss: 0.6611\n",
      "Epoch [119/300], Step [87/112], Loss: 0.6417\n",
      "Epoch [119/300], Step [91/112], Loss: 0.6676\n",
      "Epoch [119/300], Step [95/112], Loss: 0.6478\n",
      "Epoch [119/300], Step [99/112], Loss: 0.6598\n",
      "Epoch [119/300], Step [103/112], Loss: 0.6443\n",
      "Epoch [119/300], Step [107/112], Loss: 0.6373\n",
      "Epoch [119/300], Step [111/112], Loss: 0.6080\n",
      "Epoch [120/300], Step [3/112], Loss: 0.6471\n",
      "Epoch [120/300], Step [7/112], Loss: 0.6590\n",
      "Epoch [120/300], Step [11/112], Loss: 0.6469\n",
      "Epoch [120/300], Step [15/112], Loss: 0.6477\n",
      "Epoch [120/300], Step [19/112], Loss: 0.6394\n",
      "Epoch [120/300], Step [23/112], Loss: 0.6417\n",
      "Epoch [120/300], Step [27/112], Loss: 0.6264\n",
      "Epoch [120/300], Step [31/112], Loss: 0.6693\n",
      "Epoch [120/300], Step [35/112], Loss: 0.6274\n",
      "Epoch [120/300], Step [39/112], Loss: 0.6309\n",
      "Epoch [120/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [120/300], Step [47/112], Loss: 0.6470\n",
      "Epoch [120/300], Step [51/112], Loss: 0.6295\n",
      "Epoch [120/300], Step [55/112], Loss: 0.6488\n",
      "Epoch [120/300], Step [59/112], Loss: 0.6277\n",
      "Epoch [120/300], Step [63/112], Loss: 0.6357\n",
      "Epoch [120/300], Step [67/112], Loss: 0.6400\n",
      "Epoch [120/300], Step [71/112], Loss: 0.6447\n",
      "Epoch [120/300], Step [75/112], Loss: 0.6577\n",
      "Epoch [120/300], Step [79/112], Loss: 0.6403\n",
      "Epoch [120/300], Step [83/112], Loss: 0.6612\n",
      "Epoch [120/300], Step [87/112], Loss: 0.6363\n",
      "Epoch [120/300], Step [91/112], Loss: 0.6613\n",
      "Epoch [120/300], Step [95/112], Loss: 0.6494\n",
      "Epoch [120/300], Step [99/112], Loss: 0.6548\n",
      "Epoch [120/300], Step [103/112], Loss: 0.6510\n",
      "Epoch [120/300], Step [107/112], Loss: 0.6386\n",
      "Epoch [120/300], Step [111/112], Loss: 0.6122\n",
      "Epoch [121/300], Step [3/112], Loss: 0.6463\n",
      "Epoch [121/300], Step [7/112], Loss: 0.6566\n",
      "Epoch [121/300], Step [11/112], Loss: 0.6443\n",
      "Epoch [121/300], Step [15/112], Loss: 0.6423\n",
      "Epoch [121/300], Step [19/112], Loss: 0.6468\n",
      "Epoch [121/300], Step [23/112], Loss: 0.6366\n",
      "Epoch [121/300], Step [27/112], Loss: 0.6249\n",
      "Epoch [121/300], Step [31/112], Loss: 0.6709\n",
      "Epoch [121/300], Step [35/112], Loss: 0.6338\n",
      "Epoch [121/300], Step [39/112], Loss: 0.6332\n",
      "Epoch [121/300], Step [43/112], Loss: 0.6332\n",
      "Epoch [121/300], Step [47/112], Loss: 0.6444\n",
      "Epoch [121/300], Step [51/112], Loss: 0.6335\n",
      "Epoch [121/300], Step [55/112], Loss: 0.6451\n",
      "Epoch [121/300], Step [59/112], Loss: 0.6250\n",
      "Epoch [121/300], Step [63/112], Loss: 0.6346\n",
      "Epoch [121/300], Step [67/112], Loss: 0.6433\n",
      "Epoch [121/300], Step [71/112], Loss: 0.6446\n",
      "Epoch [121/300], Step [75/112], Loss: 0.6536\n",
      "Epoch [121/300], Step [79/112], Loss: 0.6365\n",
      "Epoch [121/300], Step [83/112], Loss: 0.6606\n",
      "Epoch [121/300], Step [87/112], Loss: 0.6331\n",
      "Epoch [121/300], Step [91/112], Loss: 0.6570\n",
      "Epoch [121/300], Step [95/112], Loss: 0.6461\n",
      "Epoch [121/300], Step [99/112], Loss: 0.6598\n",
      "Epoch [121/300], Step [103/112], Loss: 0.6595\n",
      "Epoch [121/300], Step [107/112], Loss: 0.6436\n",
      "Epoch [121/300], Step [111/112], Loss: 0.6069\n",
      "Epoch [122/300], Step [3/112], Loss: 0.6411\n",
      "Epoch [122/300], Step [7/112], Loss: 0.6514\n",
      "Epoch [122/300], Step [11/112], Loss: 0.6447\n",
      "Epoch [122/300], Step [15/112], Loss: 0.6392\n",
      "Epoch [122/300], Step [19/112], Loss: 0.6345\n",
      "Epoch [122/300], Step [23/112], Loss: 0.6375\n",
      "Epoch [122/300], Step [27/112], Loss: 0.6201\n",
      "Epoch [122/300], Step [31/112], Loss: 0.6765\n",
      "Epoch [122/300], Step [35/112], Loss: 0.6318\n",
      "Epoch [122/300], Step [39/112], Loss: 0.6411\n",
      "Epoch [122/300], Step [43/112], Loss: 0.6340\n",
      "Epoch [122/300], Step [47/112], Loss: 0.6469\n",
      "Epoch [122/300], Step [51/112], Loss: 0.6337\n",
      "Epoch [122/300], Step [55/112], Loss: 0.6512\n",
      "Epoch [122/300], Step [59/112], Loss: 0.6285\n",
      "Epoch [122/300], Step [63/112], Loss: 0.6317\n",
      "Epoch [122/300], Step [67/112], Loss: 0.6417\n",
      "Epoch [122/300], Step [71/112], Loss: 0.6483\n",
      "Epoch [122/300], Step [75/112], Loss: 0.6546\n",
      "Epoch [122/300], Step [79/112], Loss: 0.6386\n",
      "Epoch [122/300], Step [83/112], Loss: 0.6608\n",
      "Epoch [122/300], Step [87/112], Loss: 0.6372\n",
      "Epoch [122/300], Step [91/112], Loss: 0.6563\n",
      "Epoch [122/300], Step [95/112], Loss: 0.6552\n",
      "Epoch [122/300], Step [99/112], Loss: 0.6557\n",
      "Epoch [122/300], Step [103/112], Loss: 0.6438\n",
      "Epoch [122/300], Step [107/112], Loss: 0.6397\n",
      "Epoch [122/300], Step [111/112], Loss: 0.6004\n",
      "Epoch [123/300], Step [3/112], Loss: 0.6420\n",
      "Epoch [123/300], Step [7/112], Loss: 0.6540\n",
      "Epoch [123/300], Step [11/112], Loss: 0.6469\n",
      "Epoch [123/300], Step [15/112], Loss: 0.6398\n",
      "Epoch [123/300], Step [19/112], Loss: 0.6371\n",
      "Epoch [123/300], Step [23/112], Loss: 0.6342\n",
      "Epoch [123/300], Step [27/112], Loss: 0.6220\n",
      "Epoch [123/300], Step [31/112], Loss: 0.6661\n",
      "Epoch [123/300], Step [35/112], Loss: 0.6308\n",
      "Epoch [123/300], Step [39/112], Loss: 0.6295\n",
      "Epoch [123/300], Step [43/112], Loss: 0.6341\n",
      "Epoch [123/300], Step [47/112], Loss: 0.6389\n",
      "Epoch [123/300], Step [51/112], Loss: 0.6343\n",
      "Epoch [123/300], Step [55/112], Loss: 0.6551\n",
      "Epoch [123/300], Step [59/112], Loss: 0.6220\n",
      "Epoch [123/300], Step [63/112], Loss: 0.6313\n",
      "Epoch [123/300], Step [67/112], Loss: 0.6457\n",
      "Epoch [123/300], Step [71/112], Loss: 0.6489\n",
      "Epoch [123/300], Step [75/112], Loss: 0.6513\n",
      "Epoch [123/300], Step [79/112], Loss: 0.6338\n",
      "Epoch [123/300], Step [83/112], Loss: 0.6613\n",
      "Epoch [123/300], Step [87/112], Loss: 0.6321\n",
      "Epoch [123/300], Step [91/112], Loss: 0.6542\n",
      "Epoch [123/300], Step [95/112], Loss: 0.6488\n",
      "Epoch [123/300], Step [99/112], Loss: 0.6547\n",
      "Epoch [123/300], Step [103/112], Loss: 0.6543\n",
      "Epoch [123/300], Step [107/112], Loss: 0.6471\n",
      "Epoch [123/300], Step [111/112], Loss: 0.6110\n",
      "Epoch [124/300], Step [3/112], Loss: 0.6442\n",
      "Epoch [124/300], Step [7/112], Loss: 0.6539\n",
      "Epoch [124/300], Step [11/112], Loss: 0.6369\n",
      "Epoch [124/300], Step [15/112], Loss: 0.6344\n",
      "Epoch [124/300], Step [19/112], Loss: 0.6322\n",
      "Epoch [124/300], Step [23/112], Loss: 0.6310\n",
      "Epoch [124/300], Step [27/112], Loss: 0.6233\n",
      "Epoch [124/300], Step [31/112], Loss: 0.6720\n",
      "Epoch [124/300], Step [35/112], Loss: 0.6318\n",
      "Epoch [124/300], Step [39/112], Loss: 0.6314\n",
      "Epoch [124/300], Step [43/112], Loss: 0.6305\n",
      "Epoch [124/300], Step [47/112], Loss: 0.6384\n",
      "Epoch [124/300], Step [51/112], Loss: 0.6361\n",
      "Epoch [124/300], Step [55/112], Loss: 0.6496\n",
      "Epoch [124/300], Step [59/112], Loss: 0.6227\n",
      "Epoch [124/300], Step [63/112], Loss: 0.6352\n",
      "Epoch [124/300], Step [67/112], Loss: 0.6485\n",
      "Epoch [124/300], Step [71/112], Loss: 0.6429\n",
      "Epoch [124/300], Step [75/112], Loss: 0.6542\n",
      "Epoch [124/300], Step [79/112], Loss: 0.6396\n",
      "Epoch [124/300], Step [83/112], Loss: 0.6563\n",
      "Epoch [124/300], Step [87/112], Loss: 0.6363\n",
      "Epoch [124/300], Step [91/112], Loss: 0.6589\n",
      "Epoch [124/300], Step [95/112], Loss: 0.6484\n",
      "Epoch [124/300], Step [99/112], Loss: 0.6651\n",
      "Epoch [124/300], Step [103/112], Loss: 0.6529\n",
      "Epoch [124/300], Step [107/112], Loss: 0.6439\n",
      "Epoch [124/300], Step [111/112], Loss: 0.6095\n",
      "Epoch [125/300], Step [3/112], Loss: 0.6462\n",
      "Epoch [125/300], Step [7/112], Loss: 0.6541\n",
      "Epoch [125/300], Step [11/112], Loss: 0.6437\n",
      "Epoch [125/300], Step [15/112], Loss: 0.6355\n",
      "Epoch [125/300], Step [19/112], Loss: 0.6345\n",
      "Epoch [125/300], Step [23/112], Loss: 0.6388\n",
      "Epoch [125/300], Step [27/112], Loss: 0.6316\n",
      "Epoch [125/300], Step [31/112], Loss: 0.6751\n",
      "Epoch [125/300], Step [35/112], Loss: 0.6353\n",
      "Epoch [125/300], Step [39/112], Loss: 0.6343\n",
      "Epoch [125/300], Step [43/112], Loss: 0.6297\n",
      "Epoch [125/300], Step [47/112], Loss: 0.6406\n",
      "Epoch [125/300], Step [51/112], Loss: 0.6337\n",
      "Epoch [125/300], Step [55/112], Loss: 0.6481\n",
      "Epoch [125/300], Step [59/112], Loss: 0.6226\n",
      "Epoch [125/300], Step [63/112], Loss: 0.6390\n",
      "Epoch [125/300], Step [67/112], Loss: 0.6465\n",
      "Epoch [125/300], Step [71/112], Loss: 0.6439\n",
      "Epoch [125/300], Step [75/112], Loss: 0.6601\n",
      "Epoch [125/300], Step [79/112], Loss: 0.6365\n",
      "Epoch [125/300], Step [83/112], Loss: 0.6624\n",
      "Epoch [125/300], Step [87/112], Loss: 0.6342\n",
      "Epoch [125/300], Step [91/112], Loss: 0.6656\n",
      "Epoch [125/300], Step [95/112], Loss: 0.6471\n",
      "Epoch [125/300], Step [99/112], Loss: 0.6602\n",
      "Epoch [125/300], Step [103/112], Loss: 0.6514\n",
      "Epoch [125/300], Step [107/112], Loss: 0.6480\n",
      "Epoch [125/300], Step [111/112], Loss: 0.6089\n",
      "Epoch [126/300], Step [3/112], Loss: 0.6393\n",
      "Epoch [126/300], Step [7/112], Loss: 0.6527\n",
      "Epoch [126/300], Step [11/112], Loss: 0.6414\n",
      "Epoch [126/300], Step [15/112], Loss: 0.6337\n",
      "Epoch [126/300], Step [19/112], Loss: 0.6349\n",
      "Epoch [126/300], Step [23/112], Loss: 0.6370\n",
      "Epoch [126/300], Step [27/112], Loss: 0.6269\n",
      "Epoch [126/300], Step [31/112], Loss: 0.6696\n",
      "Epoch [126/300], Step [35/112], Loss: 0.6302\n",
      "Epoch [126/300], Step [39/112], Loss: 0.6361\n",
      "Epoch [126/300], Step [43/112], Loss: 0.6313\n",
      "Epoch [126/300], Step [47/112], Loss: 0.6411\n",
      "Epoch [126/300], Step [51/112], Loss: 0.6287\n",
      "Epoch [126/300], Step [55/112], Loss: 0.6420\n",
      "Epoch [126/300], Step [59/112], Loss: 0.6139\n",
      "Epoch [126/300], Step [63/112], Loss: 0.6341\n",
      "Epoch [126/300], Step [67/112], Loss: 0.6416\n",
      "Epoch [126/300], Step [71/112], Loss: 0.6521\n",
      "Epoch [126/300], Step [75/112], Loss: 0.6507\n",
      "Epoch [126/300], Step [79/112], Loss: 0.6349\n",
      "Epoch [126/300], Step [83/112], Loss: 0.6547\n",
      "Epoch [126/300], Step [87/112], Loss: 0.6333\n",
      "Epoch [126/300], Step [91/112], Loss: 0.6549\n",
      "Epoch [126/300], Step [95/112], Loss: 0.6477\n",
      "Epoch [126/300], Step [99/112], Loss: 0.6581\n",
      "Epoch [126/300], Step [103/112], Loss: 0.6481\n",
      "Epoch [126/300], Step [107/112], Loss: 0.6449\n",
      "Epoch [126/300], Step [111/112], Loss: 0.6057\n",
      "Epoch [127/300], Step [3/112], Loss: 0.6446\n",
      "Epoch [127/300], Step [7/112], Loss: 0.6517\n",
      "Epoch [127/300], Step [11/112], Loss: 0.6426\n",
      "Epoch [127/300], Step [15/112], Loss: 0.6356\n",
      "Epoch [127/300], Step [19/112], Loss: 0.6296\n",
      "Epoch [127/300], Step [23/112], Loss: 0.6311\n",
      "Epoch [127/300], Step [27/112], Loss: 0.6229\n",
      "Epoch [127/300], Step [31/112], Loss: 0.6635\n",
      "Epoch [127/300], Step [35/112], Loss: 0.6311\n",
      "Epoch [127/300], Step [39/112], Loss: 0.6305\n",
      "Epoch [127/300], Step [43/112], Loss: 0.6266\n",
      "Epoch [127/300], Step [47/112], Loss: 0.6344\n",
      "Epoch [127/300], Step [51/112], Loss: 0.6342\n",
      "Epoch [127/300], Step [55/112], Loss: 0.6464\n",
      "Epoch [127/300], Step [59/112], Loss: 0.6232\n",
      "Epoch [127/300], Step [63/112], Loss: 0.6355\n",
      "Epoch [127/300], Step [67/112], Loss: 0.6466\n",
      "Epoch [127/300], Step [71/112], Loss: 0.6472\n",
      "Epoch [127/300], Step [75/112], Loss: 0.6556\n",
      "Epoch [127/300], Step [79/112], Loss: 0.6362\n",
      "Epoch [127/300], Step [83/112], Loss: 0.6577\n",
      "Epoch [127/300], Step [87/112], Loss: 0.6306\n",
      "Epoch [127/300], Step [91/112], Loss: 0.6588\n",
      "Epoch [127/300], Step [95/112], Loss: 0.6492\n",
      "Epoch [127/300], Step [99/112], Loss: 0.6590\n",
      "Epoch [127/300], Step [103/112], Loss: 0.6541\n",
      "Epoch [127/300], Step [107/112], Loss: 0.6472\n",
      "Epoch [127/300], Step [111/112], Loss: 0.6100\n",
      "Epoch [128/300], Step [3/112], Loss: 0.6436\n",
      "Epoch [128/300], Step [7/112], Loss: 0.6534\n",
      "Epoch [128/300], Step [11/112], Loss: 0.6371\n",
      "Epoch [128/300], Step [15/112], Loss: 0.6393\n",
      "Epoch [128/300], Step [19/112], Loss: 0.6283\n",
      "Epoch [128/300], Step [23/112], Loss: 0.6387\n",
      "Epoch [128/300], Step [27/112], Loss: 0.6166\n",
      "Epoch [128/300], Step [31/112], Loss: 0.6692\n",
      "Epoch [128/300], Step [35/112], Loss: 0.6303\n",
      "Epoch [128/300], Step [39/112], Loss: 0.6261\n",
      "Epoch [128/300], Step [43/112], Loss: 0.6292\n",
      "Epoch [128/300], Step [47/112], Loss: 0.6388\n",
      "Epoch [128/300], Step [51/112], Loss: 0.6311\n",
      "Epoch [128/300], Step [55/112], Loss: 0.6459\n",
      "Epoch [128/300], Step [59/112], Loss: 0.6202\n",
      "Epoch [128/300], Step [63/112], Loss: 0.6343\n",
      "Epoch [128/300], Step [67/112], Loss: 0.6436\n",
      "Epoch [128/300], Step [71/112], Loss: 0.6483\n",
      "Epoch [128/300], Step [75/112], Loss: 0.6546\n",
      "Epoch [128/300], Step [79/112], Loss: 0.6317\n",
      "Epoch [128/300], Step [83/112], Loss: 0.6570\n",
      "Epoch [128/300], Step [87/112], Loss: 0.6336\n",
      "Epoch [128/300], Step [91/112], Loss: 0.6634\n",
      "Epoch [128/300], Step [95/112], Loss: 0.6474\n",
      "Epoch [128/300], Step [99/112], Loss: 0.6560\n",
      "Epoch [128/300], Step [103/112], Loss: 0.6465\n",
      "Epoch [128/300], Step [107/112], Loss: 0.6432\n",
      "Epoch [128/300], Step [111/112], Loss: 0.5983\n",
      "Epoch [129/300], Step [3/112], Loss: 0.6459\n",
      "Epoch [129/300], Step [7/112], Loss: 0.6558\n",
      "Epoch [129/300], Step [11/112], Loss: 0.6390\n",
      "Epoch [129/300], Step [15/112], Loss: 0.6392\n",
      "Epoch [129/300], Step [19/112], Loss: 0.6299\n",
      "Epoch [129/300], Step [23/112], Loss: 0.6368\n",
      "Epoch [129/300], Step [27/112], Loss: 0.6174\n",
      "Epoch [129/300], Step [31/112], Loss: 0.6720\n",
      "Epoch [129/300], Step [35/112], Loss: 0.6379\n",
      "Epoch [129/300], Step [39/112], Loss: 0.6251\n",
      "Epoch [129/300], Step [43/112], Loss: 0.6324\n",
      "Epoch [129/300], Step [47/112], Loss: 0.6345\n",
      "Epoch [129/300], Step [51/112], Loss: 0.6285\n",
      "Epoch [129/300], Step [55/112], Loss: 0.6449\n",
      "Epoch [129/300], Step [59/112], Loss: 0.6258\n",
      "Epoch [129/300], Step [63/112], Loss: 0.6318\n",
      "Epoch [129/300], Step [67/112], Loss: 0.6348\n",
      "Epoch [129/300], Step [71/112], Loss: 0.6520\n",
      "Epoch [129/300], Step [75/112], Loss: 0.6513\n",
      "Epoch [129/300], Step [79/112], Loss: 0.6376\n",
      "Epoch [129/300], Step [83/112], Loss: 0.6537\n",
      "Epoch [129/300], Step [87/112], Loss: 0.6340\n",
      "Epoch [129/300], Step [91/112], Loss: 0.6642\n",
      "Epoch [129/300], Step [95/112], Loss: 0.6454\n",
      "Epoch [129/300], Step [99/112], Loss: 0.6602\n",
      "Epoch [129/300], Step [103/112], Loss: 0.6425\n",
      "Epoch [129/300], Step [107/112], Loss: 0.6443\n",
      "Epoch [129/300], Step [111/112], Loss: 0.6097\n",
      "Epoch [130/300], Step [3/112], Loss: 0.6440\n",
      "Epoch [130/300], Step [7/112], Loss: 0.6482\n",
      "Epoch [130/300], Step [11/112], Loss: 0.6395\n",
      "Epoch [130/300], Step [15/112], Loss: 0.6401\n",
      "Epoch [130/300], Step [19/112], Loss: 0.6318\n",
      "Epoch [130/300], Step [23/112], Loss: 0.6402\n",
      "Epoch [130/300], Step [27/112], Loss: 0.6219\n",
      "Epoch [130/300], Step [31/112], Loss: 0.6715\n",
      "Epoch [130/300], Step [35/112], Loss: 0.6333\n",
      "Epoch [130/300], Step [39/112], Loss: 0.6311\n",
      "Epoch [130/300], Step [43/112], Loss: 0.6362\n",
      "Epoch [130/300], Step [47/112], Loss: 0.6417\n",
      "Epoch [130/300], Step [51/112], Loss: 0.6293\n",
      "Epoch [130/300], Step [55/112], Loss: 0.6443\n",
      "Epoch [130/300], Step [59/112], Loss: 0.6173\n",
      "Epoch [130/300], Step [63/112], Loss: 0.6390\n",
      "Epoch [130/300], Step [67/112], Loss: 0.6369\n",
      "Epoch [130/300], Step [71/112], Loss: 0.6459\n",
      "Epoch [130/300], Step [75/112], Loss: 0.6516\n",
      "Epoch [130/300], Step [79/112], Loss: 0.6307\n",
      "Epoch [130/300], Step [83/112], Loss: 0.6593\n",
      "Epoch [130/300], Step [87/112], Loss: 0.6347\n",
      "Epoch [130/300], Step [91/112], Loss: 0.6553\n",
      "Epoch [130/300], Step [95/112], Loss: 0.6440\n",
      "Epoch [130/300], Step [99/112], Loss: 0.6557\n",
      "Epoch [130/300], Step [103/112], Loss: 0.6467\n",
      "Epoch [130/300], Step [107/112], Loss: 0.6399\n",
      "Epoch [130/300], Step [111/112], Loss: 0.6001\n",
      "Epoch [131/300], Step [3/112], Loss: 0.6410\n",
      "Epoch [131/300], Step [7/112], Loss: 0.6485\n",
      "Epoch [131/300], Step [11/112], Loss: 0.6406\n",
      "Epoch [131/300], Step [15/112], Loss: 0.6354\n",
      "Epoch [131/300], Step [19/112], Loss: 0.6278\n",
      "Epoch [131/300], Step [23/112], Loss: 0.6315\n",
      "Epoch [131/300], Step [27/112], Loss: 0.6185\n",
      "Epoch [131/300], Step [31/112], Loss: 0.6682\n",
      "Epoch [131/300], Step [35/112], Loss: 0.6345\n",
      "Epoch [131/300], Step [39/112], Loss: 0.6336\n",
      "Epoch [131/300], Step [43/112], Loss: 0.6260\n",
      "Epoch [131/300], Step [47/112], Loss: 0.6465\n",
      "Epoch [131/300], Step [51/112], Loss: 0.6329\n",
      "Epoch [131/300], Step [55/112], Loss: 0.6394\n",
      "Epoch [131/300], Step [59/112], Loss: 0.6202\n",
      "Epoch [131/300], Step [63/112], Loss: 0.6307\n",
      "Epoch [131/300], Step [67/112], Loss: 0.6397\n",
      "Epoch [131/300], Step [71/112], Loss: 0.6385\n",
      "Epoch [131/300], Step [75/112], Loss: 0.6532\n",
      "Epoch [131/300], Step [79/112], Loss: 0.6310\n",
      "Epoch [131/300], Step [83/112], Loss: 0.6588\n",
      "Epoch [131/300], Step [87/112], Loss: 0.6396\n",
      "Epoch [131/300], Step [91/112], Loss: 0.6562\n",
      "Epoch [131/300], Step [95/112], Loss: 0.6564\n",
      "Epoch [131/300], Step [99/112], Loss: 0.6536\n",
      "Epoch [131/300], Step [103/112], Loss: 0.6426\n",
      "Epoch [131/300], Step [107/112], Loss: 0.6416\n",
      "Epoch [131/300], Step [111/112], Loss: 0.6028\n",
      "Epoch [132/300], Step [3/112], Loss: 0.6432\n",
      "Epoch [132/300], Step [7/112], Loss: 0.6537\n",
      "Epoch [132/300], Step [11/112], Loss: 0.6456\n",
      "Epoch [132/300], Step [15/112], Loss: 0.6383\n",
      "Epoch [132/300], Step [19/112], Loss: 0.6208\n",
      "Epoch [132/300], Step [23/112], Loss: 0.6402\n",
      "Epoch [132/300], Step [27/112], Loss: 0.6175\n",
      "Epoch [132/300], Step [31/112], Loss: 0.6725\n",
      "Epoch [132/300], Step [35/112], Loss: 0.6331\n",
      "Epoch [132/300], Step [39/112], Loss: 0.6302\n",
      "Epoch [132/300], Step [43/112], Loss: 0.6270\n",
      "Epoch [132/300], Step [47/112], Loss: 0.6410\n",
      "Epoch [132/300], Step [51/112], Loss: 0.6308\n",
      "Epoch [132/300], Step [55/112], Loss: 0.6393\n",
      "Epoch [132/300], Step [59/112], Loss: 0.6161\n",
      "Epoch [132/300], Step [63/112], Loss: 0.6334\n",
      "Epoch [132/300], Step [67/112], Loss: 0.6382\n",
      "Epoch [132/300], Step [71/112], Loss: 0.6504\n",
      "Epoch [132/300], Step [75/112], Loss: 0.6530\n",
      "Epoch [132/300], Step [79/112], Loss: 0.6356\n",
      "Epoch [132/300], Step [83/112], Loss: 0.6630\n",
      "Epoch [132/300], Step [87/112], Loss: 0.6305\n",
      "Epoch [132/300], Step [91/112], Loss: 0.6569\n",
      "Epoch [132/300], Step [95/112], Loss: 0.6465\n",
      "Epoch [132/300], Step [99/112], Loss: 0.6607\n",
      "Epoch [132/300], Step [103/112], Loss: 0.6451\n",
      "Epoch [132/300], Step [107/112], Loss: 0.6451\n",
      "Epoch [132/300], Step [111/112], Loss: 0.6020\n",
      "Epoch [133/300], Step [3/112], Loss: 0.6461\n",
      "Epoch [133/300], Step [7/112], Loss: 0.6507\n",
      "Epoch [133/300], Step [11/112], Loss: 0.6418\n",
      "Epoch [133/300], Step [15/112], Loss: 0.6458\n",
      "Epoch [133/300], Step [19/112], Loss: 0.6271\n",
      "Epoch [133/300], Step [23/112], Loss: 0.6333\n",
      "Epoch [133/300], Step [27/112], Loss: 0.6193\n",
      "Epoch [133/300], Step [31/112], Loss: 0.6641\n",
      "Epoch [133/300], Step [35/112], Loss: 0.6310\n",
      "Epoch [133/300], Step [39/112], Loss: 0.6290\n",
      "Epoch [133/300], Step [43/112], Loss: 0.6261\n",
      "Epoch [133/300], Step [47/112], Loss: 0.6331\n",
      "Epoch [133/300], Step [51/112], Loss: 0.6280\n",
      "Epoch [133/300], Step [55/112], Loss: 0.6465\n",
      "Epoch [133/300], Step [59/112], Loss: 0.6219\n",
      "Epoch [133/300], Step [63/112], Loss: 0.6281\n",
      "Epoch [133/300], Step [67/112], Loss: 0.6369\n",
      "Epoch [133/300], Step [71/112], Loss: 0.6468\n",
      "Epoch [133/300], Step [75/112], Loss: 0.6585\n",
      "Epoch [133/300], Step [79/112], Loss: 0.6318\n",
      "Epoch [133/300], Step [83/112], Loss: 0.6630\n",
      "Epoch [133/300], Step [87/112], Loss: 0.6287\n",
      "Epoch [133/300], Step [91/112], Loss: 0.6565\n",
      "Epoch [133/300], Step [95/112], Loss: 0.6489\n",
      "Epoch [133/300], Step [99/112], Loss: 0.6572\n",
      "Epoch [133/300], Step [103/112], Loss: 0.6480\n",
      "Epoch [133/300], Step [107/112], Loss: 0.6444\n",
      "Epoch [133/300], Step [111/112], Loss: 0.6160\n",
      "Epoch [134/300], Step [3/112], Loss: 0.6418\n",
      "Epoch [134/300], Step [7/112], Loss: 0.6482\n",
      "Epoch [134/300], Step [11/112], Loss: 0.6332\n",
      "Epoch [134/300], Step [15/112], Loss: 0.6341\n",
      "Epoch [134/300], Step [19/112], Loss: 0.6316\n",
      "Epoch [134/300], Step [23/112], Loss: 0.6347\n",
      "Epoch [134/300], Step [27/112], Loss: 0.6133\n",
      "Epoch [134/300], Step [31/112], Loss: 0.6626\n",
      "Epoch [134/300], Step [35/112], Loss: 0.6330\n",
      "Epoch [134/300], Step [39/112], Loss: 0.6258\n",
      "Epoch [134/300], Step [43/112], Loss: 0.6243\n",
      "Epoch [134/300], Step [47/112], Loss: 0.6369\n",
      "Epoch [134/300], Step [51/112], Loss: 0.6295\n",
      "Epoch [134/300], Step [55/112], Loss: 0.6429\n",
      "Epoch [134/300], Step [59/112], Loss: 0.6200\n",
      "Epoch [134/300], Step [63/112], Loss: 0.6369\n",
      "Epoch [134/300], Step [67/112], Loss: 0.6366\n",
      "Epoch [134/300], Step [71/112], Loss: 0.6483\n",
      "Epoch [134/300], Step [75/112], Loss: 0.6574\n",
      "Epoch [134/300], Step [79/112], Loss: 0.6333\n",
      "Epoch [134/300], Step [83/112], Loss: 0.6564\n",
      "Epoch [134/300], Step [87/112], Loss: 0.6297\n",
      "Epoch [134/300], Step [91/112], Loss: 0.6616\n",
      "Epoch [134/300], Step [95/112], Loss: 0.6453\n",
      "Epoch [134/300], Step [99/112], Loss: 0.6598\n",
      "Epoch [134/300], Step [103/112], Loss: 0.6511\n",
      "Epoch [134/300], Step [107/112], Loss: 0.6493\n",
      "Epoch [134/300], Step [111/112], Loss: 0.6029\n",
      "Epoch [135/300], Step [3/112], Loss: 0.6422\n",
      "Epoch [135/300], Step [7/112], Loss: 0.6555\n",
      "Epoch [135/300], Step [11/112], Loss: 0.6360\n",
      "Epoch [135/300], Step [15/112], Loss: 0.6384\n",
      "Epoch [135/300], Step [19/112], Loss: 0.6283\n",
      "Epoch [135/300], Step [23/112], Loss: 0.6336\n",
      "Epoch [135/300], Step [27/112], Loss: 0.6207\n",
      "Epoch [135/300], Step [31/112], Loss: 0.6662\n",
      "Epoch [135/300], Step [35/112], Loss: 0.6307\n",
      "Epoch [135/300], Step [39/112], Loss: 0.6280\n",
      "Epoch [135/300], Step [43/112], Loss: 0.6214\n",
      "Epoch [135/300], Step [47/112], Loss: 0.6342\n",
      "Epoch [135/300], Step [51/112], Loss: 0.6289\n",
      "Epoch [135/300], Step [55/112], Loss: 0.6416\n",
      "Epoch [135/300], Step [59/112], Loss: 0.6265\n",
      "Epoch [135/300], Step [63/112], Loss: 0.6353\n",
      "Epoch [135/300], Step [67/112], Loss: 0.6383\n",
      "Epoch [135/300], Step [71/112], Loss: 0.6387\n",
      "Epoch [135/300], Step [75/112], Loss: 0.6553\n",
      "Epoch [135/300], Step [79/112], Loss: 0.6296\n",
      "Epoch [135/300], Step [83/112], Loss: 0.6561\n",
      "Epoch [135/300], Step [87/112], Loss: 0.6311\n",
      "Epoch [135/300], Step [91/112], Loss: 0.6543\n",
      "Epoch [135/300], Step [95/112], Loss: 0.6525\n",
      "Epoch [135/300], Step [99/112], Loss: 0.6568\n",
      "Epoch [135/300], Step [103/112], Loss: 0.6497\n",
      "Epoch [135/300], Step [107/112], Loss: 0.6469\n",
      "Epoch [135/300], Step [111/112], Loss: 0.6112\n",
      "Epoch [136/300], Step [3/112], Loss: 0.6445\n",
      "Epoch [136/300], Step [7/112], Loss: 0.6598\n",
      "Epoch [136/300], Step [11/112], Loss: 0.6527\n",
      "Epoch [136/300], Step [15/112], Loss: 0.6449\n",
      "Epoch [136/300], Step [19/112], Loss: 0.6327\n",
      "Epoch [136/300], Step [23/112], Loss: 0.6446\n",
      "Epoch [136/300], Step [27/112], Loss: 0.6192\n",
      "Epoch [136/300], Step [31/112], Loss: 0.6773\n",
      "Epoch [136/300], Step [35/112], Loss: 0.6506\n",
      "Epoch [136/300], Step [39/112], Loss: 0.6324\n",
      "Epoch [136/300], Step [43/112], Loss: 0.6277\n",
      "Epoch [136/300], Step [47/112], Loss: 0.6453\n",
      "Epoch [136/300], Step [51/112], Loss: 0.6249\n",
      "Epoch [136/300], Step [55/112], Loss: 0.6451\n",
      "Epoch [136/300], Step [59/112], Loss: 0.6258\n",
      "Epoch [136/300], Step [63/112], Loss: 0.6432\n",
      "Epoch [136/300], Step [67/112], Loss: 0.6365\n",
      "Epoch [136/300], Step [71/112], Loss: 0.6474\n",
      "Epoch [136/300], Step [75/112], Loss: 0.6587\n",
      "Epoch [136/300], Step [79/112], Loss: 0.6310\n",
      "Epoch [136/300], Step [83/112], Loss: 0.6607\n",
      "Epoch [136/300], Step [87/112], Loss: 0.6355\n",
      "Epoch [136/300], Step [91/112], Loss: 0.6626\n",
      "Epoch [136/300], Step [95/112], Loss: 0.6469\n",
      "Epoch [136/300], Step [99/112], Loss: 0.6580\n",
      "Epoch [136/300], Step [103/112], Loss: 0.6504\n",
      "Epoch [136/300], Step [107/112], Loss: 0.6443\n",
      "Epoch [136/300], Step [111/112], Loss: 0.6180\n",
      "Epoch [137/300], Step [3/112], Loss: 0.6550\n",
      "Epoch [137/300], Step [7/112], Loss: 0.6603\n",
      "Epoch [137/300], Step [11/112], Loss: 0.6481\n",
      "Epoch [137/300], Step [15/112], Loss: 0.6485\n",
      "Epoch [137/300], Step [19/112], Loss: 0.6372\n",
      "Epoch [137/300], Step [23/112], Loss: 0.6393\n",
      "Epoch [137/300], Step [27/112], Loss: 0.6228\n",
      "Epoch [137/300], Step [31/112], Loss: 0.6744\n",
      "Epoch [137/300], Step [35/112], Loss: 0.6368\n",
      "Epoch [137/300], Step [39/112], Loss: 0.6316\n",
      "Epoch [137/300], Step [43/112], Loss: 0.6309\n",
      "Epoch [137/300], Step [47/112], Loss: 0.6379\n",
      "Epoch [137/300], Step [51/112], Loss: 0.6328\n",
      "Epoch [137/300], Step [55/112], Loss: 0.6460\n",
      "Epoch [137/300], Step [59/112], Loss: 0.6262\n",
      "Epoch [137/300], Step [63/112], Loss: 0.6394\n",
      "Epoch [137/300], Step [67/112], Loss: 0.6438\n",
      "Epoch [137/300], Step [71/112], Loss: 0.6466\n",
      "Epoch [137/300], Step [75/112], Loss: 0.6610\n",
      "Epoch [137/300], Step [79/112], Loss: 0.6322\n",
      "Epoch [137/300], Step [83/112], Loss: 0.6522\n",
      "Epoch [137/300], Step [87/112], Loss: 0.6308\n",
      "Epoch [137/300], Step [91/112], Loss: 0.6625\n",
      "Epoch [137/300], Step [95/112], Loss: 0.6430\n",
      "Epoch [137/300], Step [99/112], Loss: 0.6574\n",
      "Epoch [137/300], Step [103/112], Loss: 0.6485\n",
      "Epoch [137/300], Step [107/112], Loss: 0.6454\n",
      "Epoch [137/300], Step [111/112], Loss: 0.6079\n",
      "Epoch [138/300], Step [3/112], Loss: 0.6454\n",
      "Epoch [138/300], Step [7/112], Loss: 0.6579\n",
      "Epoch [138/300], Step [11/112], Loss: 0.6425\n",
      "Epoch [138/300], Step [15/112], Loss: 0.6421\n",
      "Epoch [138/300], Step [19/112], Loss: 0.6299\n",
      "Epoch [138/300], Step [23/112], Loss: 0.6358\n",
      "Epoch [138/300], Step [27/112], Loss: 0.6119\n",
      "Epoch [138/300], Step [31/112], Loss: 0.6715\n",
      "Epoch [138/300], Step [35/112], Loss: 0.6385\n",
      "Epoch [138/300], Step [39/112], Loss: 0.6317\n",
      "Epoch [138/300], Step [43/112], Loss: 0.6241\n",
      "Epoch [138/300], Step [47/112], Loss: 0.6381\n",
      "Epoch [138/300], Step [51/112], Loss: 0.6351\n",
      "Epoch [138/300], Step [55/112], Loss: 0.6466\n",
      "Epoch [138/300], Step [59/112], Loss: 0.6219\n",
      "Epoch [138/300], Step [63/112], Loss: 0.6406\n",
      "Epoch [138/300], Step [67/112], Loss: 0.6468\n",
      "Epoch [138/300], Step [71/112], Loss: 0.6451\n",
      "Epoch [138/300], Step [75/112], Loss: 0.6546\n",
      "Epoch [138/300], Step [79/112], Loss: 0.6280\n",
      "Epoch [138/300], Step [83/112], Loss: 0.6580\n",
      "Epoch [138/300], Step [87/112], Loss: 0.6354\n",
      "Epoch [138/300], Step [91/112], Loss: 0.6590\n",
      "Epoch [138/300], Step [95/112], Loss: 0.6419\n",
      "Epoch [138/300], Step [99/112], Loss: 0.6619\n",
      "Epoch [138/300], Step [103/112], Loss: 0.6464\n",
      "Epoch [138/300], Step [107/112], Loss: 0.6474\n",
      "Epoch [138/300], Step [111/112], Loss: 0.6093\n",
      "Epoch [139/300], Step [3/112], Loss: 0.6447\n",
      "Epoch [139/300], Step [7/112], Loss: 0.6593\n",
      "Epoch [139/300], Step [11/112], Loss: 0.6378\n",
      "Epoch [139/300], Step [15/112], Loss: 0.6402\n",
      "Epoch [139/300], Step [19/112], Loss: 0.6343\n",
      "Epoch [139/300], Step [23/112], Loss: 0.6406\n",
      "Epoch [139/300], Step [27/112], Loss: 0.6198\n",
      "Epoch [139/300], Step [31/112], Loss: 0.6747\n",
      "Epoch [139/300], Step [35/112], Loss: 0.6311\n",
      "Epoch [139/300], Step [39/112], Loss: 0.6323\n",
      "Epoch [139/300], Step [43/112], Loss: 0.6285\n",
      "Epoch [139/300], Step [47/112], Loss: 0.6467\n",
      "Epoch [139/300], Step [51/112], Loss: 0.6245\n",
      "Epoch [139/300], Step [55/112], Loss: 0.6331\n",
      "Epoch [139/300], Step [59/112], Loss: 0.6200\n",
      "Epoch [139/300], Step [63/112], Loss: 0.6373\n",
      "Epoch [139/300], Step [67/112], Loss: 0.6377\n",
      "Epoch [139/300], Step [71/112], Loss: 0.6438\n",
      "Epoch [139/300], Step [75/112], Loss: 0.6565\n",
      "Epoch [139/300], Step [79/112], Loss: 0.6316\n",
      "Epoch [139/300], Step [83/112], Loss: 0.6538\n",
      "Epoch [139/300], Step [87/112], Loss: 0.6281\n",
      "Epoch [139/300], Step [91/112], Loss: 0.6580\n",
      "Epoch [139/300], Step [95/112], Loss: 0.6475\n",
      "Epoch [139/300], Step [99/112], Loss: 0.6629\n",
      "Epoch [139/300], Step [103/112], Loss: 0.6438\n",
      "Epoch [139/300], Step [107/112], Loss: 0.6441\n",
      "Epoch [139/300], Step [111/112], Loss: 0.5978\n",
      "Epoch [140/300], Step [3/112], Loss: 0.6393\n",
      "Epoch [140/300], Step [7/112], Loss: 0.6497\n",
      "Epoch [140/300], Step [11/112], Loss: 0.6363\n",
      "Epoch [140/300], Step [15/112], Loss: 0.6400\n",
      "Epoch [140/300], Step [19/112], Loss: 0.6363\n",
      "Epoch [140/300], Step [23/112], Loss: 0.6424\n",
      "Epoch [140/300], Step [27/112], Loss: 0.6236\n",
      "Epoch [140/300], Step [31/112], Loss: 0.6768\n",
      "Epoch [140/300], Step [35/112], Loss: 0.6289\n",
      "Epoch [140/300], Step [39/112], Loss: 0.6321\n",
      "Epoch [140/300], Step [43/112], Loss: 0.6259\n",
      "Epoch [140/300], Step [47/112], Loss: 0.6407\n",
      "Epoch [140/300], Step [51/112], Loss: 0.6314\n",
      "Epoch [140/300], Step [55/112], Loss: 0.6441\n",
      "Epoch [140/300], Step [59/112], Loss: 0.6188\n",
      "Epoch [140/300], Step [63/112], Loss: 0.6304\n",
      "Epoch [140/300], Step [67/112], Loss: 0.6395\n",
      "Epoch [140/300], Step [71/112], Loss: 0.6396\n",
      "Epoch [140/300], Step [75/112], Loss: 0.6581\n",
      "Epoch [140/300], Step [79/112], Loss: 0.6292\n",
      "Epoch [140/300], Step [83/112], Loss: 0.6548\n",
      "Epoch [140/300], Step [87/112], Loss: 0.6411\n",
      "Epoch [140/300], Step [91/112], Loss: 0.6670\n",
      "Epoch [140/300], Step [95/112], Loss: 0.6484\n",
      "Epoch [140/300], Step [99/112], Loss: 0.6546\n",
      "Epoch [140/300], Step [103/112], Loss: 0.6456\n",
      "Epoch [140/300], Step [107/112], Loss: 0.6475\n",
      "Epoch [140/300], Step [111/112], Loss: 0.6027\n",
      "Epoch [141/300], Step [3/112], Loss: 0.6399\n",
      "Epoch [141/300], Step [7/112], Loss: 0.6512\n",
      "Epoch [141/300], Step [11/112], Loss: 0.6446\n",
      "Epoch [141/300], Step [15/112], Loss: 0.6384\n",
      "Epoch [141/300], Step [19/112], Loss: 0.6336\n",
      "Epoch [141/300], Step [23/112], Loss: 0.6404\n",
      "Epoch [141/300], Step [27/112], Loss: 0.6175\n",
      "Epoch [141/300], Step [31/112], Loss: 0.6659\n",
      "Epoch [141/300], Step [35/112], Loss: 0.6329\n",
      "Epoch [141/300], Step [39/112], Loss: 0.6309\n",
      "Epoch [141/300], Step [43/112], Loss: 0.6281\n",
      "Epoch [141/300], Step [47/112], Loss: 0.6396\n",
      "Epoch [141/300], Step [51/112], Loss: 0.6286\n",
      "Epoch [141/300], Step [55/112], Loss: 0.6335\n",
      "Epoch [141/300], Step [59/112], Loss: 0.6207\n",
      "Epoch [141/300], Step [63/112], Loss: 0.6388\n",
      "Epoch [141/300], Step [67/112], Loss: 0.6306\n",
      "Epoch [141/300], Step [71/112], Loss: 0.6423\n",
      "Epoch [141/300], Step [75/112], Loss: 0.6508\n",
      "Epoch [141/300], Step [79/112], Loss: 0.6285\n",
      "Epoch [141/300], Step [83/112], Loss: 0.6560\n",
      "Epoch [141/300], Step [87/112], Loss: 0.6339\n",
      "Epoch [141/300], Step [91/112], Loss: 0.6589\n",
      "Epoch [141/300], Step [95/112], Loss: 0.6482\n",
      "Epoch [141/300], Step [99/112], Loss: 0.6511\n",
      "Epoch [141/300], Step [103/112], Loss: 0.6446\n",
      "Epoch [141/300], Step [107/112], Loss: 0.6444\n",
      "Epoch [141/300], Step [111/112], Loss: 0.6031\n",
      "Epoch [142/300], Step [3/112], Loss: 0.6426\n",
      "Epoch [142/300], Step [7/112], Loss: 0.6503\n",
      "Epoch [142/300], Step [11/112], Loss: 0.6376\n",
      "Epoch [142/300], Step [15/112], Loss: 0.6398\n",
      "Epoch [142/300], Step [19/112], Loss: 0.6313\n",
      "Epoch [142/300], Step [23/112], Loss: 0.6369\n",
      "Epoch [142/300], Step [27/112], Loss: 0.6171\n",
      "Epoch [142/300], Step [31/112], Loss: 0.6659\n",
      "Epoch [142/300], Step [35/112], Loss: 0.6335\n",
      "Epoch [142/300], Step [39/112], Loss: 0.6359\n",
      "Epoch [142/300], Step [43/112], Loss: 0.6259\n",
      "Epoch [142/300], Step [47/112], Loss: 0.6423\n",
      "Epoch [142/300], Step [51/112], Loss: 0.6257\n",
      "Epoch [142/300], Step [55/112], Loss: 0.6390\n",
      "Epoch [142/300], Step [59/112], Loss: 0.6181\n",
      "Epoch [142/300], Step [63/112], Loss: 0.6390\n",
      "Epoch [142/300], Step [67/112], Loss: 0.6361\n",
      "Epoch [142/300], Step [71/112], Loss: 0.6398\n",
      "Epoch [142/300], Step [75/112], Loss: 0.6532\n",
      "Epoch [142/300], Step [79/112], Loss: 0.6337\n",
      "Epoch [142/300], Step [83/112], Loss: 0.6625\n",
      "Epoch [142/300], Step [87/112], Loss: 0.6366\n",
      "Epoch [142/300], Step [91/112], Loss: 0.6681\n",
      "Epoch [142/300], Step [95/112], Loss: 0.6454\n",
      "Epoch [142/300], Step [99/112], Loss: 0.6516\n",
      "Epoch [142/300], Step [103/112], Loss: 0.6420\n",
      "Epoch [142/300], Step [107/112], Loss: 0.6431\n",
      "Epoch [142/300], Step [111/112], Loss: 0.6040\n",
      "Epoch [143/300], Step [3/112], Loss: 0.6522\n",
      "Epoch [143/300], Step [7/112], Loss: 0.6559\n",
      "Epoch [143/300], Step [11/112], Loss: 0.6373\n",
      "Epoch [143/300], Step [15/112], Loss: 0.6406\n",
      "Epoch [143/300], Step [19/112], Loss: 0.6291\n",
      "Epoch [143/300], Step [23/112], Loss: 0.6491\n",
      "Epoch [143/300], Step [27/112], Loss: 0.6186\n",
      "Epoch [143/300], Step [31/112], Loss: 0.6747\n",
      "Epoch [143/300], Step [35/112], Loss: 0.6295\n",
      "Epoch [143/300], Step [39/112], Loss: 0.6250\n",
      "Epoch [143/300], Step [43/112], Loss: 0.6203\n",
      "Epoch [143/300], Step [47/112], Loss: 0.6388\n",
      "Epoch [143/300], Step [51/112], Loss: 0.6217\n",
      "Epoch [143/300], Step [55/112], Loss: 0.6390\n",
      "Epoch [143/300], Step [59/112], Loss: 0.6218\n",
      "Epoch [143/300], Step [63/112], Loss: 0.6293\n",
      "Epoch [143/300], Step [67/112], Loss: 0.6327\n",
      "Epoch [143/300], Step [71/112], Loss: 0.6373\n",
      "Epoch [143/300], Step [75/112], Loss: 0.6553\n",
      "Epoch [143/300], Step [79/112], Loss: 0.6274\n",
      "Epoch [143/300], Step [83/112], Loss: 0.6557\n",
      "Epoch [143/300], Step [87/112], Loss: 0.6327\n",
      "Epoch [143/300], Step [91/112], Loss: 0.6625\n",
      "Epoch [143/300], Step [95/112], Loss: 0.6485\n",
      "Epoch [143/300], Step [99/112], Loss: 0.6605\n",
      "Epoch [143/300], Step [103/112], Loss: 0.6421\n",
      "Epoch [143/300], Step [107/112], Loss: 0.6386\n",
      "Epoch [143/300], Step [111/112], Loss: 0.6029\n",
      "Epoch [144/300], Step [3/112], Loss: 0.6480\n",
      "Epoch [144/300], Step [7/112], Loss: 0.6563\n",
      "Epoch [144/300], Step [11/112], Loss: 0.6394\n",
      "Epoch [144/300], Step [15/112], Loss: 0.6357\n",
      "Epoch [144/300], Step [19/112], Loss: 0.6403\n",
      "Epoch [144/300], Step [23/112], Loss: 0.6414\n",
      "Epoch [144/300], Step [27/112], Loss: 0.6133\n",
      "Epoch [144/300], Step [31/112], Loss: 0.6656\n",
      "Epoch [144/300], Step [35/112], Loss: 0.6429\n",
      "Epoch [144/300], Step [39/112], Loss: 0.6421\n",
      "Epoch [144/300], Step [43/112], Loss: 0.6296\n",
      "Epoch [144/300], Step [47/112], Loss: 0.6399\n",
      "Epoch [144/300], Step [51/112], Loss: 0.6342\n",
      "Epoch [144/300], Step [55/112], Loss: 0.6421\n",
      "Epoch [144/300], Step [59/112], Loss: 0.6263\n",
      "Epoch [144/300], Step [63/112], Loss: 0.6399\n",
      "Epoch [144/300], Step [67/112], Loss: 0.6389\n",
      "Epoch [144/300], Step [71/112], Loss: 0.6467\n",
      "Epoch [144/300], Step [75/112], Loss: 0.6496\n",
      "Epoch [144/300], Step [79/112], Loss: 0.6279\n",
      "Epoch [144/300], Step [83/112], Loss: 0.6609\n",
      "Epoch [144/300], Step [87/112], Loss: 0.6329\n",
      "Epoch [144/300], Step [91/112], Loss: 0.6630\n",
      "Epoch [144/300], Step [95/112], Loss: 0.6458\n",
      "Epoch [144/300], Step [99/112], Loss: 0.6479\n",
      "Epoch [144/300], Step [103/112], Loss: 0.6517\n",
      "Epoch [144/300], Step [107/112], Loss: 0.6386\n",
      "Epoch [144/300], Step [111/112], Loss: 0.6094\n",
      "Epoch [145/300], Step [3/112], Loss: 0.6401\n",
      "Epoch [145/300], Step [7/112], Loss: 0.6579\n",
      "Epoch [145/300], Step [11/112], Loss: 0.6336\n",
      "Epoch [145/300], Step [15/112], Loss: 0.6465\n",
      "Epoch [145/300], Step [19/112], Loss: 0.6342\n",
      "Epoch [145/300], Step [23/112], Loss: 0.6349\n",
      "Epoch [145/300], Step [27/112], Loss: 0.6163\n",
      "Epoch [145/300], Step [31/112], Loss: 0.6669\n",
      "Epoch [145/300], Step [35/112], Loss: 0.6376\n",
      "Epoch [145/300], Step [39/112], Loss: 0.6316\n",
      "Epoch [145/300], Step [43/112], Loss: 0.6275\n",
      "Epoch [145/300], Step [47/112], Loss: 0.6385\n",
      "Epoch [145/300], Step [51/112], Loss: 0.6327\n",
      "Epoch [145/300], Step [55/112], Loss: 0.6353\n",
      "Epoch [145/300], Step [59/112], Loss: 0.6319\n",
      "Epoch [145/300], Step [63/112], Loss: 0.6332\n",
      "Epoch [145/300], Step [67/112], Loss: 0.6320\n",
      "Epoch [145/300], Step [71/112], Loss: 0.6409\n",
      "Epoch [145/300], Step [75/112], Loss: 0.6445\n",
      "Epoch [145/300], Step [79/112], Loss: 0.6339\n",
      "Epoch [145/300], Step [83/112], Loss: 0.6523\n",
      "Epoch [145/300], Step [87/112], Loss: 0.6377\n",
      "Epoch [145/300], Step [91/112], Loss: 0.6571\n",
      "Epoch [145/300], Step [95/112], Loss: 0.6479\n",
      "Epoch [145/300], Step [99/112], Loss: 0.6529\n",
      "Epoch [145/300], Step [103/112], Loss: 0.6426\n",
      "Epoch [145/300], Step [107/112], Loss: 0.6469\n",
      "Epoch [145/300], Step [111/112], Loss: 0.6054\n",
      "Epoch [146/300], Step [3/112], Loss: 0.6444\n",
      "Epoch [146/300], Step [7/112], Loss: 0.6524\n",
      "Epoch [146/300], Step [11/112], Loss: 0.6379\n",
      "Epoch [146/300], Step [15/112], Loss: 0.6358\n",
      "Epoch [146/300], Step [19/112], Loss: 0.6295\n",
      "Epoch [146/300], Step [23/112], Loss: 0.6450\n",
      "Epoch [146/300], Step [27/112], Loss: 0.6095\n",
      "Epoch [146/300], Step [31/112], Loss: 0.6673\n",
      "Epoch [146/300], Step [35/112], Loss: 0.6408\n",
      "Epoch [146/300], Step [39/112], Loss: 0.6293\n",
      "Epoch [146/300], Step [43/112], Loss: 0.6300\n",
      "Epoch [146/300], Step [47/112], Loss: 0.6356\n",
      "Epoch [146/300], Step [51/112], Loss: 0.6306\n",
      "Epoch [146/300], Step [55/112], Loss: 0.6417\n",
      "Epoch [146/300], Step [59/112], Loss: 0.6306\n",
      "Epoch [146/300], Step [63/112], Loss: 0.6365\n",
      "Epoch [146/300], Step [67/112], Loss: 0.6501\n",
      "Epoch [146/300], Step [71/112], Loss: 0.6413\n",
      "Epoch [146/300], Step [75/112], Loss: 0.6535\n",
      "Epoch [146/300], Step [79/112], Loss: 0.6396\n",
      "Epoch [146/300], Step [83/112], Loss: 0.6614\n",
      "Epoch [146/300], Step [87/112], Loss: 0.6352\n",
      "Epoch [146/300], Step [91/112], Loss: 0.6620\n",
      "Epoch [146/300], Step [95/112], Loss: 0.6565\n",
      "Epoch [146/300], Step [99/112], Loss: 0.6509\n",
      "Epoch [146/300], Step [103/112], Loss: 0.6456\n",
      "Epoch [146/300], Step [107/112], Loss: 0.6357\n",
      "Epoch [146/300], Step [111/112], Loss: 0.6067\n",
      "Epoch [147/300], Step [3/112], Loss: 0.6432\n",
      "Epoch [147/300], Step [7/112], Loss: 0.6560\n",
      "Epoch [147/300], Step [11/112], Loss: 0.6411\n",
      "Epoch [147/300], Step [15/112], Loss: 0.6404\n",
      "Epoch [147/300], Step [19/112], Loss: 0.6323\n",
      "Epoch [147/300], Step [23/112], Loss: 0.6430\n",
      "Epoch [147/300], Step [27/112], Loss: 0.6132\n",
      "Epoch [147/300], Step [31/112], Loss: 0.6673\n",
      "Epoch [147/300], Step [35/112], Loss: 0.6431\n",
      "Epoch [147/300], Step [39/112], Loss: 0.6256\n",
      "Epoch [147/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [147/300], Step [47/112], Loss: 0.6332\n",
      "Epoch [147/300], Step [51/112], Loss: 0.6253\n",
      "Epoch [147/300], Step [55/112], Loss: 0.6339\n",
      "Epoch [147/300], Step [59/112], Loss: 0.6287\n",
      "Epoch [147/300], Step [63/112], Loss: 0.6310\n",
      "Epoch [147/300], Step [67/112], Loss: 0.6416\n",
      "Epoch [147/300], Step [71/112], Loss: 0.6479\n",
      "Epoch [147/300], Step [75/112], Loss: 0.6494\n",
      "Epoch [147/300], Step [79/112], Loss: 0.6367\n",
      "Epoch [147/300], Step [83/112], Loss: 0.6612\n",
      "Epoch [147/300], Step [87/112], Loss: 0.6256\n",
      "Epoch [147/300], Step [91/112], Loss: 0.6650\n",
      "Epoch [147/300], Step [95/112], Loss: 0.6476\n",
      "Epoch [147/300], Step [99/112], Loss: 0.6529\n",
      "Epoch [147/300], Step [103/112], Loss: 0.6437\n",
      "Epoch [147/300], Step [107/112], Loss: 0.6410\n",
      "Epoch [147/300], Step [111/112], Loss: 0.6072\n",
      "Epoch [148/300], Step [3/112], Loss: 0.6498\n",
      "Epoch [148/300], Step [7/112], Loss: 0.6554\n",
      "Epoch [148/300], Step [11/112], Loss: 0.6377\n",
      "Epoch [148/300], Step [15/112], Loss: 0.6429\n",
      "Epoch [148/300], Step [19/112], Loss: 0.6237\n",
      "Epoch [148/300], Step [23/112], Loss: 0.6357\n",
      "Epoch [148/300], Step [27/112], Loss: 0.6098\n",
      "Epoch [148/300], Step [31/112], Loss: 0.6633\n",
      "Epoch [148/300], Step [35/112], Loss: 0.6338\n",
      "Epoch [148/300], Step [39/112], Loss: 0.6334\n",
      "Epoch [148/300], Step [43/112], Loss: 0.6238\n",
      "Epoch [148/300], Step [47/112], Loss: 0.6354\n",
      "Epoch [148/300], Step [51/112], Loss: 0.6292\n",
      "Epoch [148/300], Step [55/112], Loss: 0.6397\n",
      "Epoch [148/300], Step [59/112], Loss: 0.6175\n",
      "Epoch [148/300], Step [63/112], Loss: 0.6398\n",
      "Epoch [148/300], Step [67/112], Loss: 0.6343\n",
      "Epoch [148/300], Step [71/112], Loss: 0.6386\n",
      "Epoch [148/300], Step [75/112], Loss: 0.6521\n",
      "Epoch [148/300], Step [79/112], Loss: 0.6277\n",
      "Epoch [148/300], Step [83/112], Loss: 0.6589\n",
      "Epoch [148/300], Step [87/112], Loss: 0.6306\n",
      "Epoch [148/300], Step [91/112], Loss: 0.6547\n",
      "Epoch [148/300], Step [95/112], Loss: 0.6404\n",
      "Epoch [148/300], Step [99/112], Loss: 0.6529\n",
      "Epoch [148/300], Step [103/112], Loss: 0.6393\n",
      "Epoch [148/300], Step [107/112], Loss: 0.6405\n",
      "Epoch [148/300], Step [111/112], Loss: 0.6024\n",
      "Epoch [149/300], Step [3/112], Loss: 0.6405\n",
      "Epoch [149/300], Step [7/112], Loss: 0.6509\n",
      "Epoch [149/300], Step [11/112], Loss: 0.6379\n",
      "Epoch [149/300], Step [15/112], Loss: 0.6434\n",
      "Epoch [149/300], Step [19/112], Loss: 0.6224\n",
      "Epoch [149/300], Step [23/112], Loss: 0.6341\n",
      "Epoch [149/300], Step [27/112], Loss: 0.6116\n",
      "Epoch [149/300], Step [31/112], Loss: 0.6574\n",
      "Epoch [149/300], Step [35/112], Loss: 0.6337\n",
      "Epoch [149/300], Step [39/112], Loss: 0.6255\n",
      "Epoch [149/300], Step [43/112], Loss: 0.6212\n",
      "Epoch [149/300], Step [47/112], Loss: 0.6379\n",
      "Epoch [149/300], Step [51/112], Loss: 0.6297\n",
      "Epoch [149/300], Step [55/112], Loss: 0.6405\n",
      "Epoch [149/300], Step [59/112], Loss: 0.6228\n",
      "Epoch [149/300], Step [63/112], Loss: 0.6292\n",
      "Epoch [149/300], Step [67/112], Loss: 0.6292\n",
      "Epoch [149/300], Step [71/112], Loss: 0.6405\n",
      "Epoch [149/300], Step [75/112], Loss: 0.6464\n",
      "Epoch [149/300], Step [79/112], Loss: 0.6323\n",
      "Epoch [149/300], Step [83/112], Loss: 0.6562\n",
      "Epoch [149/300], Step [87/112], Loss: 0.6294\n",
      "Epoch [149/300], Step [91/112], Loss: 0.6616\n",
      "Epoch [149/300], Step [95/112], Loss: 0.6426\n",
      "Epoch [149/300], Step [99/112], Loss: 0.6562\n",
      "Epoch [149/300], Step [103/112], Loss: 0.6453\n",
      "Epoch [149/300], Step [107/112], Loss: 0.6421\n",
      "Epoch [149/300], Step [111/112], Loss: 0.6010\n",
      "Epoch [150/300], Step [3/112], Loss: 0.6508\n",
      "Epoch [150/300], Step [7/112], Loss: 0.6501\n",
      "Epoch [150/300], Step [11/112], Loss: 0.6382\n",
      "Epoch [150/300], Step [15/112], Loss: 0.6398\n",
      "Epoch [150/300], Step [19/112], Loss: 0.6241\n",
      "Epoch [150/300], Step [23/112], Loss: 0.6328\n",
      "Epoch [150/300], Step [27/112], Loss: 0.6214\n",
      "Epoch [150/300], Step [31/112], Loss: 0.6656\n",
      "Epoch [150/300], Step [35/112], Loss: 0.6245\n",
      "Epoch [150/300], Step [39/112], Loss: 0.6269\n",
      "Epoch [150/300], Step [43/112], Loss: 0.6236\n",
      "Epoch [150/300], Step [47/112], Loss: 0.6313\n",
      "Epoch [150/300], Step [51/112], Loss: 0.6294\n",
      "Epoch [150/300], Step [55/112], Loss: 0.6388\n",
      "Epoch [150/300], Step [59/112], Loss: 0.6204\n",
      "Epoch [150/300], Step [63/112], Loss: 0.6278\n",
      "Epoch [150/300], Step [67/112], Loss: 0.6380\n",
      "Epoch [150/300], Step [71/112], Loss: 0.6432\n",
      "Epoch [150/300], Step [75/112], Loss: 0.6532\n",
      "Epoch [150/300], Step [79/112], Loss: 0.6304\n",
      "Epoch [150/300], Step [83/112], Loss: 0.6516\n",
      "Epoch [150/300], Step [87/112], Loss: 0.6291\n",
      "Epoch [150/300], Step [91/112], Loss: 0.6549\n",
      "Epoch [150/300], Step [95/112], Loss: 0.6460\n",
      "Epoch [150/300], Step [99/112], Loss: 0.6497\n",
      "Epoch [150/300], Step [103/112], Loss: 0.6404\n",
      "Epoch [150/300], Step [107/112], Loss: 0.6389\n",
      "Epoch [150/300], Step [111/112], Loss: 0.6012\n",
      "Epoch [151/300], Step [3/112], Loss: 0.6441\n",
      "Epoch [151/300], Step [7/112], Loss: 0.6529\n",
      "Epoch [151/300], Step [11/112], Loss: 0.6338\n",
      "Epoch [151/300], Step [15/112], Loss: 0.6330\n",
      "Epoch [151/300], Step [19/112], Loss: 0.6298\n",
      "Epoch [151/300], Step [23/112], Loss: 0.6373\n",
      "Epoch [151/300], Step [27/112], Loss: 0.6119\n",
      "Epoch [151/300], Step [31/112], Loss: 0.6617\n",
      "Epoch [151/300], Step [35/112], Loss: 0.6292\n",
      "Epoch [151/300], Step [39/112], Loss: 0.6255\n",
      "Epoch [151/300], Step [43/112], Loss: 0.6237\n",
      "Epoch [151/300], Step [47/112], Loss: 0.6329\n",
      "Epoch [151/300], Step [51/112], Loss: 0.6257\n",
      "Epoch [151/300], Step [55/112], Loss: 0.6322\n",
      "Epoch [151/300], Step [59/112], Loss: 0.6174\n",
      "Epoch [151/300], Step [63/112], Loss: 0.6320\n",
      "Epoch [151/300], Step [67/112], Loss: 0.6373\n",
      "Epoch [151/300], Step [71/112], Loss: 0.6371\n",
      "Epoch [151/300], Step [75/112], Loss: 0.6490\n",
      "Epoch [151/300], Step [79/112], Loss: 0.6325\n",
      "Epoch [151/300], Step [83/112], Loss: 0.6603\n",
      "Epoch [151/300], Step [87/112], Loss: 0.6374\n",
      "Epoch [151/300], Step [91/112], Loss: 0.6596\n",
      "Epoch [151/300], Step [95/112], Loss: 0.6398\n",
      "Epoch [151/300], Step [99/112], Loss: 0.6490\n",
      "Epoch [151/300], Step [103/112], Loss: 0.6384\n",
      "Epoch [151/300], Step [107/112], Loss: 0.6446\n",
      "Epoch [151/300], Step [111/112], Loss: 0.5977\n",
      "Epoch [152/300], Step [3/112], Loss: 0.6432\n",
      "Epoch [152/300], Step [7/112], Loss: 0.6618\n",
      "Epoch [152/300], Step [11/112], Loss: 0.6408\n",
      "Epoch [152/300], Step [15/112], Loss: 0.6359\n",
      "Epoch [152/300], Step [19/112], Loss: 0.6212\n",
      "Epoch [152/300], Step [23/112], Loss: 0.6339\n",
      "Epoch [152/300], Step [27/112], Loss: 0.6132\n",
      "Epoch [152/300], Step [31/112], Loss: 0.6589\n",
      "Epoch [152/300], Step [35/112], Loss: 0.6340\n",
      "Epoch [152/300], Step [39/112], Loss: 0.6290\n",
      "Epoch [152/300], Step [43/112], Loss: 0.6191\n",
      "Epoch [152/300], Step [47/112], Loss: 0.6422\n",
      "Epoch [152/300], Step [51/112], Loss: 0.6284\n",
      "Epoch [152/300], Step [55/112], Loss: 0.6336\n",
      "Epoch [152/300], Step [59/112], Loss: 0.6201\n",
      "Epoch [152/300], Step [63/112], Loss: 0.6368\n",
      "Epoch [152/300], Step [67/112], Loss: 0.6297\n",
      "Epoch [152/300], Step [71/112], Loss: 0.6398\n",
      "Epoch [152/300], Step [75/112], Loss: 0.6504\n",
      "Epoch [152/300], Step [79/112], Loss: 0.6298\n",
      "Epoch [152/300], Step [83/112], Loss: 0.6566\n",
      "Epoch [152/300], Step [87/112], Loss: 0.6293\n",
      "Epoch [152/300], Step [91/112], Loss: 0.6582\n",
      "Epoch [152/300], Step [95/112], Loss: 0.6442\n",
      "Epoch [152/300], Step [99/112], Loss: 0.6498\n",
      "Epoch [152/300], Step [103/112], Loss: 0.6395\n",
      "Epoch [152/300], Step [107/112], Loss: 0.6387\n",
      "Epoch [152/300], Step [111/112], Loss: 0.6036\n",
      "Epoch [153/300], Step [3/112], Loss: 0.6446\n",
      "Epoch [153/300], Step [7/112], Loss: 0.6530\n",
      "Epoch [153/300], Step [11/112], Loss: 0.6331\n",
      "Epoch [153/300], Step [15/112], Loss: 0.6435\n",
      "Epoch [153/300], Step [19/112], Loss: 0.6320\n",
      "Epoch [153/300], Step [23/112], Loss: 0.6351\n",
      "Epoch [153/300], Step [27/112], Loss: 0.6111\n",
      "Epoch [153/300], Step [31/112], Loss: 0.6724\n",
      "Epoch [153/300], Step [35/112], Loss: 0.6410\n",
      "Epoch [153/300], Step [39/112], Loss: 0.6316\n",
      "Epoch [153/300], Step [43/112], Loss: 0.6276\n",
      "Epoch [153/300], Step [47/112], Loss: 0.6405\n",
      "Epoch [153/300], Step [51/112], Loss: 0.6260\n",
      "Epoch [153/300], Step [55/112], Loss: 0.6393\n",
      "Epoch [153/300], Step [59/112], Loss: 0.6203\n",
      "Epoch [153/300], Step [63/112], Loss: 0.6286\n",
      "Epoch [153/300], Step [67/112], Loss: 0.6259\n",
      "Epoch [153/300], Step [71/112], Loss: 0.6396\n",
      "Epoch [153/300], Step [75/112], Loss: 0.6495\n",
      "Epoch [153/300], Step [79/112], Loss: 0.6367\n",
      "Epoch [153/300], Step [83/112], Loss: 0.6595\n",
      "Epoch [153/300], Step [87/112], Loss: 0.6349\n",
      "Epoch [153/300], Step [91/112], Loss: 0.6644\n",
      "Epoch [153/300], Step [95/112], Loss: 0.6522\n",
      "Epoch [153/300], Step [99/112], Loss: 0.6482\n",
      "Epoch [153/300], Step [103/112], Loss: 0.6425\n",
      "Epoch [153/300], Step [107/112], Loss: 0.6388\n",
      "Epoch [153/300], Step [111/112], Loss: 0.6044\n",
      "Epoch [154/300], Step [3/112], Loss: 0.6527\n",
      "Epoch [154/300], Step [7/112], Loss: 0.6545\n",
      "Epoch [154/300], Step [11/112], Loss: 0.6404\n",
      "Epoch [154/300], Step [15/112], Loss: 0.6349\n",
      "Epoch [154/300], Step [19/112], Loss: 0.6351\n",
      "Epoch [154/300], Step [23/112], Loss: 0.6384\n",
      "Epoch [154/300], Step [27/112], Loss: 0.6155\n",
      "Epoch [154/300], Step [31/112], Loss: 0.6627\n",
      "Epoch [154/300], Step [35/112], Loss: 0.6335\n",
      "Epoch [154/300], Step [39/112], Loss: 0.6326\n",
      "Epoch [154/300], Step [43/112], Loss: 0.6249\n",
      "Epoch [154/300], Step [47/112], Loss: 0.6433\n",
      "Epoch [154/300], Step [51/112], Loss: 0.6322\n",
      "Epoch [154/300], Step [55/112], Loss: 0.6361\n",
      "Epoch [154/300], Step [59/112], Loss: 0.6189\n",
      "Epoch [154/300], Step [63/112], Loss: 0.6361\n",
      "Epoch [154/300], Step [67/112], Loss: 0.6310\n",
      "Epoch [154/300], Step [71/112], Loss: 0.6388\n",
      "Epoch [154/300], Step [75/112], Loss: 0.6460\n",
      "Epoch [154/300], Step [79/112], Loss: 0.6332\n",
      "Epoch [154/300], Step [83/112], Loss: 0.6575\n",
      "Epoch [154/300], Step [87/112], Loss: 0.6346\n",
      "Epoch [154/300], Step [91/112], Loss: 0.6636\n",
      "Epoch [154/300], Step [95/112], Loss: 0.6499\n",
      "Epoch [154/300], Step [99/112], Loss: 0.6512\n",
      "Epoch [154/300], Step [103/112], Loss: 0.6340\n",
      "Epoch [154/300], Step [107/112], Loss: 0.6416\n",
      "Epoch [154/300], Step [111/112], Loss: 0.6042\n",
      "Epoch [155/300], Step [3/112], Loss: 0.6478\n",
      "Epoch [155/300], Step [7/112], Loss: 0.6595\n",
      "Epoch [155/300], Step [11/112], Loss: 0.6387\n",
      "Epoch [155/300], Step [15/112], Loss: 0.6415\n",
      "Epoch [155/300], Step [19/112], Loss: 0.6264\n",
      "Epoch [155/300], Step [23/112], Loss: 0.6311\n",
      "Epoch [155/300], Step [27/112], Loss: 0.6158\n",
      "Epoch [155/300], Step [31/112], Loss: 0.6728\n",
      "Epoch [155/300], Step [35/112], Loss: 0.6301\n",
      "Epoch [155/300], Step [39/112], Loss: 0.6226\n",
      "Epoch [155/300], Step [43/112], Loss: 0.6280\n",
      "Epoch [155/300], Step [47/112], Loss: 0.6364\n",
      "Epoch [155/300], Step [51/112], Loss: 0.6353\n",
      "Epoch [155/300], Step [55/112], Loss: 0.6367\n",
      "Epoch [155/300], Step [59/112], Loss: 0.6268\n",
      "Epoch [155/300], Step [63/112], Loss: 0.6447\n",
      "Epoch [155/300], Step [67/112], Loss: 0.6358\n",
      "Epoch [155/300], Step [71/112], Loss: 0.6447\n",
      "Epoch [155/300], Step [75/112], Loss: 0.6540\n",
      "Epoch [155/300], Step [79/112], Loss: 0.6284\n",
      "Epoch [155/300], Step [83/112], Loss: 0.6641\n",
      "Epoch [155/300], Step [87/112], Loss: 0.6273\n",
      "Epoch [155/300], Step [91/112], Loss: 0.6613\n",
      "Epoch [155/300], Step [95/112], Loss: 0.6448\n",
      "Epoch [155/300], Step [99/112], Loss: 0.6490\n",
      "Epoch [155/300], Step [103/112], Loss: 0.6366\n",
      "Epoch [155/300], Step [107/112], Loss: 0.6384\n",
      "Epoch [155/300], Step [111/112], Loss: 0.5985\n",
      "Epoch [156/300], Step [3/112], Loss: 0.6572\n",
      "Epoch [156/300], Step [7/112], Loss: 0.6566\n",
      "Epoch [156/300], Step [11/112], Loss: 0.6483\n",
      "Epoch [156/300], Step [15/112], Loss: 0.6342\n",
      "Epoch [156/300], Step [19/112], Loss: 0.6286\n",
      "Epoch [156/300], Step [23/112], Loss: 0.6380\n",
      "Epoch [156/300], Step [27/112], Loss: 0.6140\n",
      "Epoch [156/300], Step [31/112], Loss: 0.6595\n",
      "Epoch [156/300], Step [35/112], Loss: 0.6346\n",
      "Epoch [156/300], Step [39/112], Loss: 0.6255\n",
      "Epoch [156/300], Step [43/112], Loss: 0.6231\n",
      "Epoch [156/300], Step [47/112], Loss: 0.6425\n",
      "Epoch [156/300], Step [51/112], Loss: 0.6285\n",
      "Epoch [156/300], Step [55/112], Loss: 0.6300\n",
      "Epoch [156/300], Step [59/112], Loss: 0.6192\n",
      "Epoch [156/300], Step [63/112], Loss: 0.6305\n",
      "Epoch [156/300], Step [67/112], Loss: 0.6361\n",
      "Epoch [156/300], Step [71/112], Loss: 0.6430\n",
      "Epoch [156/300], Step [75/112], Loss: 0.6451\n",
      "Epoch [156/300], Step [79/112], Loss: 0.6370\n",
      "Epoch [156/300], Step [83/112], Loss: 0.6583\n",
      "Epoch [156/300], Step [87/112], Loss: 0.6324\n",
      "Epoch [156/300], Step [91/112], Loss: 0.6612\n",
      "Epoch [156/300], Step [95/112], Loss: 0.6488\n",
      "Epoch [156/300], Step [99/112], Loss: 0.6536\n",
      "Epoch [156/300], Step [103/112], Loss: 0.6375\n",
      "Epoch [156/300], Step [107/112], Loss: 0.6445\n",
      "Epoch [156/300], Step [111/112], Loss: 0.6039\n",
      "Epoch [157/300], Step [3/112], Loss: 0.6394\n",
      "Epoch [157/300], Step [7/112], Loss: 0.6494\n",
      "Epoch [157/300], Step [11/112], Loss: 0.6398\n",
      "Epoch [157/300], Step [15/112], Loss: 0.6449\n",
      "Epoch [157/300], Step [19/112], Loss: 0.6331\n",
      "Epoch [157/300], Step [23/112], Loss: 0.6388\n",
      "Epoch [157/300], Step [27/112], Loss: 0.6178\n",
      "Epoch [157/300], Step [31/112], Loss: 0.6683\n",
      "Epoch [157/300], Step [35/112], Loss: 0.6344\n",
      "Epoch [157/300], Step [39/112], Loss: 0.6302\n",
      "Epoch [157/300], Step [43/112], Loss: 0.6234\n",
      "Epoch [157/300], Step [47/112], Loss: 0.6370\n",
      "Epoch [157/300], Step [51/112], Loss: 0.6305\n",
      "Epoch [157/300], Step [55/112], Loss: 0.6318\n",
      "Epoch [157/300], Step [59/112], Loss: 0.6239\n",
      "Epoch [157/300], Step [63/112], Loss: 0.6352\n",
      "Epoch [157/300], Step [67/112], Loss: 0.6286\n",
      "Epoch [157/300], Step [71/112], Loss: 0.6439\n",
      "Epoch [157/300], Step [75/112], Loss: 0.6451\n",
      "Epoch [157/300], Step [79/112], Loss: 0.6264\n",
      "Epoch [157/300], Step [83/112], Loss: 0.6534\n",
      "Epoch [157/300], Step [87/112], Loss: 0.6293\n",
      "Epoch [157/300], Step [91/112], Loss: 0.6539\n",
      "Epoch [157/300], Step [95/112], Loss: 0.6423\n",
      "Epoch [157/300], Step [99/112], Loss: 0.6478\n",
      "Epoch [157/300], Step [103/112], Loss: 0.6418\n",
      "Epoch [157/300], Step [107/112], Loss: 0.6370\n",
      "Epoch [157/300], Step [111/112], Loss: 0.6071\n",
      "Epoch [158/300], Step [3/112], Loss: 0.6398\n",
      "Epoch [158/300], Step [7/112], Loss: 0.6476\n",
      "Epoch [158/300], Step [11/112], Loss: 0.6438\n",
      "Epoch [158/300], Step [15/112], Loss: 0.6472\n",
      "Epoch [158/300], Step [19/112], Loss: 0.6246\n",
      "Epoch [158/300], Step [23/112], Loss: 0.6382\n",
      "Epoch [158/300], Step [27/112], Loss: 0.6182\n",
      "Epoch [158/300], Step [31/112], Loss: 0.6650\n",
      "Epoch [158/300], Step [35/112], Loss: 0.6342\n",
      "Epoch [158/300], Step [39/112], Loss: 0.6303\n",
      "Epoch [158/300], Step [43/112], Loss: 0.6275\n",
      "Epoch [158/300], Step [47/112], Loss: 0.6378\n",
      "Epoch [158/300], Step [51/112], Loss: 0.6251\n",
      "Epoch [158/300], Step [55/112], Loss: 0.6325\n",
      "Epoch [158/300], Step [59/112], Loss: 0.6254\n",
      "Epoch [158/300], Step [63/112], Loss: 0.6290\n",
      "Epoch [158/300], Step [67/112], Loss: 0.6427\n",
      "Epoch [158/300], Step [71/112], Loss: 0.6412\n",
      "Epoch [158/300], Step [75/112], Loss: 0.6560\n",
      "Epoch [158/300], Step [79/112], Loss: 0.6399\n",
      "Epoch [158/300], Step [83/112], Loss: 0.6545\n",
      "Epoch [158/300], Step [87/112], Loss: 0.6311\n",
      "Epoch [158/300], Step [91/112], Loss: 0.6619\n",
      "Epoch [158/300], Step [95/112], Loss: 0.6485\n",
      "Epoch [158/300], Step [99/112], Loss: 0.6503\n",
      "Epoch [158/300], Step [103/112], Loss: 0.6447\n",
      "Epoch [158/300], Step [107/112], Loss: 0.6374\n",
      "Epoch [158/300], Step [111/112], Loss: 0.6012\n",
      "Epoch [159/300], Step [3/112], Loss: 0.6431\n",
      "Epoch [159/300], Step [7/112], Loss: 0.6604\n",
      "Epoch [159/300], Step [11/112], Loss: 0.6428\n",
      "Epoch [159/300], Step [15/112], Loss: 0.6428\n",
      "Epoch [159/300], Step [19/112], Loss: 0.6386\n",
      "Epoch [159/300], Step [23/112], Loss: 0.6483\n",
      "Epoch [159/300], Step [27/112], Loss: 0.6148\n",
      "Epoch [159/300], Step [31/112], Loss: 0.6726\n",
      "Epoch [159/300], Step [35/112], Loss: 0.6308\n",
      "Epoch [159/300], Step [39/112], Loss: 0.6356\n",
      "Epoch [159/300], Step [43/112], Loss: 0.6263\n",
      "Epoch [159/300], Step [47/112], Loss: 0.6401\n",
      "Epoch [159/300], Step [51/112], Loss: 0.6332\n",
      "Epoch [159/300], Step [55/112], Loss: 0.6396\n",
      "Epoch [159/300], Step [59/112], Loss: 0.6226\n",
      "Epoch [159/300], Step [63/112], Loss: 0.6430\n",
      "Epoch [159/300], Step [67/112], Loss: 0.6403\n",
      "Epoch [159/300], Step [71/112], Loss: 0.6445\n",
      "Epoch [159/300], Step [75/112], Loss: 0.6550\n",
      "Epoch [159/300], Step [79/112], Loss: 0.6336\n",
      "Epoch [159/300], Step [83/112], Loss: 0.6595\n",
      "Epoch [159/300], Step [87/112], Loss: 0.6343\n",
      "Epoch [159/300], Step [91/112], Loss: 0.6601\n",
      "Epoch [159/300], Step [95/112], Loss: 0.6459\n",
      "Epoch [159/300], Step [99/112], Loss: 0.6452\n",
      "Epoch [159/300], Step [103/112], Loss: 0.6477\n",
      "Epoch [159/300], Step [107/112], Loss: 0.6471\n",
      "Epoch [159/300], Step [111/112], Loss: 0.6033\n",
      "Epoch [160/300], Step [3/112], Loss: 0.6517\n",
      "Epoch [160/300], Step [7/112], Loss: 0.6553\n",
      "Epoch [160/300], Step [11/112], Loss: 0.6449\n",
      "Epoch [160/300], Step [15/112], Loss: 0.6389\n",
      "Epoch [160/300], Step [19/112], Loss: 0.6310\n",
      "Epoch [160/300], Step [23/112], Loss: 0.6323\n",
      "Epoch [160/300], Step [27/112], Loss: 0.6148\n",
      "Epoch [160/300], Step [31/112], Loss: 0.6668\n",
      "Epoch [160/300], Step [35/112], Loss: 0.6226\n",
      "Epoch [160/300], Step [39/112], Loss: 0.6220\n",
      "Epoch [160/300], Step [43/112], Loss: 0.6203\n",
      "Epoch [160/300], Step [47/112], Loss: 0.6394\n",
      "Epoch [160/300], Step [51/112], Loss: 0.6402\n",
      "Epoch [160/300], Step [55/112], Loss: 0.6391\n",
      "Epoch [160/300], Step [59/112], Loss: 0.6204\n",
      "Epoch [160/300], Step [63/112], Loss: 0.6449\n",
      "Epoch [160/300], Step [67/112], Loss: 0.6446\n",
      "Epoch [160/300], Step [71/112], Loss: 0.6397\n",
      "Epoch [160/300], Step [75/112], Loss: 0.6464\n",
      "Epoch [160/300], Step [79/112], Loss: 0.6350\n",
      "Epoch [160/300], Step [83/112], Loss: 0.6481\n",
      "Epoch [160/300], Step [87/112], Loss: 0.6347\n",
      "Epoch [160/300], Step [91/112], Loss: 0.6633\n",
      "Epoch [160/300], Step [95/112], Loss: 0.6452\n",
      "Epoch [160/300], Step [99/112], Loss: 0.6486\n",
      "Epoch [160/300], Step [103/112], Loss: 0.6342\n",
      "Epoch [160/300], Step [107/112], Loss: 0.6474\n",
      "Epoch [160/300], Step [111/112], Loss: 0.6063\n",
      "Epoch [161/300], Step [3/112], Loss: 0.6493\n",
      "Epoch [161/300], Step [7/112], Loss: 0.6459\n",
      "Epoch [161/300], Step [11/112], Loss: 0.6460\n",
      "Epoch [161/300], Step [15/112], Loss: 0.6397\n",
      "Epoch [161/300], Step [19/112], Loss: 0.6340\n",
      "Epoch [161/300], Step [23/112], Loss: 0.6381\n",
      "Epoch [161/300], Step [27/112], Loss: 0.6094\n",
      "Epoch [161/300], Step [31/112], Loss: 0.6687\n",
      "Epoch [161/300], Step [35/112], Loss: 0.6236\n",
      "Epoch [161/300], Step [39/112], Loss: 0.6290\n",
      "Epoch [161/300], Step [43/112], Loss: 0.6205\n",
      "Epoch [161/300], Step [47/112], Loss: 0.6397\n",
      "Epoch [161/300], Step [51/112], Loss: 0.6238\n",
      "Epoch [161/300], Step [55/112], Loss: 0.6390\n",
      "Epoch [161/300], Step [59/112], Loss: 0.6258\n",
      "Epoch [161/300], Step [63/112], Loss: 0.6365\n",
      "Epoch [161/300], Step [67/112], Loss: 0.6319\n",
      "Epoch [161/300], Step [71/112], Loss: 0.6356\n",
      "Epoch [161/300], Step [75/112], Loss: 0.6559\n",
      "Epoch [161/300], Step [79/112], Loss: 0.6337\n",
      "Epoch [161/300], Step [83/112], Loss: 0.6521\n",
      "Epoch [161/300], Step [87/112], Loss: 0.6285\n",
      "Epoch [161/300], Step [91/112], Loss: 0.6555\n",
      "Epoch [161/300], Step [95/112], Loss: 0.6517\n",
      "Epoch [161/300], Step [99/112], Loss: 0.6453\n",
      "Epoch [161/300], Step [103/112], Loss: 0.6419\n",
      "Epoch [161/300], Step [107/112], Loss: 0.6399\n",
      "Epoch [161/300], Step [111/112], Loss: 0.6008\n",
      "Epoch [162/300], Step [3/112], Loss: 0.6487\n",
      "Epoch [162/300], Step [7/112], Loss: 0.6434\n",
      "Epoch [162/300], Step [11/112], Loss: 0.6410\n",
      "Epoch [162/300], Step [15/112], Loss: 0.6374\n",
      "Epoch [162/300], Step [19/112], Loss: 0.6202\n",
      "Epoch [162/300], Step [23/112], Loss: 0.6346\n",
      "Epoch [162/300], Step [27/112], Loss: 0.6138\n",
      "Epoch [162/300], Step [31/112], Loss: 0.6693\n",
      "Epoch [162/300], Step [35/112], Loss: 0.6248\n",
      "Epoch [162/300], Step [39/112], Loss: 0.6266\n",
      "Epoch [162/300], Step [43/112], Loss: 0.6234\n",
      "Epoch [162/300], Step [47/112], Loss: 0.6415\n",
      "Epoch [162/300], Step [51/112], Loss: 0.6196\n",
      "Epoch [162/300], Step [55/112], Loss: 0.6301\n",
      "Epoch [162/300], Step [59/112], Loss: 0.6263\n",
      "Epoch [162/300], Step [63/112], Loss: 0.6306\n",
      "Epoch [162/300], Step [67/112], Loss: 0.6381\n",
      "Epoch [162/300], Step [71/112], Loss: 0.6384\n",
      "Epoch [162/300], Step [75/112], Loss: 0.6547\n",
      "Epoch [162/300], Step [79/112], Loss: 0.6330\n",
      "Epoch [162/300], Step [83/112], Loss: 0.6550\n",
      "Epoch [162/300], Step [87/112], Loss: 0.6358\n",
      "Epoch [162/300], Step [91/112], Loss: 0.6667\n",
      "Epoch [162/300], Step [95/112], Loss: 0.6498\n",
      "Epoch [162/300], Step [99/112], Loss: 0.6460\n",
      "Epoch [162/300], Step [103/112], Loss: 0.6429\n",
      "Epoch [162/300], Step [107/112], Loss: 0.6424\n",
      "Epoch [162/300], Step [111/112], Loss: 0.5989\n",
      "Epoch [163/300], Step [3/112], Loss: 0.6501\n",
      "Epoch [163/300], Step [7/112], Loss: 0.6485\n",
      "Epoch [163/300], Step [11/112], Loss: 0.6386\n",
      "Epoch [163/300], Step [15/112], Loss: 0.6316\n",
      "Epoch [163/300], Step [19/112], Loss: 0.6230\n",
      "Epoch [163/300], Step [23/112], Loss: 0.6352\n",
      "Epoch [163/300], Step [27/112], Loss: 0.6105\n",
      "Epoch [163/300], Step [31/112], Loss: 0.6621\n",
      "Epoch [163/300], Step [35/112], Loss: 0.6365\n",
      "Epoch [163/300], Step [39/112], Loss: 0.6255\n",
      "Epoch [163/300], Step [43/112], Loss: 0.6324\n",
      "Epoch [163/300], Step [47/112], Loss: 0.6431\n",
      "Epoch [163/300], Step [51/112], Loss: 0.6269\n",
      "Epoch [163/300], Step [55/112], Loss: 0.6322\n",
      "Epoch [163/300], Step [59/112], Loss: 0.6271\n",
      "Epoch [163/300], Step [63/112], Loss: 0.6342\n",
      "Epoch [163/300], Step [67/112], Loss: 0.6331\n",
      "Epoch [163/300], Step [71/112], Loss: 0.6333\n",
      "Epoch [163/300], Step [75/112], Loss: 0.6442\n",
      "Epoch [163/300], Step [79/112], Loss: 0.6292\n",
      "Epoch [163/300], Step [83/112], Loss: 0.6531\n",
      "Epoch [163/300], Step [87/112], Loss: 0.6253\n",
      "Epoch [163/300], Step [91/112], Loss: 0.6632\n",
      "Epoch [163/300], Step [95/112], Loss: 0.6456\n",
      "Epoch [163/300], Step [99/112], Loss: 0.6479\n",
      "Epoch [163/300], Step [103/112], Loss: 0.6410\n",
      "Epoch [163/300], Step [107/112], Loss: 0.6356\n",
      "Epoch [163/300], Step [111/112], Loss: 0.6007\n",
      "Epoch [164/300], Step [3/112], Loss: 0.6477\n",
      "Epoch [164/300], Step [7/112], Loss: 0.6473\n",
      "Epoch [164/300], Step [11/112], Loss: 0.6436\n",
      "Epoch [164/300], Step [15/112], Loss: 0.6360\n",
      "Epoch [164/300], Step [19/112], Loss: 0.6315\n",
      "Epoch [164/300], Step [23/112], Loss: 0.6408\n",
      "Epoch [164/300], Step [27/112], Loss: 0.6090\n",
      "Epoch [164/300], Step [31/112], Loss: 0.6644\n",
      "Epoch [164/300], Step [35/112], Loss: 0.6246\n",
      "Epoch [164/300], Step [39/112], Loss: 0.6274\n",
      "Epoch [164/300], Step [43/112], Loss: 0.6265\n",
      "Epoch [164/300], Step [47/112], Loss: 0.6325\n",
      "Epoch [164/300], Step [51/112], Loss: 0.6280\n",
      "Epoch [164/300], Step [55/112], Loss: 0.6383\n",
      "Epoch [164/300], Step [59/112], Loss: 0.6253\n",
      "Epoch [164/300], Step [63/112], Loss: 0.6385\n",
      "Epoch [164/300], Step [67/112], Loss: 0.6342\n",
      "Epoch [164/300], Step [71/112], Loss: 0.6381\n",
      "Epoch [164/300], Step [75/112], Loss: 0.6642\n",
      "Epoch [164/300], Step [79/112], Loss: 0.6306\n",
      "Epoch [164/300], Step [83/112], Loss: 0.6526\n",
      "Epoch [164/300], Step [87/112], Loss: 0.6283\n",
      "Epoch [164/300], Step [91/112], Loss: 0.6582\n",
      "Epoch [164/300], Step [95/112], Loss: 0.6544\n",
      "Epoch [164/300], Step [99/112], Loss: 0.6498\n",
      "Epoch [164/300], Step [103/112], Loss: 0.6453\n",
      "Epoch [164/300], Step [107/112], Loss: 0.6397\n",
      "Epoch [164/300], Step [111/112], Loss: 0.5996\n",
      "Epoch [165/300], Step [3/112], Loss: 0.6530\n",
      "Epoch [165/300], Step [7/112], Loss: 0.6477\n",
      "Epoch [165/300], Step [11/112], Loss: 0.6463\n",
      "Epoch [165/300], Step [15/112], Loss: 0.6362\n",
      "Epoch [165/300], Step [19/112], Loss: 0.6255\n",
      "Epoch [165/300], Step [23/112], Loss: 0.6404\n",
      "Epoch [165/300], Step [27/112], Loss: 0.6121\n",
      "Epoch [165/300], Step [31/112], Loss: 0.6648\n",
      "Epoch [165/300], Step [35/112], Loss: 0.6261\n",
      "Epoch [165/300], Step [39/112], Loss: 0.6271\n",
      "Epoch [165/300], Step [43/112], Loss: 0.6339\n",
      "Epoch [165/300], Step [47/112], Loss: 0.6334\n",
      "Epoch [165/300], Step [51/112], Loss: 0.6183\n",
      "Epoch [165/300], Step [55/112], Loss: 0.6338\n",
      "Epoch [165/300], Step [59/112], Loss: 0.6287\n",
      "Epoch [165/300], Step [63/112], Loss: 0.6303\n",
      "Epoch [165/300], Step [67/112], Loss: 0.6379\n",
      "Epoch [165/300], Step [71/112], Loss: 0.6386\n",
      "Epoch [165/300], Step [75/112], Loss: 0.6455\n",
      "Epoch [165/300], Step [79/112], Loss: 0.6298\n",
      "Epoch [165/300], Step [83/112], Loss: 0.6611\n",
      "Epoch [165/300], Step [87/112], Loss: 0.6363\n",
      "Epoch [165/300], Step [91/112], Loss: 0.6657\n",
      "Epoch [165/300], Step [95/112], Loss: 0.6526\n",
      "Epoch [165/300], Step [99/112], Loss: 0.6502\n",
      "Epoch [165/300], Step [103/112], Loss: 0.6447\n",
      "Epoch [165/300], Step [107/112], Loss: 0.6387\n",
      "Epoch [165/300], Step [111/112], Loss: 0.6038\n",
      "Epoch [166/300], Step [3/112], Loss: 0.6507\n",
      "Epoch [166/300], Step [7/112], Loss: 0.6503\n",
      "Epoch [166/300], Step [11/112], Loss: 0.6414\n",
      "Epoch [166/300], Step [15/112], Loss: 0.6352\n",
      "Epoch [166/300], Step [19/112], Loss: 0.6293\n",
      "Epoch [166/300], Step [23/112], Loss: 0.6401\n",
      "Epoch [166/300], Step [27/112], Loss: 0.6136\n",
      "Epoch [166/300], Step [31/112], Loss: 0.6684\n",
      "Epoch [166/300], Step [35/112], Loss: 0.6214\n",
      "Epoch [166/300], Step [39/112], Loss: 0.6280\n",
      "Epoch [166/300], Step [43/112], Loss: 0.6274\n",
      "Epoch [166/300], Step [47/112], Loss: 0.6280\n",
      "Epoch [166/300], Step [51/112], Loss: 0.6240\n",
      "Epoch [166/300], Step [55/112], Loss: 0.6367\n",
      "Epoch [166/300], Step [59/112], Loss: 0.6273\n",
      "Epoch [166/300], Step [63/112], Loss: 0.6319\n",
      "Epoch [166/300], Step [67/112], Loss: 0.6334\n",
      "Epoch [166/300], Step [71/112], Loss: 0.6344\n",
      "Epoch [166/300], Step [75/112], Loss: 0.6430\n",
      "Epoch [166/300], Step [79/112], Loss: 0.6311\n",
      "Epoch [166/300], Step [83/112], Loss: 0.6539\n",
      "Epoch [166/300], Step [87/112], Loss: 0.6261\n",
      "Epoch [166/300], Step [91/112], Loss: 0.6603\n",
      "Epoch [166/300], Step [95/112], Loss: 0.6448\n",
      "Epoch [166/300], Step [99/112], Loss: 0.6507\n",
      "Epoch [166/300], Step [103/112], Loss: 0.6430\n",
      "Epoch [166/300], Step [107/112], Loss: 0.6371\n",
      "Epoch [166/300], Step [111/112], Loss: 0.5987\n",
      "Epoch [167/300], Step [3/112], Loss: 0.6426\n",
      "Epoch [167/300], Step [7/112], Loss: 0.6454\n",
      "Epoch [167/300], Step [11/112], Loss: 0.6540\n",
      "Epoch [167/300], Step [15/112], Loss: 0.6358\n",
      "Epoch [167/300], Step [19/112], Loss: 0.6284\n",
      "Epoch [167/300], Step [23/112], Loss: 0.6394\n",
      "Epoch [167/300], Step [27/112], Loss: 0.6086\n",
      "Epoch [167/300], Step [31/112], Loss: 0.6719\n",
      "Epoch [167/300], Step [35/112], Loss: 0.6238\n",
      "Epoch [167/300], Step [39/112], Loss: 0.6279\n",
      "Epoch [167/300], Step [43/112], Loss: 0.6369\n",
      "Epoch [167/300], Step [47/112], Loss: 0.6376\n",
      "Epoch [167/300], Step [51/112], Loss: 0.6224\n",
      "Epoch [167/300], Step [55/112], Loss: 0.6301\n",
      "Epoch [167/300], Step [59/112], Loss: 0.6203\n",
      "Epoch [167/300], Step [63/112], Loss: 0.6295\n",
      "Epoch [167/300], Step [67/112], Loss: 0.6401\n",
      "Epoch [167/300], Step [71/112], Loss: 0.6378\n",
      "Epoch [167/300], Step [75/112], Loss: 0.6518\n",
      "Epoch [167/300], Step [79/112], Loss: 0.6325\n",
      "Epoch [167/300], Step [83/112], Loss: 0.6580\n",
      "Epoch [167/300], Step [87/112], Loss: 0.6265\n",
      "Epoch [167/300], Step [91/112], Loss: 0.6645\n",
      "Epoch [167/300], Step [95/112], Loss: 0.6504\n",
      "Epoch [167/300], Step [99/112], Loss: 0.6520\n",
      "Epoch [167/300], Step [103/112], Loss: 0.6445\n",
      "Epoch [167/300], Step [107/112], Loss: 0.6346\n",
      "Epoch [167/300], Step [111/112], Loss: 0.5937\n",
      "Epoch [168/300], Step [3/112], Loss: 0.6431\n",
      "Epoch [168/300], Step [7/112], Loss: 0.6491\n",
      "Epoch [168/300], Step [11/112], Loss: 0.6431\n",
      "Epoch [168/300], Step [15/112], Loss: 0.6331\n",
      "Epoch [168/300], Step [19/112], Loss: 0.6189\n",
      "Epoch [168/300], Step [23/112], Loss: 0.6304\n",
      "Epoch [168/300], Step [27/112], Loss: 0.6115\n",
      "Epoch [168/300], Step [31/112], Loss: 0.6694\n",
      "Epoch [168/300], Step [35/112], Loss: 0.6307\n",
      "Epoch [168/300], Step [39/112], Loss: 0.6325\n",
      "Epoch [168/300], Step [43/112], Loss: 0.6317\n",
      "Epoch [168/300], Step [47/112], Loss: 0.6335\n",
      "Epoch [168/300], Step [51/112], Loss: 0.6253\n",
      "Epoch [168/300], Step [55/112], Loss: 0.6331\n",
      "Epoch [168/300], Step [59/112], Loss: 0.6320\n",
      "Epoch [168/300], Step [63/112], Loss: 0.6308\n",
      "Epoch [168/300], Step [67/112], Loss: 0.6365\n",
      "Epoch [168/300], Step [71/112], Loss: 0.6387\n",
      "Epoch [168/300], Step [75/112], Loss: 0.6476\n",
      "Epoch [168/300], Step [79/112], Loss: 0.6320\n",
      "Epoch [168/300], Step [83/112], Loss: 0.6592\n",
      "Epoch [168/300], Step [87/112], Loss: 0.6359\n",
      "Epoch [168/300], Step [91/112], Loss: 0.6565\n",
      "Epoch [168/300], Step [95/112], Loss: 0.6411\n",
      "Epoch [168/300], Step [99/112], Loss: 0.6417\n",
      "Epoch [168/300], Step [103/112], Loss: 0.6385\n",
      "Epoch [168/300], Step [107/112], Loss: 0.6345\n",
      "Epoch [168/300], Step [111/112], Loss: 0.6028\n",
      "Epoch [169/300], Step [3/112], Loss: 0.6413\n",
      "Epoch [169/300], Step [7/112], Loss: 0.6435\n",
      "Epoch [169/300], Step [11/112], Loss: 0.6461\n",
      "Epoch [169/300], Step [15/112], Loss: 0.6360\n",
      "Epoch [169/300], Step [19/112], Loss: 0.6364\n",
      "Epoch [169/300], Step [23/112], Loss: 0.6378\n",
      "Epoch [169/300], Step [27/112], Loss: 0.6072\n",
      "Epoch [169/300], Step [31/112], Loss: 0.6699\n",
      "Epoch [169/300], Step [35/112], Loss: 0.6243\n",
      "Epoch [169/300], Step [39/112], Loss: 0.6301\n",
      "Epoch [169/300], Step [43/112], Loss: 0.6213\n",
      "Epoch [169/300], Step [47/112], Loss: 0.6355\n",
      "Epoch [169/300], Step [51/112], Loss: 0.6259\n",
      "Epoch [169/300], Step [55/112], Loss: 0.6321\n",
      "Epoch [169/300], Step [59/112], Loss: 0.6265\n",
      "Epoch [169/300], Step [63/112], Loss: 0.6373\n",
      "Epoch [169/300], Step [67/112], Loss: 0.6380\n",
      "Epoch [169/300], Step [71/112], Loss: 0.6369\n",
      "Epoch [169/300], Step [75/112], Loss: 0.6524\n",
      "Epoch [169/300], Step [79/112], Loss: 0.6335\n",
      "Epoch [169/300], Step [83/112], Loss: 0.6552\n",
      "Epoch [169/300], Step [87/112], Loss: 0.6334\n",
      "Epoch [169/300], Step [91/112], Loss: 0.6605\n",
      "Epoch [169/300], Step [95/112], Loss: 0.6453\n",
      "Epoch [169/300], Step [99/112], Loss: 0.6486\n",
      "Epoch [169/300], Step [103/112], Loss: 0.6443\n",
      "Epoch [169/300], Step [107/112], Loss: 0.6407\n",
      "Epoch [169/300], Step [111/112], Loss: 0.6002\n",
      "Epoch [170/300], Step [3/112], Loss: 0.6504\n",
      "Epoch [170/300], Step [7/112], Loss: 0.6504\n",
      "Epoch [170/300], Step [11/112], Loss: 0.6398\n",
      "Epoch [170/300], Step [15/112], Loss: 0.6324\n",
      "Epoch [170/300], Step [19/112], Loss: 0.6299\n",
      "Epoch [170/300], Step [23/112], Loss: 0.6411\n",
      "Epoch [170/300], Step [27/112], Loss: 0.6134\n",
      "Epoch [170/300], Step [31/112], Loss: 0.6689\n",
      "Epoch [170/300], Step [35/112], Loss: 0.6359\n",
      "Epoch [170/300], Step [39/112], Loss: 0.6396\n",
      "Epoch [170/300], Step [43/112], Loss: 0.6307\n",
      "Epoch [170/300], Step [47/112], Loss: 0.6337\n",
      "Epoch [170/300], Step [51/112], Loss: 0.6338\n",
      "Epoch [170/300], Step [55/112], Loss: 0.6290\n",
      "Epoch [170/300], Step [59/112], Loss: 0.6255\n",
      "Epoch [170/300], Step [63/112], Loss: 0.6321\n",
      "Epoch [170/300], Step [67/112], Loss: 0.6451\n",
      "Epoch [170/300], Step [71/112], Loss: 0.6472\n",
      "Epoch [170/300], Step [75/112], Loss: 0.6551\n",
      "Epoch [170/300], Step [79/112], Loss: 0.6362\n",
      "Epoch [170/300], Step [83/112], Loss: 0.6585\n",
      "Epoch [170/300], Step [87/112], Loss: 0.6379\n",
      "Epoch [170/300], Step [91/112], Loss: 0.6693\n",
      "Epoch [170/300], Step [95/112], Loss: 0.6532\n",
      "Epoch [170/300], Step [99/112], Loss: 0.6498\n",
      "Epoch [170/300], Step [103/112], Loss: 0.6464\n",
      "Epoch [170/300], Step [107/112], Loss: 0.6506\n",
      "Epoch [170/300], Step [111/112], Loss: 0.5959\n",
      "Epoch [171/300], Step [3/112], Loss: 0.6520\n",
      "Epoch [171/300], Step [7/112], Loss: 0.6505\n",
      "Epoch [171/300], Step [11/112], Loss: 0.6432\n",
      "Epoch [171/300], Step [15/112], Loss: 0.6399\n",
      "Epoch [171/300], Step [19/112], Loss: 0.6324\n",
      "Epoch [171/300], Step [23/112], Loss: 0.6414\n",
      "Epoch [171/300], Step [27/112], Loss: 0.6155\n",
      "Epoch [171/300], Step [31/112], Loss: 0.6701\n",
      "Epoch [171/300], Step [35/112], Loss: 0.6314\n",
      "Epoch [171/300], Step [39/112], Loss: 0.6331\n",
      "Epoch [171/300], Step [43/112], Loss: 0.6250\n",
      "Epoch [171/300], Step [47/112], Loss: 0.6310\n",
      "Epoch [171/300], Step [51/112], Loss: 0.6257\n",
      "Epoch [171/300], Step [55/112], Loss: 0.6361\n",
      "Epoch [171/300], Step [59/112], Loss: 0.6282\n",
      "Epoch [171/300], Step [63/112], Loss: 0.6300\n",
      "Epoch [171/300], Step [67/112], Loss: 0.6386\n",
      "Epoch [171/300], Step [71/112], Loss: 0.6396\n",
      "Epoch [171/300], Step [75/112], Loss: 0.6513\n",
      "Epoch [171/300], Step [79/112], Loss: 0.6296\n",
      "Epoch [171/300], Step [83/112], Loss: 0.6590\n",
      "Epoch [171/300], Step [87/112], Loss: 0.6354\n",
      "Epoch [171/300], Step [91/112], Loss: 0.6649\n",
      "Epoch [171/300], Step [95/112], Loss: 0.6430\n",
      "Epoch [171/300], Step [99/112], Loss: 0.6521\n",
      "Epoch [171/300], Step [103/112], Loss: 0.6470\n",
      "Epoch [171/300], Step [107/112], Loss: 0.6387\n",
      "Epoch [171/300], Step [111/112], Loss: 0.6006\n",
      "Epoch [172/300], Step [3/112], Loss: 0.6484\n",
      "Epoch [172/300], Step [7/112], Loss: 0.6475\n",
      "Epoch [172/300], Step [11/112], Loss: 0.6463\n",
      "Epoch [172/300], Step [15/112], Loss: 0.6377\n",
      "Epoch [172/300], Step [19/112], Loss: 0.6316\n",
      "Epoch [172/300], Step [23/112], Loss: 0.6291\n",
      "Epoch [172/300], Step [27/112], Loss: 0.6050\n",
      "Epoch [172/300], Step [31/112], Loss: 0.6613\n",
      "Epoch [172/300], Step [35/112], Loss: 0.6302\n",
      "Epoch [172/300], Step [39/112], Loss: 0.6258\n",
      "Epoch [172/300], Step [43/112], Loss: 0.6217\n",
      "Epoch [172/300], Step [47/112], Loss: 0.6342\n",
      "Epoch [172/300], Step [51/112], Loss: 0.6192\n",
      "Epoch [172/300], Step [55/112], Loss: 0.6316\n",
      "Epoch [172/300], Step [59/112], Loss: 0.6188\n",
      "Epoch [172/300], Step [63/112], Loss: 0.6323\n",
      "Epoch [172/300], Step [67/112], Loss: 0.6438\n",
      "Epoch [172/300], Step [71/112], Loss: 0.6435\n",
      "Epoch [172/300], Step [75/112], Loss: 0.6503\n",
      "Epoch [172/300], Step [79/112], Loss: 0.6349\n",
      "Epoch [172/300], Step [83/112], Loss: 0.6585\n",
      "Epoch [172/300], Step [87/112], Loss: 0.6300\n",
      "Epoch [172/300], Step [91/112], Loss: 0.6583\n",
      "Epoch [172/300], Step [95/112], Loss: 0.6492\n",
      "Epoch [172/300], Step [99/112], Loss: 0.6472\n",
      "Epoch [172/300], Step [103/112], Loss: 0.6435\n",
      "Epoch [172/300], Step [107/112], Loss: 0.6384\n",
      "Epoch [172/300], Step [111/112], Loss: 0.6076\n",
      "Epoch [173/300], Step [3/112], Loss: 0.6505\n",
      "Epoch [173/300], Step [7/112], Loss: 0.6522\n",
      "Epoch [173/300], Step [11/112], Loss: 0.6436\n",
      "Epoch [173/300], Step [15/112], Loss: 0.6325\n",
      "Epoch [173/300], Step [19/112], Loss: 0.6298\n",
      "Epoch [173/300], Step [23/112], Loss: 0.6355\n",
      "Epoch [173/300], Step [27/112], Loss: 0.6130\n",
      "Epoch [173/300], Step [31/112], Loss: 0.6606\n",
      "Epoch [173/300], Step [35/112], Loss: 0.6273\n",
      "Epoch [173/300], Step [39/112], Loss: 0.6227\n",
      "Epoch [173/300], Step [43/112], Loss: 0.6267\n",
      "Epoch [173/300], Step [47/112], Loss: 0.6296\n",
      "Epoch [173/300], Step [51/112], Loss: 0.6225\n",
      "Epoch [173/300], Step [55/112], Loss: 0.6309\n",
      "Epoch [173/300], Step [59/112], Loss: 0.6214\n",
      "Epoch [173/300], Step [63/112], Loss: 0.6349\n",
      "Epoch [173/300], Step [67/112], Loss: 0.6390\n",
      "Epoch [173/300], Step [71/112], Loss: 0.6361\n",
      "Epoch [173/300], Step [75/112], Loss: 0.6444\n",
      "Epoch [173/300], Step [79/112], Loss: 0.6266\n",
      "Epoch [173/300], Step [83/112], Loss: 0.6624\n",
      "Epoch [173/300], Step [87/112], Loss: 0.6303\n",
      "Epoch [173/300], Step [91/112], Loss: 0.6760\n",
      "Epoch [173/300], Step [95/112], Loss: 0.6520\n",
      "Epoch [173/300], Step [99/112], Loss: 0.6535\n",
      "Epoch [173/300], Step [103/112], Loss: 0.6412\n",
      "Epoch [173/300], Step [107/112], Loss: 0.6347\n",
      "Epoch [173/300], Step [111/112], Loss: 0.5906\n",
      "Epoch [174/300], Step [3/112], Loss: 0.6441\n",
      "Epoch [174/300], Step [7/112], Loss: 0.6507\n",
      "Epoch [174/300], Step [11/112], Loss: 0.6440\n",
      "Epoch [174/300], Step [15/112], Loss: 0.6322\n",
      "Epoch [174/300], Step [19/112], Loss: 0.6318\n",
      "Epoch [174/300], Step [23/112], Loss: 0.6396\n",
      "Epoch [174/300], Step [27/112], Loss: 0.6084\n",
      "Epoch [174/300], Step [31/112], Loss: 0.6648\n",
      "Epoch [174/300], Step [35/112], Loss: 0.6274\n",
      "Epoch [174/300], Step [39/112], Loss: 0.6287\n",
      "Epoch [174/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [174/300], Step [47/112], Loss: 0.6292\n",
      "Epoch [174/300], Step [51/112], Loss: 0.6193\n",
      "Epoch [174/300], Step [55/112], Loss: 0.6365\n",
      "Epoch [174/300], Step [59/112], Loss: 0.6178\n",
      "Epoch [174/300], Step [63/112], Loss: 0.6344\n",
      "Epoch [174/300], Step [67/112], Loss: 0.6339\n",
      "Epoch [174/300], Step [71/112], Loss: 0.6410\n",
      "Epoch [174/300], Step [75/112], Loss: 0.6494\n",
      "Epoch [174/300], Step [79/112], Loss: 0.6378\n",
      "Epoch [174/300], Step [83/112], Loss: 0.6684\n",
      "Epoch [174/300], Step [87/112], Loss: 0.6321\n",
      "Epoch [174/300], Step [91/112], Loss: 0.6654\n",
      "Epoch [174/300], Step [95/112], Loss: 0.6563\n",
      "Epoch [174/300], Step [99/112], Loss: 0.6565\n",
      "Epoch [174/300], Step [103/112], Loss: 0.6408\n",
      "Epoch [174/300], Step [107/112], Loss: 0.6360\n",
      "Epoch [174/300], Step [111/112], Loss: 0.6044\n",
      "Epoch [175/300], Step [3/112], Loss: 0.6376\n",
      "Epoch [175/300], Step [7/112], Loss: 0.6423\n",
      "Epoch [175/300], Step [11/112], Loss: 0.6457\n",
      "Epoch [175/300], Step [15/112], Loss: 0.6404\n",
      "Epoch [175/300], Step [19/112], Loss: 0.6317\n",
      "Epoch [175/300], Step [23/112], Loss: 0.6407\n",
      "Epoch [175/300], Step [27/112], Loss: 0.6082\n",
      "Epoch [175/300], Step [31/112], Loss: 0.6703\n",
      "Epoch [175/300], Step [35/112], Loss: 0.6294\n",
      "Epoch [175/300], Step [39/112], Loss: 0.6350\n",
      "Epoch [175/300], Step [43/112], Loss: 0.6254\n",
      "Epoch [175/300], Step [47/112], Loss: 0.6307\n",
      "Epoch [175/300], Step [51/112], Loss: 0.6214\n",
      "Epoch [175/300], Step [55/112], Loss: 0.6380\n",
      "Epoch [175/300], Step [59/112], Loss: 0.6262\n",
      "Epoch [175/300], Step [63/112], Loss: 0.6302\n",
      "Epoch [175/300], Step [67/112], Loss: 0.6443\n",
      "Epoch [175/300], Step [71/112], Loss: 0.6428\n",
      "Epoch [175/300], Step [75/112], Loss: 0.6557\n",
      "Epoch [175/300], Step [79/112], Loss: 0.6258\n",
      "Epoch [175/300], Step [83/112], Loss: 0.6601\n",
      "Epoch [175/300], Step [87/112], Loss: 0.6263\n",
      "Epoch [175/300], Step [91/112], Loss: 0.6600\n",
      "Epoch [175/300], Step [95/112], Loss: 0.6431\n",
      "Epoch [175/300], Step [99/112], Loss: 0.6481\n",
      "Epoch [175/300], Step [103/112], Loss: 0.6417\n",
      "Epoch [175/300], Step [107/112], Loss: 0.6358\n",
      "Epoch [175/300], Step [111/112], Loss: 0.5983\n",
      "Epoch [176/300], Step [3/112], Loss: 0.6338\n",
      "Epoch [176/300], Step [7/112], Loss: 0.6457\n",
      "Epoch [176/300], Step [11/112], Loss: 0.6327\n",
      "Epoch [176/300], Step [15/112], Loss: 0.6335\n",
      "Epoch [176/300], Step [19/112], Loss: 0.6251\n",
      "Epoch [176/300], Step [23/112], Loss: 0.6295\n",
      "Epoch [176/300], Step [27/112], Loss: 0.6041\n",
      "Epoch [176/300], Step [31/112], Loss: 0.6541\n",
      "Epoch [176/300], Step [35/112], Loss: 0.6186\n",
      "Epoch [176/300], Step [39/112], Loss: 0.6263\n",
      "Epoch [176/300], Step [43/112], Loss: 0.6214\n",
      "Epoch [176/300], Step [47/112], Loss: 0.6332\n",
      "Epoch [176/300], Step [51/112], Loss: 0.6196\n",
      "Epoch [176/300], Step [55/112], Loss: 0.6271\n",
      "Epoch [176/300], Step [59/112], Loss: 0.6260\n",
      "Epoch [176/300], Step [63/112], Loss: 0.6284\n",
      "Epoch [176/300], Step [67/112], Loss: 0.6357\n",
      "Epoch [176/300], Step [71/112], Loss: 0.6304\n",
      "Epoch [176/300], Step [75/112], Loss: 0.6401\n",
      "Epoch [176/300], Step [79/112], Loss: 0.6239\n",
      "Epoch [176/300], Step [83/112], Loss: 0.6589\n",
      "Epoch [176/300], Step [87/112], Loss: 0.6244\n",
      "Epoch [176/300], Step [91/112], Loss: 0.6566\n",
      "Epoch [176/300], Step [95/112], Loss: 0.6492\n",
      "Epoch [176/300], Step [99/112], Loss: 0.6487\n",
      "Epoch [176/300], Step [103/112], Loss: 0.6442\n",
      "Epoch [176/300], Step [107/112], Loss: 0.6319\n",
      "Epoch [176/300], Step [111/112], Loss: 0.5926\n",
      "Epoch [177/300], Step [3/112], Loss: 0.6436\n",
      "Epoch [177/300], Step [7/112], Loss: 0.6496\n",
      "Epoch [177/300], Step [11/112], Loss: 0.6378\n",
      "Epoch [177/300], Step [15/112], Loss: 0.6334\n",
      "Epoch [177/300], Step [19/112], Loss: 0.6222\n",
      "Epoch [177/300], Step [23/112], Loss: 0.6283\n",
      "Epoch [177/300], Step [27/112], Loss: 0.6022\n",
      "Epoch [177/300], Step [31/112], Loss: 0.6635\n",
      "Epoch [177/300], Step [35/112], Loss: 0.6247\n",
      "Epoch [177/300], Step [39/112], Loss: 0.6259\n",
      "Epoch [177/300], Step [43/112], Loss: 0.6218\n",
      "Epoch [177/300], Step [47/112], Loss: 0.6294\n",
      "Epoch [177/300], Step [51/112], Loss: 0.6212\n",
      "Epoch [177/300], Step [55/112], Loss: 0.6306\n",
      "Epoch [177/300], Step [59/112], Loss: 0.6168\n",
      "Epoch [177/300], Step [63/112], Loss: 0.6356\n",
      "Epoch [177/300], Step [67/112], Loss: 0.6307\n",
      "Epoch [177/300], Step [71/112], Loss: 0.6289\n",
      "Epoch [177/300], Step [75/112], Loss: 0.6470\n",
      "Epoch [177/300], Step [79/112], Loss: 0.6282\n",
      "Epoch [177/300], Step [83/112], Loss: 0.6583\n",
      "Epoch [177/300], Step [87/112], Loss: 0.6209\n",
      "Epoch [177/300], Step [91/112], Loss: 0.6638\n",
      "Epoch [177/300], Step [95/112], Loss: 0.6457\n",
      "Epoch [177/300], Step [99/112], Loss: 0.6504\n",
      "Epoch [177/300], Step [103/112], Loss: 0.6352\n",
      "Epoch [177/300], Step [107/112], Loss: 0.6325\n",
      "Epoch [177/300], Step [111/112], Loss: 0.6030\n",
      "Epoch [178/300], Step [3/112], Loss: 0.6422\n",
      "Epoch [178/300], Step [7/112], Loss: 0.6471\n",
      "Epoch [178/300], Step [11/112], Loss: 0.6386\n",
      "Epoch [178/300], Step [15/112], Loss: 0.6417\n",
      "Epoch [178/300], Step [19/112], Loss: 0.6283\n",
      "Epoch [178/300], Step [23/112], Loss: 0.6344\n",
      "Epoch [178/300], Step [27/112], Loss: 0.6031\n",
      "Epoch [178/300], Step [31/112], Loss: 0.6580\n",
      "Epoch [178/300], Step [35/112], Loss: 0.6279\n",
      "Epoch [178/300], Step [39/112], Loss: 0.6183\n",
      "Epoch [178/300], Step [43/112], Loss: 0.6238\n",
      "Epoch [178/300], Step [47/112], Loss: 0.6309\n",
      "Epoch [178/300], Step [51/112], Loss: 0.6172\n",
      "Epoch [178/300], Step [55/112], Loss: 0.6302\n",
      "Epoch [178/300], Step [59/112], Loss: 0.6144\n",
      "Epoch [178/300], Step [63/112], Loss: 0.6300\n",
      "Epoch [178/300], Step [67/112], Loss: 0.6316\n",
      "Epoch [178/300], Step [71/112], Loss: 0.6353\n",
      "Epoch [178/300], Step [75/112], Loss: 0.6467\n",
      "Epoch [178/300], Step [79/112], Loss: 0.6323\n",
      "Epoch [178/300], Step [83/112], Loss: 0.6617\n",
      "Epoch [178/300], Step [87/112], Loss: 0.6336\n",
      "Epoch [178/300], Step [91/112], Loss: 0.6618\n",
      "Epoch [178/300], Step [95/112], Loss: 0.6569\n",
      "Epoch [178/300], Step [99/112], Loss: 0.6467\n",
      "Epoch [178/300], Step [103/112], Loss: 0.6477\n",
      "Epoch [178/300], Step [107/112], Loss: 0.6353\n",
      "Epoch [178/300], Step [111/112], Loss: 0.5950\n",
      "Epoch [179/300], Step [3/112], Loss: 0.6409\n",
      "Epoch [179/300], Step [7/112], Loss: 0.6496\n",
      "Epoch [179/300], Step [11/112], Loss: 0.6373\n",
      "Epoch [179/300], Step [15/112], Loss: 0.6310\n",
      "Epoch [179/300], Step [19/112], Loss: 0.6289\n",
      "Epoch [179/300], Step [23/112], Loss: 0.6263\n",
      "Epoch [179/300], Step [27/112], Loss: 0.6107\n",
      "Epoch [179/300], Step [31/112], Loss: 0.6588\n",
      "Epoch [179/300], Step [35/112], Loss: 0.6278\n",
      "Epoch [179/300], Step [39/112], Loss: 0.6251\n",
      "Epoch [179/300], Step [43/112], Loss: 0.6292\n",
      "Epoch [179/300], Step [47/112], Loss: 0.6288\n",
      "Epoch [179/300], Step [51/112], Loss: 0.6258\n",
      "Epoch [179/300], Step [55/112], Loss: 0.6330\n",
      "Epoch [179/300], Step [59/112], Loss: 0.6267\n",
      "Epoch [179/300], Step [63/112], Loss: 0.6280\n",
      "Epoch [179/300], Step [67/112], Loss: 0.6319\n",
      "Epoch [179/300], Step [71/112], Loss: 0.6423\n",
      "Epoch [179/300], Step [75/112], Loss: 0.6495\n",
      "Epoch [179/300], Step [79/112], Loss: 0.6286\n",
      "Epoch [179/300], Step [83/112], Loss: 0.6597\n",
      "Epoch [179/300], Step [87/112], Loss: 0.6364\n",
      "Epoch [179/300], Step [91/112], Loss: 0.6620\n",
      "Epoch [179/300], Step [95/112], Loss: 0.6518\n",
      "Epoch [179/300], Step [99/112], Loss: 0.6522\n",
      "Epoch [179/300], Step [103/112], Loss: 0.6445\n",
      "Epoch [179/300], Step [107/112], Loss: 0.6377\n",
      "Epoch [179/300], Step [111/112], Loss: 0.6030\n",
      "Epoch [180/300], Step [3/112], Loss: 0.6494\n",
      "Epoch [180/300], Step [7/112], Loss: 0.6528\n",
      "Epoch [180/300], Step [11/112], Loss: 0.6385\n",
      "Epoch [180/300], Step [15/112], Loss: 0.6366\n",
      "Epoch [180/300], Step [19/112], Loss: 0.6350\n",
      "Epoch [180/300], Step [23/112], Loss: 0.6408\n",
      "Epoch [180/300], Step [27/112], Loss: 0.6061\n",
      "Epoch [180/300], Step [31/112], Loss: 0.6599\n",
      "Epoch [180/300], Step [35/112], Loss: 0.6264\n",
      "Epoch [180/300], Step [39/112], Loss: 0.6317\n",
      "Epoch [180/300], Step [43/112], Loss: 0.6268\n",
      "Epoch [180/300], Step [47/112], Loss: 0.6350\n",
      "Epoch [180/300], Step [51/112], Loss: 0.6207\n",
      "Epoch [180/300], Step [55/112], Loss: 0.6345\n",
      "Epoch [180/300], Step [59/112], Loss: 0.6210\n",
      "Epoch [180/300], Step [63/112], Loss: 0.6246\n",
      "Epoch [180/300], Step [67/112], Loss: 0.6348\n",
      "Epoch [180/300], Step [71/112], Loss: 0.6358\n",
      "Epoch [180/300], Step [75/112], Loss: 0.6574\n",
      "Epoch [180/300], Step [79/112], Loss: 0.6393\n",
      "Epoch [180/300], Step [83/112], Loss: 0.6701\n",
      "Epoch [180/300], Step [87/112], Loss: 0.6369\n",
      "Epoch [180/300], Step [91/112], Loss: 0.6707\n",
      "Epoch [180/300], Step [95/112], Loss: 0.6519\n",
      "Epoch [180/300], Step [99/112], Loss: 0.6601\n",
      "Epoch [180/300], Step [103/112], Loss: 0.6431\n",
      "Epoch [180/300], Step [107/112], Loss: 0.6429\n",
      "Epoch [180/300], Step [111/112], Loss: 0.5990\n",
      "Epoch [181/300], Step [3/112], Loss: 0.6487\n",
      "Epoch [181/300], Step [7/112], Loss: 0.6463\n",
      "Epoch [181/300], Step [11/112], Loss: 0.6412\n",
      "Epoch [181/300], Step [15/112], Loss: 0.6348\n",
      "Epoch [181/300], Step [19/112], Loss: 0.6293\n",
      "Epoch [181/300], Step [23/112], Loss: 0.6397\n",
      "Epoch [181/300], Step [27/112], Loss: 0.6131\n",
      "Epoch [181/300], Step [31/112], Loss: 0.6657\n",
      "Epoch [181/300], Step [35/112], Loss: 0.6269\n",
      "Epoch [181/300], Step [39/112], Loss: 0.6233\n",
      "Epoch [181/300], Step [43/112], Loss: 0.6264\n",
      "Epoch [181/300], Step [47/112], Loss: 0.6274\n",
      "Epoch [181/300], Step [51/112], Loss: 0.6236\n",
      "Epoch [181/300], Step [55/112], Loss: 0.6279\n",
      "Epoch [181/300], Step [59/112], Loss: 0.6241\n",
      "Epoch [181/300], Step [63/112], Loss: 0.6328\n",
      "Epoch [181/300], Step [67/112], Loss: 0.6395\n",
      "Epoch [181/300], Step [71/112], Loss: 0.6308\n",
      "Epoch [181/300], Step [75/112], Loss: 0.6492\n",
      "Epoch [181/300], Step [79/112], Loss: 0.6297\n",
      "Epoch [181/300], Step [83/112], Loss: 0.6577\n",
      "Epoch [181/300], Step [87/112], Loss: 0.6309\n",
      "Epoch [181/300], Step [91/112], Loss: 0.6768\n",
      "Epoch [181/300], Step [95/112], Loss: 0.6575\n",
      "Epoch [181/300], Step [99/112], Loss: 0.6608\n",
      "Epoch [181/300], Step [103/112], Loss: 0.6509\n",
      "Epoch [181/300], Step [107/112], Loss: 0.6434\n",
      "Epoch [181/300], Step [111/112], Loss: 0.5984\n",
      "Epoch [182/300], Step [3/112], Loss: 0.6552\n",
      "Epoch [182/300], Step [7/112], Loss: 0.6599\n",
      "Epoch [182/300], Step [11/112], Loss: 0.6587\n",
      "Epoch [182/300], Step [15/112], Loss: 0.6422\n",
      "Epoch [182/300], Step [19/112], Loss: 0.6273\n",
      "Epoch [182/300], Step [23/112], Loss: 0.6378\n",
      "Epoch [182/300], Step [27/112], Loss: 0.6125\n",
      "Epoch [182/300], Step [31/112], Loss: 0.6696\n",
      "Epoch [182/300], Step [35/112], Loss: 0.6311\n",
      "Epoch [182/300], Step [39/112], Loss: 0.6398\n",
      "Epoch [182/300], Step [43/112], Loss: 0.6268\n",
      "Epoch [182/300], Step [47/112], Loss: 0.6309\n",
      "Epoch [182/300], Step [51/112], Loss: 0.6162\n",
      "Epoch [182/300], Step [55/112], Loss: 0.6366\n",
      "Epoch [182/300], Step [59/112], Loss: 0.6246\n",
      "Epoch [182/300], Step [63/112], Loss: 0.6415\n",
      "Epoch [182/300], Step [67/112], Loss: 0.6392\n",
      "Epoch [182/300], Step [71/112], Loss: 0.6383\n",
      "Epoch [182/300], Step [75/112], Loss: 0.6424\n",
      "Epoch [182/300], Step [79/112], Loss: 0.6361\n",
      "Epoch [182/300], Step [83/112], Loss: 0.6601\n",
      "Epoch [182/300], Step [87/112], Loss: 0.6336\n",
      "Epoch [182/300], Step [91/112], Loss: 0.6656\n",
      "Epoch [182/300], Step [95/112], Loss: 0.6445\n",
      "Epoch [182/300], Step [99/112], Loss: 0.6529\n",
      "Epoch [182/300], Step [103/112], Loss: 0.6464\n",
      "Epoch [182/300], Step [107/112], Loss: 0.6433\n",
      "Epoch [182/300], Step [111/112], Loss: 0.5967\n",
      "Epoch [183/300], Step [3/112], Loss: 0.6412\n",
      "Epoch [183/300], Step [7/112], Loss: 0.6493\n",
      "Epoch [183/300], Step [11/112], Loss: 0.6440\n",
      "Epoch [183/300], Step [15/112], Loss: 0.6448\n",
      "Epoch [183/300], Step [19/112], Loss: 0.6322\n",
      "Epoch [183/300], Step [23/112], Loss: 0.6330\n",
      "Epoch [183/300], Step [27/112], Loss: 0.6155\n",
      "Epoch [183/300], Step [31/112], Loss: 0.6592\n",
      "Epoch [183/300], Step [35/112], Loss: 0.6213\n",
      "Epoch [183/300], Step [39/112], Loss: 0.6253\n",
      "Epoch [183/300], Step [43/112], Loss: 0.6351\n",
      "Epoch [183/300], Step [47/112], Loss: 0.6268\n",
      "Epoch [183/300], Step [51/112], Loss: 0.6228\n",
      "Epoch [183/300], Step [55/112], Loss: 0.6423\n",
      "Epoch [183/300], Step [59/112], Loss: 0.6152\n",
      "Epoch [183/300], Step [63/112], Loss: 0.6302\n",
      "Epoch [183/300], Step [67/112], Loss: 0.6339\n",
      "Epoch [183/300], Step [71/112], Loss: 0.6400\n",
      "Epoch [183/300], Step [75/112], Loss: 0.6528\n",
      "Epoch [183/300], Step [79/112], Loss: 0.6322\n",
      "Epoch [183/300], Step [83/112], Loss: 0.6540\n",
      "Epoch [183/300], Step [87/112], Loss: 0.6311\n",
      "Epoch [183/300], Step [91/112], Loss: 0.6618\n",
      "Epoch [183/300], Step [95/112], Loss: 0.6476\n",
      "Epoch [183/300], Step [99/112], Loss: 0.6478\n",
      "Epoch [183/300], Step [103/112], Loss: 0.6449\n",
      "Epoch [183/300], Step [107/112], Loss: 0.6319\n",
      "Epoch [183/300], Step [111/112], Loss: 0.6030\n",
      "Epoch [184/300], Step [3/112], Loss: 0.6411\n",
      "Epoch [184/300], Step [7/112], Loss: 0.6474\n",
      "Epoch [184/300], Step [11/112], Loss: 0.6405\n",
      "Epoch [184/300], Step [15/112], Loss: 0.6364\n",
      "Epoch [184/300], Step [19/112], Loss: 0.6294\n",
      "Epoch [184/300], Step [23/112], Loss: 0.6370\n",
      "Epoch [184/300], Step [27/112], Loss: 0.6085\n",
      "Epoch [184/300], Step [31/112], Loss: 0.6587\n",
      "Epoch [184/300], Step [35/112], Loss: 0.6222\n",
      "Epoch [184/300], Step [39/112], Loss: 0.6218\n",
      "Epoch [184/300], Step [43/112], Loss: 0.6255\n",
      "Epoch [184/300], Step [47/112], Loss: 0.6289\n",
      "Epoch [184/300], Step [51/112], Loss: 0.6196\n",
      "Epoch [184/300], Step [55/112], Loss: 0.6294\n",
      "Epoch [184/300], Step [59/112], Loss: 0.6217\n",
      "Epoch [184/300], Step [63/112], Loss: 0.6287\n",
      "Epoch [184/300], Step [67/112], Loss: 0.6427\n",
      "Epoch [184/300], Step [71/112], Loss: 0.6334\n",
      "Epoch [184/300], Step [75/112], Loss: 0.6430\n",
      "Epoch [184/300], Step [79/112], Loss: 0.6269\n",
      "Epoch [184/300], Step [83/112], Loss: 0.6595\n",
      "Epoch [184/300], Step [87/112], Loss: 0.6320\n",
      "Epoch [184/300], Step [91/112], Loss: 0.6629\n",
      "Epoch [184/300], Step [95/112], Loss: 0.6469\n",
      "Epoch [184/300], Step [99/112], Loss: 0.6487\n",
      "Epoch [184/300], Step [103/112], Loss: 0.6405\n",
      "Epoch [184/300], Step [107/112], Loss: 0.6360\n",
      "Epoch [184/300], Step [111/112], Loss: 0.5988\n",
      "Epoch [185/300], Step [3/112], Loss: 0.6455\n",
      "Epoch [185/300], Step [7/112], Loss: 0.6527\n",
      "Epoch [185/300], Step [11/112], Loss: 0.6415\n",
      "Epoch [185/300], Step [15/112], Loss: 0.6420\n",
      "Epoch [185/300], Step [19/112], Loss: 0.6350\n",
      "Epoch [185/300], Step [23/112], Loss: 0.6311\n",
      "Epoch [185/300], Step [27/112], Loss: 0.6057\n",
      "Epoch [185/300], Step [31/112], Loss: 0.6617\n",
      "Epoch [185/300], Step [35/112], Loss: 0.6271\n",
      "Epoch [185/300], Step [39/112], Loss: 0.6381\n",
      "Epoch [185/300], Step [43/112], Loss: 0.6247\n",
      "Epoch [185/300], Step [47/112], Loss: 0.6284\n",
      "Epoch [185/300], Step [51/112], Loss: 0.6186\n",
      "Epoch [185/300], Step [55/112], Loss: 0.6418\n",
      "Epoch [185/300], Step [59/112], Loss: 0.6266\n",
      "Epoch [185/300], Step [63/112], Loss: 0.6284\n",
      "Epoch [185/300], Step [67/112], Loss: 0.6332\n",
      "Epoch [185/300], Step [71/112], Loss: 0.6366\n",
      "Epoch [185/300], Step [75/112], Loss: 0.6523\n",
      "Epoch [185/300], Step [79/112], Loss: 0.6277\n",
      "Epoch [185/300], Step [83/112], Loss: 0.6551\n",
      "Epoch [185/300], Step [87/112], Loss: 0.6289\n",
      "Epoch [185/300], Step [91/112], Loss: 0.6692\n",
      "Epoch [185/300], Step [95/112], Loss: 0.6514\n",
      "Epoch [185/300], Step [99/112], Loss: 0.6539\n",
      "Epoch [185/300], Step [103/112], Loss: 0.6357\n",
      "Epoch [185/300], Step [107/112], Loss: 0.6359\n",
      "Epoch [185/300], Step [111/112], Loss: 0.5960\n",
      "Epoch [186/300], Step [3/112], Loss: 0.6396\n",
      "Epoch [186/300], Step [7/112], Loss: 0.6537\n",
      "Epoch [186/300], Step [11/112], Loss: 0.6430\n",
      "Epoch [186/300], Step [15/112], Loss: 0.6279\n",
      "Epoch [186/300], Step [19/112], Loss: 0.6310\n",
      "Epoch [186/300], Step [23/112], Loss: 0.6301\n",
      "Epoch [186/300], Step [27/112], Loss: 0.6087\n",
      "Epoch [186/300], Step [31/112], Loss: 0.6653\n",
      "Epoch [186/300], Step [35/112], Loss: 0.6198\n",
      "Epoch [186/300], Step [39/112], Loss: 0.6300\n",
      "Epoch [186/300], Step [43/112], Loss: 0.6340\n",
      "Epoch [186/300], Step [47/112], Loss: 0.6334\n",
      "Epoch [186/300], Step [51/112], Loss: 0.6226\n",
      "Epoch [186/300], Step [55/112], Loss: 0.6414\n",
      "Epoch [186/300], Step [59/112], Loss: 0.6248\n",
      "Epoch [186/300], Step [63/112], Loss: 0.6321\n",
      "Epoch [186/300], Step [67/112], Loss: 0.6384\n",
      "Epoch [186/300], Step [71/112], Loss: 0.6404\n",
      "Epoch [186/300], Step [75/112], Loss: 0.6492\n",
      "Epoch [186/300], Step [79/112], Loss: 0.6342\n",
      "Epoch [186/300], Step [83/112], Loss: 0.6622\n",
      "Epoch [186/300], Step [87/112], Loss: 0.6271\n",
      "Epoch [186/300], Step [91/112], Loss: 0.6687\n",
      "Epoch [186/300], Step [95/112], Loss: 0.6638\n",
      "Epoch [186/300], Step [99/112], Loss: 0.6524\n",
      "Epoch [186/300], Step [103/112], Loss: 0.6513\n",
      "Epoch [186/300], Step [107/112], Loss: 0.6417\n",
      "Epoch [186/300], Step [111/112], Loss: 0.6016\n",
      "Epoch [187/300], Step [3/112], Loss: 0.6456\n",
      "Epoch [187/300], Step [7/112], Loss: 0.6540\n",
      "Epoch [187/300], Step [11/112], Loss: 0.6393\n",
      "Epoch [187/300], Step [15/112], Loss: 0.6383\n",
      "Epoch [187/300], Step [19/112], Loss: 0.6245\n",
      "Epoch [187/300], Step [23/112], Loss: 0.6364\n",
      "Epoch [187/300], Step [27/112], Loss: 0.6018\n",
      "Epoch [187/300], Step [31/112], Loss: 0.6550\n",
      "Epoch [187/300], Step [35/112], Loss: 0.6275\n",
      "Epoch [187/300], Step [39/112], Loss: 0.6269\n",
      "Epoch [187/300], Step [43/112], Loss: 0.6348\n",
      "Epoch [187/300], Step [47/112], Loss: 0.6268\n",
      "Epoch [187/300], Step [51/112], Loss: 0.6194\n",
      "Epoch [187/300], Step [55/112], Loss: 0.6325\n",
      "Epoch [187/300], Step [59/112], Loss: 0.6199\n",
      "Epoch [187/300], Step [63/112], Loss: 0.6291\n",
      "Epoch [187/300], Step [67/112], Loss: 0.6355\n",
      "Epoch [187/300], Step [71/112], Loss: 0.6350\n",
      "Epoch [187/300], Step [75/112], Loss: 0.6452\n",
      "Epoch [187/300], Step [79/112], Loss: 0.6247\n",
      "Epoch [187/300], Step [83/112], Loss: 0.6582\n",
      "Epoch [187/300], Step [87/112], Loss: 0.6313\n",
      "Epoch [187/300], Step [91/112], Loss: 0.6704\n",
      "Epoch [187/300], Step [95/112], Loss: 0.6515\n",
      "Epoch [187/300], Step [99/112], Loss: 0.6517\n",
      "Epoch [187/300], Step [103/112], Loss: 0.6451\n",
      "Epoch [187/300], Step [107/112], Loss: 0.6340\n",
      "Epoch [187/300], Step [111/112], Loss: 0.5943\n",
      "Epoch [188/300], Step [3/112], Loss: 0.6519\n",
      "Epoch [188/300], Step [7/112], Loss: 0.6551\n",
      "Epoch [188/300], Step [11/112], Loss: 0.6389\n",
      "Epoch [188/300], Step [15/112], Loss: 0.6310\n",
      "Epoch [188/300], Step [19/112], Loss: 0.6283\n",
      "Epoch [188/300], Step [23/112], Loss: 0.6311\n",
      "Epoch [188/300], Step [27/112], Loss: 0.6031\n",
      "Epoch [188/300], Step [31/112], Loss: 0.6616\n",
      "Epoch [188/300], Step [35/112], Loss: 0.6214\n",
      "Epoch [188/300], Step [39/112], Loss: 0.6258\n",
      "Epoch [188/300], Step [43/112], Loss: 0.6258\n",
      "Epoch [188/300], Step [47/112], Loss: 0.6301\n",
      "Epoch [188/300], Step [51/112], Loss: 0.6186\n",
      "Epoch [188/300], Step [55/112], Loss: 0.6314\n",
      "Epoch [188/300], Step [59/112], Loss: 0.6132\n",
      "Epoch [188/300], Step [63/112], Loss: 0.6279\n",
      "Epoch [188/300], Step [67/112], Loss: 0.6345\n",
      "Epoch [188/300], Step [71/112], Loss: 0.6399\n",
      "Epoch [188/300], Step [75/112], Loss: 0.6553\n",
      "Epoch [188/300], Step [79/112], Loss: 0.6255\n",
      "Epoch [188/300], Step [83/112], Loss: 0.6645\n",
      "Epoch [188/300], Step [87/112], Loss: 0.6219\n",
      "Epoch [188/300], Step [91/112], Loss: 0.6595\n",
      "Epoch [188/300], Step [95/112], Loss: 0.6529\n",
      "Epoch [188/300], Step [99/112], Loss: 0.6512\n",
      "Epoch [188/300], Step [103/112], Loss: 0.6370\n",
      "Epoch [188/300], Step [107/112], Loss: 0.6369\n",
      "Epoch [188/300], Step [111/112], Loss: 0.5925\n",
      "Epoch [189/300], Step [3/112], Loss: 0.6435\n",
      "Epoch [189/300], Step [7/112], Loss: 0.6482\n",
      "Epoch [189/300], Step [11/112], Loss: 0.6348\n",
      "Epoch [189/300], Step [15/112], Loss: 0.6409\n",
      "Epoch [189/300], Step [19/112], Loss: 0.6304\n",
      "Epoch [189/300], Step [23/112], Loss: 0.6285\n",
      "Epoch [189/300], Step [27/112], Loss: 0.6030\n",
      "Epoch [189/300], Step [31/112], Loss: 0.6597\n",
      "Epoch [189/300], Step [35/112], Loss: 0.6266\n",
      "Epoch [189/300], Step [39/112], Loss: 0.6260\n",
      "Epoch [189/300], Step [43/112], Loss: 0.6233\n",
      "Epoch [189/300], Step [47/112], Loss: 0.6262\n",
      "Epoch [189/300], Step [51/112], Loss: 0.6189\n",
      "Epoch [189/300], Step [55/112], Loss: 0.6323\n",
      "Epoch [189/300], Step [59/112], Loss: 0.6202\n",
      "Epoch [189/300], Step [63/112], Loss: 0.6281\n",
      "Epoch [189/300], Step [67/112], Loss: 0.6294\n",
      "Epoch [189/300], Step [71/112], Loss: 0.6367\n",
      "Epoch [189/300], Step [75/112], Loss: 0.6449\n",
      "Epoch [189/300], Step [79/112], Loss: 0.6262\n",
      "Epoch [189/300], Step [83/112], Loss: 0.6567\n",
      "Epoch [189/300], Step [87/112], Loss: 0.6279\n",
      "Epoch [189/300], Step [91/112], Loss: 0.6642\n",
      "Epoch [189/300], Step [95/112], Loss: 0.6543\n",
      "Epoch [189/300], Step [99/112], Loss: 0.6468\n",
      "Epoch [189/300], Step [103/112], Loss: 0.6369\n",
      "Epoch [189/300], Step [107/112], Loss: 0.6337\n",
      "Epoch [189/300], Step [111/112], Loss: 0.6009\n",
      "Epoch [190/300], Step [3/112], Loss: 0.6422\n",
      "Epoch [190/300], Step [7/112], Loss: 0.6496\n",
      "Epoch [190/300], Step [11/112], Loss: 0.6408\n",
      "Epoch [190/300], Step [15/112], Loss: 0.6326\n",
      "Epoch [190/300], Step [19/112], Loss: 0.6214\n",
      "Epoch [190/300], Step [23/112], Loss: 0.6390\n",
      "Epoch [190/300], Step [27/112], Loss: 0.6076\n",
      "Epoch [190/300], Step [31/112], Loss: 0.6607\n",
      "Epoch [190/300], Step [35/112], Loss: 0.6248\n",
      "Epoch [190/300], Step [39/112], Loss: 0.6244\n",
      "Epoch [190/300], Step [43/112], Loss: 0.6263\n",
      "Epoch [190/300], Step [47/112], Loss: 0.6302\n",
      "Epoch [190/300], Step [51/112], Loss: 0.6188\n",
      "Epoch [190/300], Step [55/112], Loss: 0.6345\n",
      "Epoch [190/300], Step [59/112], Loss: 0.6229\n",
      "Epoch [190/300], Step [63/112], Loss: 0.6252\n",
      "Epoch [190/300], Step [67/112], Loss: 0.6349\n",
      "Epoch [190/300], Step [71/112], Loss: 0.6366\n",
      "Epoch [190/300], Step [75/112], Loss: 0.6508\n",
      "Epoch [190/300], Step [79/112], Loss: 0.6274\n",
      "Epoch [190/300], Step [83/112], Loss: 0.6658\n",
      "Epoch [190/300], Step [87/112], Loss: 0.6243\n",
      "Epoch [190/300], Step [91/112], Loss: 0.6708\n",
      "Epoch [190/300], Step [95/112], Loss: 0.6506\n",
      "Epoch [190/300], Step [99/112], Loss: 0.6547\n",
      "Epoch [190/300], Step [103/112], Loss: 0.6479\n",
      "Epoch [190/300], Step [107/112], Loss: 0.6442\n",
      "Epoch [190/300], Step [111/112], Loss: 0.5967\n",
      "Epoch [191/300], Step [3/112], Loss: 0.6495\n",
      "Epoch [191/300], Step [7/112], Loss: 0.6524\n",
      "Epoch [191/300], Step [11/112], Loss: 0.6404\n",
      "Epoch [191/300], Step [15/112], Loss: 0.6318\n",
      "Epoch [191/300], Step [19/112], Loss: 0.6324\n",
      "Epoch [191/300], Step [23/112], Loss: 0.6303\n",
      "Epoch [191/300], Step [27/112], Loss: 0.6013\n",
      "Epoch [191/300], Step [31/112], Loss: 0.6554\n",
      "Epoch [191/300], Step [35/112], Loss: 0.6231\n",
      "Epoch [191/300], Step [39/112], Loss: 0.6228\n",
      "Epoch [191/300], Step [43/112], Loss: 0.6221\n",
      "Epoch [191/300], Step [47/112], Loss: 0.6274\n",
      "Epoch [191/300], Step [51/112], Loss: 0.6237\n",
      "Epoch [191/300], Step [55/112], Loss: 0.6381\n",
      "Epoch [191/300], Step [59/112], Loss: 0.6152\n",
      "Epoch [191/300], Step [63/112], Loss: 0.6253\n",
      "Epoch [191/300], Step [67/112], Loss: 0.6315\n",
      "Epoch [191/300], Step [71/112], Loss: 0.6421\n",
      "Epoch [191/300], Step [75/112], Loss: 0.6463\n",
      "Epoch [191/300], Step [79/112], Loss: 0.6249\n",
      "Epoch [191/300], Step [83/112], Loss: 0.6649\n",
      "Epoch [191/300], Step [87/112], Loss: 0.6243\n",
      "Epoch [191/300], Step [91/112], Loss: 0.6670\n",
      "Epoch [191/300], Step [95/112], Loss: 0.6546\n",
      "Epoch [191/300], Step [99/112], Loss: 0.6457\n",
      "Epoch [191/300], Step [103/112], Loss: 0.6515\n",
      "Epoch [191/300], Step [107/112], Loss: 0.6376\n",
      "Epoch [191/300], Step [111/112], Loss: 0.5955\n",
      "Epoch [192/300], Step [3/112], Loss: 0.6462\n",
      "Epoch [192/300], Step [7/112], Loss: 0.6504\n",
      "Epoch [192/300], Step [11/112], Loss: 0.6407\n",
      "Epoch [192/300], Step [15/112], Loss: 0.6367\n",
      "Epoch [192/300], Step [19/112], Loss: 0.6279\n",
      "Epoch [192/300], Step [23/112], Loss: 0.6249\n",
      "Epoch [192/300], Step [27/112], Loss: 0.6089\n",
      "Epoch [192/300], Step [31/112], Loss: 0.6570\n",
      "Epoch [192/300], Step [35/112], Loss: 0.6302\n",
      "Epoch [192/300], Step [39/112], Loss: 0.6294\n",
      "Epoch [192/300], Step [43/112], Loss: 0.6304\n",
      "Epoch [192/300], Step [47/112], Loss: 0.6278\n",
      "Epoch [192/300], Step [51/112], Loss: 0.6142\n",
      "Epoch [192/300], Step [55/112], Loss: 0.6383\n",
      "Epoch [192/300], Step [59/112], Loss: 0.6179\n",
      "Epoch [192/300], Step [63/112], Loss: 0.6262\n",
      "Epoch [192/300], Step [67/112], Loss: 0.6316\n",
      "Epoch [192/300], Step [71/112], Loss: 0.6431\n",
      "Epoch [192/300], Step [75/112], Loss: 0.6464\n",
      "Epoch [192/300], Step [79/112], Loss: 0.6316\n",
      "Epoch [192/300], Step [83/112], Loss: 0.6663\n",
      "Epoch [192/300], Step [87/112], Loss: 0.6281\n",
      "Epoch [192/300], Step [91/112], Loss: 0.6799\n",
      "Epoch [192/300], Step [95/112], Loss: 0.6496\n",
      "Epoch [192/300], Step [99/112], Loss: 0.6562\n",
      "Epoch [192/300], Step [103/112], Loss: 0.6414\n",
      "Epoch [192/300], Step [107/112], Loss: 0.6430\n",
      "Epoch [192/300], Step [111/112], Loss: 0.5984\n",
      "Epoch [193/300], Step [3/112], Loss: 0.6369\n",
      "Epoch [193/300], Step [7/112], Loss: 0.6430\n",
      "Epoch [193/300], Step [11/112], Loss: 0.6437\n",
      "Epoch [193/300], Step [15/112], Loss: 0.6396\n",
      "Epoch [193/300], Step [19/112], Loss: 0.6364\n",
      "Epoch [193/300], Step [23/112], Loss: 0.6285\n",
      "Epoch [193/300], Step [27/112], Loss: 0.6027\n",
      "Epoch [193/300], Step [31/112], Loss: 0.6588\n",
      "Epoch [193/300], Step [35/112], Loss: 0.6233\n",
      "Epoch [193/300], Step [39/112], Loss: 0.6298\n",
      "Epoch [193/300], Step [43/112], Loss: 0.6227\n",
      "Epoch [193/300], Step [47/112], Loss: 0.6277\n",
      "Epoch [193/300], Step [51/112], Loss: 0.6223\n",
      "Epoch [193/300], Step [55/112], Loss: 0.6494\n",
      "Epoch [193/300], Step [59/112], Loss: 0.6163\n",
      "Epoch [193/300], Step [63/112], Loss: 0.6222\n",
      "Epoch [193/300], Step [67/112], Loss: 0.6343\n",
      "Epoch [193/300], Step [71/112], Loss: 0.6330\n",
      "Epoch [193/300], Step [75/112], Loss: 0.6472\n",
      "Epoch [193/300], Step [79/112], Loss: 0.6345\n",
      "Epoch [193/300], Step [83/112], Loss: 0.6570\n",
      "Epoch [193/300], Step [87/112], Loss: 0.6269\n",
      "Epoch [193/300], Step [91/112], Loss: 0.6680\n",
      "Epoch [193/300], Step [95/112], Loss: 0.6517\n",
      "Epoch [193/300], Step [99/112], Loss: 0.6498\n",
      "Epoch [193/300], Step [103/112], Loss: 0.6538\n",
      "Epoch [193/300], Step [107/112], Loss: 0.6377\n",
      "Epoch [193/300], Step [111/112], Loss: 0.5943\n",
      "Epoch [194/300], Step [3/112], Loss: 0.6495\n",
      "Epoch [194/300], Step [7/112], Loss: 0.6529\n",
      "Epoch [194/300], Step [11/112], Loss: 0.6449\n",
      "Epoch [194/300], Step [15/112], Loss: 0.6315\n",
      "Epoch [194/300], Step [19/112], Loss: 0.6212\n",
      "Epoch [194/300], Step [23/112], Loss: 0.6347\n",
      "Epoch [194/300], Step [27/112], Loss: 0.6111\n",
      "Epoch [194/300], Step [31/112], Loss: 0.6520\n",
      "Epoch [194/300], Step [35/112], Loss: 0.6226\n",
      "Epoch [194/300], Step [39/112], Loss: 0.6294\n",
      "Epoch [194/300], Step [43/112], Loss: 0.6201\n",
      "Epoch [194/300], Step [47/112], Loss: 0.6247\n",
      "Epoch [194/300], Step [51/112], Loss: 0.6230\n",
      "Epoch [194/300], Step [55/112], Loss: 0.6379\n",
      "Epoch [194/300], Step [59/112], Loss: 0.6188\n",
      "Epoch [194/300], Step [63/112], Loss: 0.6232\n",
      "Epoch [194/300], Step [67/112], Loss: 0.6342\n",
      "Epoch [194/300], Step [71/112], Loss: 0.6277\n",
      "Epoch [194/300], Step [75/112], Loss: 0.6474\n",
      "Epoch [194/300], Step [79/112], Loss: 0.6284\n",
      "Epoch [194/300], Step [83/112], Loss: 0.6515\n",
      "Epoch [194/300], Step [87/112], Loss: 0.6227\n",
      "Epoch [194/300], Step [91/112], Loss: 0.6697\n",
      "Epoch [194/300], Step [95/112], Loss: 0.6536\n",
      "Epoch [194/300], Step [99/112], Loss: 0.6534\n",
      "Epoch [194/300], Step [103/112], Loss: 0.6496\n",
      "Epoch [194/300], Step [107/112], Loss: 0.6354\n",
      "Epoch [194/300], Step [111/112], Loss: 0.5986\n",
      "Epoch [195/300], Step [3/112], Loss: 0.6414\n",
      "Epoch [195/300], Step [7/112], Loss: 0.6502\n",
      "Epoch [195/300], Step [11/112], Loss: 0.6462\n",
      "Epoch [195/300], Step [15/112], Loss: 0.6414\n",
      "Epoch [195/300], Step [19/112], Loss: 0.6248\n",
      "Epoch [195/300], Step [23/112], Loss: 0.6344\n",
      "Epoch [195/300], Step [27/112], Loss: 0.6071\n",
      "Epoch [195/300], Step [31/112], Loss: 0.6539\n",
      "Epoch [195/300], Step [35/112], Loss: 0.6172\n",
      "Epoch [195/300], Step [39/112], Loss: 0.6307\n",
      "Epoch [195/300], Step [43/112], Loss: 0.6270\n",
      "Epoch [195/300], Step [47/112], Loss: 0.6303\n",
      "Epoch [195/300], Step [51/112], Loss: 0.6226\n",
      "Epoch [195/300], Step [55/112], Loss: 0.6357\n",
      "Epoch [195/300], Step [59/112], Loss: 0.6221\n",
      "Epoch [195/300], Step [63/112], Loss: 0.6257\n",
      "Epoch [195/300], Step [67/112], Loss: 0.6313\n",
      "Epoch [195/300], Step [71/112], Loss: 0.6392\n",
      "Epoch [195/300], Step [75/112], Loss: 0.6546\n",
      "Epoch [195/300], Step [79/112], Loss: 0.6310\n",
      "Epoch [195/300], Step [83/112], Loss: 0.6607\n",
      "Epoch [195/300], Step [87/112], Loss: 0.6196\n",
      "Epoch [195/300], Step [91/112], Loss: 0.6712\n",
      "Epoch [195/300], Step [95/112], Loss: 0.6541\n",
      "Epoch [195/300], Step [99/112], Loss: 0.6619\n",
      "Epoch [195/300], Step [103/112], Loss: 0.6554\n",
      "Epoch [195/300], Step [107/112], Loss: 0.6421\n",
      "Epoch [195/300], Step [111/112], Loss: 0.5980\n",
      "Epoch [196/300], Step [3/112], Loss: 0.6498\n",
      "Epoch [196/300], Step [7/112], Loss: 0.6514\n",
      "Epoch [196/300], Step [11/112], Loss: 0.6528\n",
      "Epoch [196/300], Step [15/112], Loss: 0.6372\n",
      "Epoch [196/300], Step [19/112], Loss: 0.6190\n",
      "Epoch [196/300], Step [23/112], Loss: 0.6328\n",
      "Epoch [196/300], Step [27/112], Loss: 0.6019\n",
      "Epoch [196/300], Step [31/112], Loss: 0.6571\n",
      "Epoch [196/300], Step [35/112], Loss: 0.6177\n",
      "Epoch [196/300], Step [39/112], Loss: 0.6267\n",
      "Epoch [196/300], Step [43/112], Loss: 0.6209\n",
      "Epoch [196/300], Step [47/112], Loss: 0.6294\n",
      "Epoch [196/300], Step [51/112], Loss: 0.6233\n",
      "Epoch [196/300], Step [55/112], Loss: 0.6339\n",
      "Epoch [196/300], Step [59/112], Loss: 0.6259\n",
      "Epoch [196/300], Step [63/112], Loss: 0.6253\n",
      "Epoch [196/300], Step [67/112], Loss: 0.6302\n",
      "Epoch [196/300], Step [71/112], Loss: 0.6422\n",
      "Epoch [196/300], Step [75/112], Loss: 0.6473\n",
      "Epoch [196/300], Step [79/112], Loss: 0.6307\n",
      "Epoch [196/300], Step [83/112], Loss: 0.6671\n",
      "Epoch [196/300], Step [87/112], Loss: 0.6333\n",
      "Epoch [196/300], Step [91/112], Loss: 0.6659\n",
      "Epoch [196/300], Step [95/112], Loss: 0.6570\n",
      "Epoch [196/300], Step [99/112], Loss: 0.6545\n",
      "Epoch [196/300], Step [103/112], Loss: 0.6521\n",
      "Epoch [196/300], Step [107/112], Loss: 0.6408\n",
      "Epoch [196/300], Step [111/112], Loss: 0.6035\n",
      "Epoch [197/300], Step [3/112], Loss: 0.6522\n",
      "Epoch [197/300], Step [7/112], Loss: 0.6518\n",
      "Epoch [197/300], Step [11/112], Loss: 0.6406\n",
      "Epoch [197/300], Step [15/112], Loss: 0.6383\n",
      "Epoch [197/300], Step [19/112], Loss: 0.6250\n",
      "Epoch [197/300], Step [23/112], Loss: 0.6352\n",
      "Epoch [197/300], Step [27/112], Loss: 0.6044\n",
      "Epoch [197/300], Step [31/112], Loss: 0.6608\n",
      "Epoch [197/300], Step [35/112], Loss: 0.6198\n",
      "Epoch [197/300], Step [39/112], Loss: 0.6256\n",
      "Epoch [197/300], Step [43/112], Loss: 0.6263\n",
      "Epoch [197/300], Step [47/112], Loss: 0.6320\n",
      "Epoch [197/300], Step [51/112], Loss: 0.6275\n",
      "Epoch [197/300], Step [55/112], Loss: 0.6494\n",
      "Epoch [197/300], Step [59/112], Loss: 0.6221\n",
      "Epoch [197/300], Step [63/112], Loss: 0.6270\n",
      "Epoch [197/300], Step [67/112], Loss: 0.6348\n",
      "Epoch [197/300], Step [71/112], Loss: 0.6333\n",
      "Epoch [197/300], Step [75/112], Loss: 0.6518\n",
      "Epoch [197/300], Step [79/112], Loss: 0.6268\n",
      "Epoch [197/300], Step [83/112], Loss: 0.6635\n",
      "Epoch [197/300], Step [87/112], Loss: 0.6345\n",
      "Epoch [197/300], Step [91/112], Loss: 0.6764\n",
      "Epoch [197/300], Step [95/112], Loss: 0.6565\n",
      "Epoch [197/300], Step [99/112], Loss: 0.6568\n",
      "Epoch [197/300], Step [103/112], Loss: 0.6510\n",
      "Epoch [197/300], Step [107/112], Loss: 0.6350\n",
      "Epoch [197/300], Step [111/112], Loss: 0.5998\n",
      "Epoch [198/300], Step [3/112], Loss: 0.6471\n",
      "Epoch [198/300], Step [7/112], Loss: 0.6433\n",
      "Epoch [198/300], Step [11/112], Loss: 0.6448\n",
      "Epoch [198/300], Step [15/112], Loss: 0.6462\n",
      "Epoch [198/300], Step [19/112], Loss: 0.6232\n",
      "Epoch [198/300], Step [23/112], Loss: 0.6298\n",
      "Epoch [198/300], Step [27/112], Loss: 0.6148\n",
      "Epoch [198/300], Step [31/112], Loss: 0.6706\n",
      "Epoch [198/300], Step [35/112], Loss: 0.6268\n",
      "Epoch [198/300], Step [39/112], Loss: 0.6285\n",
      "Epoch [198/300], Step [43/112], Loss: 0.6368\n",
      "Epoch [198/300], Step [47/112], Loss: 0.6259\n",
      "Epoch [198/300], Step [51/112], Loss: 0.6178\n",
      "Epoch [198/300], Step [55/112], Loss: 0.6411\n",
      "Epoch [198/300], Step [59/112], Loss: 0.6271\n",
      "Epoch [198/300], Step [63/112], Loss: 0.6483\n",
      "Epoch [198/300], Step [67/112], Loss: 0.6343\n",
      "Epoch [198/300], Step [71/112], Loss: 0.6359\n",
      "Epoch [198/300], Step [75/112], Loss: 0.6424\n",
      "Epoch [198/300], Step [79/112], Loss: 0.6238\n",
      "Epoch [198/300], Step [83/112], Loss: 0.6545\n",
      "Epoch [198/300], Step [87/112], Loss: 0.6338\n",
      "Epoch [198/300], Step [91/112], Loss: 0.6660\n",
      "Epoch [198/300], Step [95/112], Loss: 0.6582\n",
      "Epoch [198/300], Step [99/112], Loss: 0.6547\n",
      "Epoch [198/300], Step [103/112], Loss: 0.6472\n",
      "Epoch [198/300], Step [107/112], Loss: 0.6395\n",
      "Epoch [198/300], Step [111/112], Loss: 0.5995\n",
      "Epoch [199/300], Step [3/112], Loss: 0.6495\n",
      "Epoch [199/300], Step [7/112], Loss: 0.6532\n",
      "Epoch [199/300], Step [11/112], Loss: 0.6405\n",
      "Epoch [199/300], Step [15/112], Loss: 0.6348\n",
      "Epoch [199/300], Step [19/112], Loss: 0.6183\n",
      "Epoch [199/300], Step [23/112], Loss: 0.6381\n",
      "Epoch [199/300], Step [27/112], Loss: 0.6094\n",
      "Epoch [199/300], Step [31/112], Loss: 0.6528\n",
      "Epoch [199/300], Step [35/112], Loss: 0.6336\n",
      "Epoch [199/300], Step [39/112], Loss: 0.6256\n",
      "Epoch [199/300], Step [43/112], Loss: 0.6319\n",
      "Epoch [199/300], Step [47/112], Loss: 0.6288\n",
      "Epoch [199/300], Step [51/112], Loss: 0.6209\n",
      "Epoch [199/300], Step [55/112], Loss: 0.6427\n",
      "Epoch [199/300], Step [59/112], Loss: 0.6245\n",
      "Epoch [199/300], Step [63/112], Loss: 0.6272\n",
      "Epoch [199/300], Step [67/112], Loss: 0.6364\n",
      "Epoch [199/300], Step [71/112], Loss: 0.6425\n",
      "Epoch [199/300], Step [75/112], Loss: 0.6491\n",
      "Epoch [199/300], Step [79/112], Loss: 0.6345\n",
      "Epoch [199/300], Step [83/112], Loss: 0.6668\n",
      "Epoch [199/300], Step [87/112], Loss: 0.6388\n",
      "Epoch [199/300], Step [91/112], Loss: 0.6860\n",
      "Epoch [199/300], Step [95/112], Loss: 0.6522\n",
      "Epoch [199/300], Step [99/112], Loss: 0.6514\n",
      "Epoch [199/300], Step [103/112], Loss: 0.6497\n",
      "Epoch [199/300], Step [107/112], Loss: 0.6487\n",
      "Epoch [199/300], Step [111/112], Loss: 0.6106\n",
      "Epoch [200/300], Step [3/112], Loss: 0.6679\n",
      "Epoch [200/300], Step [7/112], Loss: 0.6599\n",
      "Epoch [200/300], Step [11/112], Loss: 0.6541\n",
      "Epoch [200/300], Step [15/112], Loss: 0.6345\n",
      "Epoch [200/300], Step [19/112], Loss: 0.6292\n",
      "Epoch [200/300], Step [23/112], Loss: 0.6332\n",
      "Epoch [200/300], Step [27/112], Loss: 0.6128\n",
      "Epoch [200/300], Step [31/112], Loss: 0.6629\n",
      "Epoch [200/300], Step [35/112], Loss: 0.6293\n",
      "Epoch [200/300], Step [39/112], Loss: 0.6300\n",
      "Epoch [200/300], Step [43/112], Loss: 0.6315\n",
      "Epoch [200/300], Step [47/112], Loss: 0.6286\n",
      "Epoch [200/300], Step [51/112], Loss: 0.6236\n",
      "Epoch [200/300], Step [55/112], Loss: 0.6434\n",
      "Epoch [200/300], Step [59/112], Loss: 0.6269\n",
      "Epoch [200/300], Step [63/112], Loss: 0.6325\n",
      "Epoch [200/300], Step [67/112], Loss: 0.6368\n",
      "Epoch [200/300], Step [71/112], Loss: 0.6386\n",
      "Epoch [200/300], Step [75/112], Loss: 0.6560\n",
      "Epoch [200/300], Step [79/112], Loss: 0.6289\n",
      "Epoch [200/300], Step [83/112], Loss: 0.6685\n",
      "Epoch [200/300], Step [87/112], Loss: 0.6306\n",
      "Epoch [200/300], Step [91/112], Loss: 0.6701\n",
      "Epoch [200/300], Step [95/112], Loss: 0.6466\n",
      "Epoch [200/300], Step [99/112], Loss: 0.6524\n",
      "Epoch [200/300], Step [103/112], Loss: 0.6478\n",
      "Epoch [200/300], Step [107/112], Loss: 0.6490\n",
      "Epoch [200/300], Step [111/112], Loss: 0.6071\n",
      "Epoch [201/300], Step [3/112], Loss: 0.6563\n",
      "Epoch [201/300], Step [7/112], Loss: 0.6630\n",
      "Epoch [201/300], Step [11/112], Loss: 0.6530\n",
      "Epoch [201/300], Step [15/112], Loss: 0.6436\n",
      "Epoch [201/300], Step [19/112], Loss: 0.6341\n",
      "Epoch [201/300], Step [23/112], Loss: 0.6347\n",
      "Epoch [201/300], Step [27/112], Loss: 0.6148\n",
      "Epoch [201/300], Step [31/112], Loss: 0.6632\n",
      "Epoch [201/300], Step [35/112], Loss: 0.6222\n",
      "Epoch [201/300], Step [39/112], Loss: 0.6308\n",
      "Epoch [201/300], Step [43/112], Loss: 0.6367\n",
      "Epoch [201/300], Step [47/112], Loss: 0.6359\n",
      "Epoch [201/300], Step [51/112], Loss: 0.6349\n",
      "Epoch [201/300], Step [55/112], Loss: 0.6552\n",
      "Epoch [201/300], Step [59/112], Loss: 0.6355\n",
      "Epoch [201/300], Step [63/112], Loss: 0.6328\n",
      "Epoch [201/300], Step [67/112], Loss: 0.6346\n",
      "Epoch [201/300], Step [71/112], Loss: 0.6529\n",
      "Epoch [201/300], Step [75/112], Loss: 0.6448\n",
      "Epoch [201/300], Step [79/112], Loss: 0.6302\n",
      "Epoch [201/300], Step [83/112], Loss: 0.6752\n",
      "Epoch [201/300], Step [87/112], Loss: 0.6377\n",
      "Epoch [201/300], Step [91/112], Loss: 0.6724\n",
      "Epoch [201/300], Step [95/112], Loss: 0.6513\n",
      "Epoch [201/300], Step [99/112], Loss: 0.6513\n",
      "Epoch [201/300], Step [103/112], Loss: 0.6556\n",
      "Epoch [201/300], Step [107/112], Loss: 0.6389\n",
      "Epoch [201/300], Step [111/112], Loss: 0.6063\n",
      "Epoch [202/300], Step [3/112], Loss: 0.6584\n",
      "Epoch [202/300], Step [7/112], Loss: 0.6663\n",
      "Epoch [202/300], Step [11/112], Loss: 0.6399\n",
      "Epoch [202/300], Step [15/112], Loss: 0.6433\n",
      "Epoch [202/300], Step [19/112], Loss: 0.6397\n",
      "Epoch [202/300], Step [23/112], Loss: 0.6428\n",
      "Epoch [202/300], Step [27/112], Loss: 0.6154\n",
      "Epoch [202/300], Step [31/112], Loss: 0.6632\n",
      "Epoch [202/300], Step [35/112], Loss: 0.6247\n",
      "Epoch [202/300], Step [39/112], Loss: 0.6221\n",
      "Epoch [202/300], Step [43/112], Loss: 0.6344\n",
      "Epoch [202/300], Step [47/112], Loss: 0.6299\n",
      "Epoch [202/300], Step [51/112], Loss: 0.6334\n",
      "Epoch [202/300], Step [55/112], Loss: 0.6539\n",
      "Epoch [202/300], Step [59/112], Loss: 0.6200\n",
      "Epoch [202/300], Step [63/112], Loss: 0.6248\n",
      "Epoch [202/300], Step [67/112], Loss: 0.6226\n",
      "Epoch [202/300], Step [71/112], Loss: 0.6416\n",
      "Epoch [202/300], Step [75/112], Loss: 0.6579\n",
      "Epoch [202/300], Step [79/112], Loss: 0.6383\n",
      "Epoch [202/300], Step [83/112], Loss: 0.6681\n",
      "Epoch [202/300], Step [87/112], Loss: 0.6339\n",
      "Epoch [202/300], Step [91/112], Loss: 0.6678\n",
      "Epoch [202/300], Step [95/112], Loss: 0.6401\n",
      "Epoch [202/300], Step [99/112], Loss: 0.6527\n",
      "Epoch [202/300], Step [103/112], Loss: 0.6579\n",
      "Epoch [202/300], Step [107/112], Loss: 0.6552\n",
      "Epoch [202/300], Step [111/112], Loss: 0.6040\n",
      "Epoch [203/300], Step [3/112], Loss: 0.6679\n",
      "Epoch [203/300], Step [7/112], Loss: 0.6568\n",
      "Epoch [203/300], Step [11/112], Loss: 0.6484\n",
      "Epoch [203/300], Step [15/112], Loss: 0.6472\n",
      "Epoch [203/300], Step [19/112], Loss: 0.6497\n",
      "Epoch [203/300], Step [23/112], Loss: 0.6483\n",
      "Epoch [203/300], Step [27/112], Loss: 0.6143\n",
      "Epoch [203/300], Step [31/112], Loss: 0.6588\n",
      "Epoch [203/300], Step [35/112], Loss: 0.6295\n",
      "Epoch [203/300], Step [39/112], Loss: 0.6206\n",
      "Epoch [203/300], Step [43/112], Loss: 0.6354\n",
      "Epoch [203/300], Step [47/112], Loss: 0.6266\n",
      "Epoch [203/300], Step [51/112], Loss: 0.6278\n",
      "Epoch [203/300], Step [55/112], Loss: 0.6453\n",
      "Epoch [203/300], Step [59/112], Loss: 0.6157\n",
      "Epoch [203/300], Step [63/112], Loss: 0.6236\n",
      "Epoch [203/300], Step [67/112], Loss: 0.6391\n",
      "Epoch [203/300], Step [71/112], Loss: 0.6405\n",
      "Epoch [203/300], Step [75/112], Loss: 0.6579\n",
      "Epoch [203/300], Step [79/112], Loss: 0.6313\n",
      "Epoch [203/300], Step [83/112], Loss: 0.6695\n",
      "Epoch [203/300], Step [87/112], Loss: 0.6333\n",
      "Epoch [203/300], Step [91/112], Loss: 0.6692\n",
      "Epoch [203/300], Step [95/112], Loss: 0.6467\n",
      "Epoch [203/300], Step [99/112], Loss: 0.6526\n",
      "Epoch [203/300], Step [103/112], Loss: 0.6540\n",
      "Epoch [203/300], Step [107/112], Loss: 0.6501\n",
      "Epoch [203/300], Step [111/112], Loss: 0.5972\n",
      "Epoch [204/300], Step [3/112], Loss: 0.6549\n",
      "Epoch [204/300], Step [7/112], Loss: 0.6460\n",
      "Epoch [204/300], Step [11/112], Loss: 0.6522\n",
      "Epoch [204/300], Step [15/112], Loss: 0.6518\n",
      "Epoch [204/300], Step [19/112], Loss: 0.6459\n",
      "Epoch [204/300], Step [23/112], Loss: 0.6395\n",
      "Epoch [204/300], Step [27/112], Loss: 0.6170\n",
      "Epoch [204/300], Step [31/112], Loss: 0.6595\n",
      "Epoch [204/300], Step [35/112], Loss: 0.6228\n",
      "Epoch [204/300], Step [39/112], Loss: 0.6277\n",
      "Epoch [204/300], Step [43/112], Loss: 0.6410\n",
      "Epoch [204/300], Step [47/112], Loss: 0.6255\n",
      "Epoch [204/300], Step [51/112], Loss: 0.6275\n",
      "Epoch [204/300], Step [55/112], Loss: 0.6362\n",
      "Epoch [204/300], Step [59/112], Loss: 0.6268\n",
      "Epoch [204/300], Step [63/112], Loss: 0.6243\n",
      "Epoch [204/300], Step [67/112], Loss: 0.6310\n",
      "Epoch [204/300], Step [71/112], Loss: 0.6356\n",
      "Epoch [204/300], Step [75/112], Loss: 0.6499\n",
      "Epoch [204/300], Step [79/112], Loss: 0.6314\n",
      "Epoch [204/300], Step [83/112], Loss: 0.6603\n",
      "Epoch [204/300], Step [87/112], Loss: 0.6239\n",
      "Epoch [204/300], Step [91/112], Loss: 0.6648\n",
      "Epoch [204/300], Step [95/112], Loss: 0.6496\n",
      "Epoch [204/300], Step [99/112], Loss: 0.6511\n",
      "Epoch [204/300], Step [103/112], Loss: 0.6509\n",
      "Epoch [204/300], Step [107/112], Loss: 0.6383\n",
      "Epoch [204/300], Step [111/112], Loss: 0.5860\n",
      "Epoch [205/300], Step [3/112], Loss: 0.6593\n",
      "Epoch [205/300], Step [7/112], Loss: 0.6529\n",
      "Epoch [205/300], Step [11/112], Loss: 0.6485\n",
      "Epoch [205/300], Step [15/112], Loss: 0.6436\n",
      "Epoch [205/300], Step [19/112], Loss: 0.6355\n",
      "Epoch [205/300], Step [23/112], Loss: 0.6306\n",
      "Epoch [205/300], Step [27/112], Loss: 0.6010\n",
      "Epoch [205/300], Step [31/112], Loss: 0.6613\n",
      "Epoch [205/300], Step [35/112], Loss: 0.6237\n",
      "Epoch [205/300], Step [39/112], Loss: 0.6172\n",
      "Epoch [205/300], Step [43/112], Loss: 0.6297\n",
      "Epoch [205/300], Step [47/112], Loss: 0.6309\n",
      "Epoch [205/300], Step [51/112], Loss: 0.6215\n",
      "Epoch [205/300], Step [55/112], Loss: 0.6315\n",
      "Epoch [205/300], Step [59/112], Loss: 0.6313\n",
      "Epoch [205/300], Step [63/112], Loss: 0.6225\n",
      "Epoch [205/300], Step [67/112], Loss: 0.6289\n",
      "Epoch [205/300], Step [71/112], Loss: 0.6354\n",
      "Epoch [205/300], Step [75/112], Loss: 0.6423\n",
      "Epoch [205/300], Step [79/112], Loss: 0.6289\n",
      "Epoch [205/300], Step [83/112], Loss: 0.6589\n",
      "Epoch [205/300], Step [87/112], Loss: 0.6286\n",
      "Epoch [205/300], Step [91/112], Loss: 0.6730\n",
      "Epoch [205/300], Step [95/112], Loss: 0.6429\n",
      "Epoch [205/300], Step [99/112], Loss: 0.6445\n",
      "Epoch [205/300], Step [103/112], Loss: 0.6502\n",
      "Epoch [205/300], Step [107/112], Loss: 0.6435\n",
      "Epoch [205/300], Step [111/112], Loss: 0.5991\n",
      "Epoch [206/300], Step [3/112], Loss: 0.6487\n",
      "Epoch [206/300], Step [7/112], Loss: 0.6479\n",
      "Epoch [206/300], Step [11/112], Loss: 0.6459\n",
      "Epoch [206/300], Step [15/112], Loss: 0.6515\n",
      "Epoch [206/300], Step [19/112], Loss: 0.6385\n",
      "Epoch [206/300], Step [23/112], Loss: 0.6426\n",
      "Epoch [206/300], Step [27/112], Loss: 0.6072\n",
      "Epoch [206/300], Step [31/112], Loss: 0.6529\n",
      "Epoch [206/300], Step [35/112], Loss: 0.6189\n",
      "Epoch [206/300], Step [39/112], Loss: 0.6162\n",
      "Epoch [206/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [206/300], Step [47/112], Loss: 0.6286\n",
      "Epoch [206/300], Step [51/112], Loss: 0.6247\n",
      "Epoch [206/300], Step [55/112], Loss: 0.6298\n",
      "Epoch [206/300], Step [59/112], Loss: 0.6252\n",
      "Epoch [206/300], Step [63/112], Loss: 0.6264\n",
      "Epoch [206/300], Step [67/112], Loss: 0.6225\n",
      "Epoch [206/300], Step [71/112], Loss: 0.6380\n",
      "Epoch [206/300], Step [75/112], Loss: 0.6396\n",
      "Epoch [206/300], Step [79/112], Loss: 0.6232\n",
      "Epoch [206/300], Step [83/112], Loss: 0.6621\n",
      "Epoch [206/300], Step [87/112], Loss: 0.6216\n",
      "Epoch [206/300], Step [91/112], Loss: 0.6634\n",
      "Epoch [206/300], Step [95/112], Loss: 0.6468\n",
      "Epoch [206/300], Step [99/112], Loss: 0.6466\n",
      "Epoch [206/300], Step [103/112], Loss: 0.6534\n",
      "Epoch [206/300], Step [107/112], Loss: 0.6368\n",
      "Epoch [206/300], Step [111/112], Loss: 0.5977\n",
      "Epoch [207/300], Step [3/112], Loss: 0.6481\n",
      "Epoch [207/300], Step [7/112], Loss: 0.6514\n",
      "Epoch [207/300], Step [11/112], Loss: 0.6432\n",
      "Epoch [207/300], Step [15/112], Loss: 0.6356\n",
      "Epoch [207/300], Step [19/112], Loss: 0.6234\n",
      "Epoch [207/300], Step [23/112], Loss: 0.6270\n",
      "Epoch [207/300], Step [27/112], Loss: 0.6032\n",
      "Epoch [207/300], Step [31/112], Loss: 0.6572\n",
      "Epoch [207/300], Step [35/112], Loss: 0.6217\n",
      "Epoch [207/300], Step [39/112], Loss: 0.6113\n",
      "Epoch [207/300], Step [43/112], Loss: 0.6256\n",
      "Epoch [207/300], Step [47/112], Loss: 0.6268\n",
      "Epoch [207/300], Step [51/112], Loss: 0.6213\n",
      "Epoch [207/300], Step [55/112], Loss: 0.6361\n",
      "Epoch [207/300], Step [59/112], Loss: 0.6198\n",
      "Epoch [207/300], Step [63/112], Loss: 0.6187\n",
      "Epoch [207/300], Step [67/112], Loss: 0.6347\n",
      "Epoch [207/300], Step [71/112], Loss: 0.6323\n",
      "Epoch [207/300], Step [75/112], Loss: 0.6515\n",
      "Epoch [207/300], Step [79/112], Loss: 0.6185\n",
      "Epoch [207/300], Step [83/112], Loss: 0.6647\n",
      "Epoch [207/300], Step [87/112], Loss: 0.6257\n",
      "Epoch [207/300], Step [91/112], Loss: 0.6637\n",
      "Epoch [207/300], Step [95/112], Loss: 0.6426\n",
      "Epoch [207/300], Step [99/112], Loss: 0.6385\n",
      "Epoch [207/300], Step [103/112], Loss: 0.6470\n",
      "Epoch [207/300], Step [107/112], Loss: 0.6350\n",
      "Epoch [207/300], Step [111/112], Loss: 0.5912\n",
      "Epoch [208/300], Step [3/112], Loss: 0.6565\n",
      "Epoch [208/300], Step [7/112], Loss: 0.6502\n",
      "Epoch [208/300], Step [11/112], Loss: 0.6396\n",
      "Epoch [208/300], Step [15/112], Loss: 0.6361\n",
      "Epoch [208/300], Step [19/112], Loss: 0.6311\n",
      "Epoch [208/300], Step [23/112], Loss: 0.6344\n",
      "Epoch [208/300], Step [27/112], Loss: 0.6086\n",
      "Epoch [208/300], Step [31/112], Loss: 0.6587\n",
      "Epoch [208/300], Step [35/112], Loss: 0.6175\n",
      "Epoch [208/300], Step [39/112], Loss: 0.6239\n",
      "Epoch [208/300], Step [43/112], Loss: 0.6282\n",
      "Epoch [208/300], Step [47/112], Loss: 0.6299\n",
      "Epoch [208/300], Step [51/112], Loss: 0.6261\n",
      "Epoch [208/300], Step [55/112], Loss: 0.6389\n",
      "Epoch [208/300], Step [59/112], Loss: 0.6259\n",
      "Epoch [208/300], Step [63/112], Loss: 0.6199\n",
      "Epoch [208/300], Step [67/112], Loss: 0.6250\n",
      "Epoch [208/300], Step [71/112], Loss: 0.6348\n",
      "Epoch [208/300], Step [75/112], Loss: 0.6496\n",
      "Epoch [208/300], Step [79/112], Loss: 0.6300\n",
      "Epoch [208/300], Step [83/112], Loss: 0.6626\n",
      "Epoch [208/300], Step [87/112], Loss: 0.6268\n",
      "Epoch [208/300], Step [91/112], Loss: 0.6721\n",
      "Epoch [208/300], Step [95/112], Loss: 0.6492\n",
      "Epoch [208/300], Step [99/112], Loss: 0.6468\n",
      "Epoch [208/300], Step [103/112], Loss: 0.6530\n",
      "Epoch [208/300], Step [107/112], Loss: 0.6488\n",
      "Epoch [208/300], Step [111/112], Loss: 0.6061\n",
      "Epoch [209/300], Step [3/112], Loss: 0.6516\n",
      "Epoch [209/300], Step [7/112], Loss: 0.6425\n",
      "Epoch [209/300], Step [11/112], Loss: 0.6419\n",
      "Epoch [209/300], Step [15/112], Loss: 0.6322\n",
      "Epoch [209/300], Step [19/112], Loss: 0.6283\n",
      "Epoch [209/300], Step [23/112], Loss: 0.6356\n",
      "Epoch [209/300], Step [27/112], Loss: 0.6006\n",
      "Epoch [209/300], Step [31/112], Loss: 0.6513\n",
      "Epoch [209/300], Step [35/112], Loss: 0.6175\n",
      "Epoch [209/300], Step [39/112], Loss: 0.6187\n",
      "Epoch [209/300], Step [43/112], Loss: 0.6277\n",
      "Epoch [209/300], Step [47/112], Loss: 0.6282\n",
      "Epoch [209/300], Step [51/112], Loss: 0.6265\n",
      "Epoch [209/300], Step [55/112], Loss: 0.6482\n",
      "Epoch [209/300], Step [59/112], Loss: 0.6275\n",
      "Epoch [209/300], Step [63/112], Loss: 0.6207\n",
      "Epoch [209/300], Step [67/112], Loss: 0.6223\n",
      "Epoch [209/300], Step [71/112], Loss: 0.6429\n",
      "Epoch [209/300], Step [75/112], Loss: 0.6501\n",
      "Epoch [209/300], Step [79/112], Loss: 0.6167\n",
      "Epoch [209/300], Step [83/112], Loss: 0.6616\n",
      "Epoch [209/300], Step [87/112], Loss: 0.6268\n",
      "Epoch [209/300], Step [91/112], Loss: 0.6589\n",
      "Epoch [209/300], Step [95/112], Loss: 0.6441\n",
      "Epoch [209/300], Step [99/112], Loss: 0.6489\n",
      "Epoch [209/300], Step [103/112], Loss: 0.6533\n",
      "Epoch [209/300], Step [107/112], Loss: 0.6401\n",
      "Epoch [209/300], Step [111/112], Loss: 0.5896\n",
      "Epoch [210/300], Step [3/112], Loss: 0.6539\n",
      "Epoch [210/300], Step [7/112], Loss: 0.6464\n",
      "Epoch [210/300], Step [11/112], Loss: 0.6382\n",
      "Epoch [210/300], Step [15/112], Loss: 0.6377\n",
      "Epoch [210/300], Step [19/112], Loss: 0.6206\n",
      "Epoch [210/300], Step [23/112], Loss: 0.6272\n",
      "Epoch [210/300], Step [27/112], Loss: 0.6040\n",
      "Epoch [210/300], Step [31/112], Loss: 0.6556\n",
      "Epoch [210/300], Step [35/112], Loss: 0.6120\n",
      "Epoch [210/300], Step [39/112], Loss: 0.6145\n",
      "Epoch [210/300], Step [43/112], Loss: 0.6278\n",
      "Epoch [210/300], Step [47/112], Loss: 0.6240\n",
      "Epoch [210/300], Step [51/112], Loss: 0.6255\n",
      "Epoch [210/300], Step [55/112], Loss: 0.6347\n",
      "Epoch [210/300], Step [59/112], Loss: 0.6269\n",
      "Epoch [210/300], Step [63/112], Loss: 0.6293\n",
      "Epoch [210/300], Step [67/112], Loss: 0.6267\n",
      "Epoch [210/300], Step [71/112], Loss: 0.6442\n",
      "Epoch [210/300], Step [75/112], Loss: 0.6531\n",
      "Epoch [210/300], Step [79/112], Loss: 0.6332\n",
      "Epoch [210/300], Step [83/112], Loss: 0.6602\n",
      "Epoch [210/300], Step [87/112], Loss: 0.6269\n",
      "Epoch [210/300], Step [91/112], Loss: 0.6671\n",
      "Epoch [210/300], Step [95/112], Loss: 0.6485\n",
      "Epoch [210/300], Step [99/112], Loss: 0.6466\n",
      "Epoch [210/300], Step [103/112], Loss: 0.6473\n",
      "Epoch [210/300], Step [107/112], Loss: 0.6391\n",
      "Epoch [210/300], Step [111/112], Loss: 0.5981\n",
      "Epoch [211/300], Step [3/112], Loss: 0.6661\n",
      "Epoch [211/300], Step [7/112], Loss: 0.6467\n",
      "Epoch [211/300], Step [11/112], Loss: 0.6361\n",
      "Epoch [211/300], Step [15/112], Loss: 0.6330\n",
      "Epoch [211/300], Step [19/112], Loss: 0.6255\n",
      "Epoch [211/300], Step [23/112], Loss: 0.6315\n",
      "Epoch [211/300], Step [27/112], Loss: 0.6150\n",
      "Epoch [211/300], Step [31/112], Loss: 0.6586\n",
      "Epoch [211/300], Step [35/112], Loss: 0.6132\n",
      "Epoch [211/300], Step [39/112], Loss: 0.6224\n",
      "Epoch [211/300], Step [43/112], Loss: 0.6306\n",
      "Epoch [211/300], Step [47/112], Loss: 0.6264\n",
      "Epoch [211/300], Step [51/112], Loss: 0.6268\n",
      "Epoch [211/300], Step [55/112], Loss: 0.6354\n",
      "Epoch [211/300], Step [59/112], Loss: 0.6223\n",
      "Epoch [211/300], Step [63/112], Loss: 0.6223\n",
      "Epoch [211/300], Step [67/112], Loss: 0.6257\n",
      "Epoch [211/300], Step [71/112], Loss: 0.6373\n",
      "Epoch [211/300], Step [75/112], Loss: 0.6511\n",
      "Epoch [211/300], Step [79/112], Loss: 0.6316\n",
      "Epoch [211/300], Step [83/112], Loss: 0.6638\n",
      "Epoch [211/300], Step [87/112], Loss: 0.6278\n",
      "Epoch [211/300], Step [91/112], Loss: 0.6663\n",
      "Epoch [211/300], Step [95/112], Loss: 0.6430\n",
      "Epoch [211/300], Step [99/112], Loss: 0.6454\n",
      "Epoch [211/300], Step [103/112], Loss: 0.6502\n",
      "Epoch [211/300], Step [107/112], Loss: 0.6329\n",
      "Epoch [211/300], Step [111/112], Loss: 0.5970\n",
      "Epoch [212/300], Step [3/112], Loss: 0.6499\n",
      "Epoch [212/300], Step [7/112], Loss: 0.6421\n",
      "Epoch [212/300], Step [11/112], Loss: 0.6333\n",
      "Epoch [212/300], Step [15/112], Loss: 0.6370\n",
      "Epoch [212/300], Step [19/112], Loss: 0.6338\n",
      "Epoch [212/300], Step [23/112], Loss: 0.6369\n",
      "Epoch [212/300], Step [27/112], Loss: 0.6047\n",
      "Epoch [212/300], Step [31/112], Loss: 0.6554\n",
      "Epoch [212/300], Step [35/112], Loss: 0.6174\n",
      "Epoch [212/300], Step [39/112], Loss: 0.6204\n",
      "Epoch [212/300], Step [43/112], Loss: 0.6261\n",
      "Epoch [212/300], Step [47/112], Loss: 0.6256\n",
      "Epoch [212/300], Step [51/112], Loss: 0.6262\n",
      "Epoch [212/300], Step [55/112], Loss: 0.6419\n",
      "Epoch [212/300], Step [59/112], Loss: 0.6139\n",
      "Epoch [212/300], Step [63/112], Loss: 0.6292\n",
      "Epoch [212/300], Step [67/112], Loss: 0.6330\n",
      "Epoch [212/300], Step [71/112], Loss: 0.6339\n",
      "Epoch [212/300], Step [75/112], Loss: 0.6499\n",
      "Epoch [212/300], Step [79/112], Loss: 0.6320\n",
      "Epoch [212/300], Step [83/112], Loss: 0.6637\n",
      "Epoch [212/300], Step [87/112], Loss: 0.6315\n",
      "Epoch [212/300], Step [91/112], Loss: 0.6692\n",
      "Epoch [212/300], Step [95/112], Loss: 0.6464\n",
      "Epoch [212/300], Step [99/112], Loss: 0.6433\n",
      "Epoch [212/300], Step [103/112], Loss: 0.6496\n",
      "Epoch [212/300], Step [107/112], Loss: 0.6350\n",
      "Epoch [212/300], Step [111/112], Loss: 0.5944\n",
      "Epoch [213/300], Step [3/112], Loss: 0.6537\n",
      "Epoch [213/300], Step [7/112], Loss: 0.6470\n",
      "Epoch [213/300], Step [11/112], Loss: 0.6372\n",
      "Epoch [213/300], Step [15/112], Loss: 0.6411\n",
      "Epoch [213/300], Step [19/112], Loss: 0.6238\n",
      "Epoch [213/300], Step [23/112], Loss: 0.6468\n",
      "Epoch [213/300], Step [27/112], Loss: 0.6118\n",
      "Epoch [213/300], Step [31/112], Loss: 0.6486\n",
      "Epoch [213/300], Step [35/112], Loss: 0.6193\n",
      "Epoch [213/300], Step [39/112], Loss: 0.6244\n",
      "Epoch [213/300], Step [43/112], Loss: 0.6268\n",
      "Epoch [213/300], Step [47/112], Loss: 0.6247\n",
      "Epoch [213/300], Step [51/112], Loss: 0.6301\n",
      "Epoch [213/300], Step [55/112], Loss: 0.6548\n",
      "Epoch [213/300], Step [59/112], Loss: 0.6224\n",
      "Epoch [213/300], Step [63/112], Loss: 0.6297\n",
      "Epoch [213/300], Step [67/112], Loss: 0.6229\n",
      "Epoch [213/300], Step [71/112], Loss: 0.6410\n",
      "Epoch [213/300], Step [75/112], Loss: 0.6503\n",
      "Epoch [213/300], Step [79/112], Loss: 0.6271\n",
      "Epoch [213/300], Step [83/112], Loss: 0.6659\n",
      "Epoch [213/300], Step [87/112], Loss: 0.6315\n",
      "Epoch [213/300], Step [91/112], Loss: 0.6677\n",
      "Epoch [213/300], Step [95/112], Loss: 0.6414\n",
      "Epoch [213/300], Step [99/112], Loss: 0.6503\n",
      "Epoch [213/300], Step [103/112], Loss: 0.6498\n",
      "Epoch [213/300], Step [107/112], Loss: 0.6439\n",
      "Epoch [213/300], Step [111/112], Loss: 0.5980\n",
      "Epoch [214/300], Step [3/112], Loss: 0.6519\n",
      "Epoch [214/300], Step [7/112], Loss: 0.6432\n",
      "Epoch [214/300], Step [11/112], Loss: 0.6405\n",
      "Epoch [214/300], Step [15/112], Loss: 0.6387\n",
      "Epoch [214/300], Step [19/112], Loss: 0.6291\n",
      "Epoch [214/300], Step [23/112], Loss: 0.6332\n",
      "Epoch [214/300], Step [27/112], Loss: 0.6063\n",
      "Epoch [214/300], Step [31/112], Loss: 0.6507\n",
      "Epoch [214/300], Step [35/112], Loss: 0.6239\n",
      "Epoch [214/300], Step [39/112], Loss: 0.6290\n",
      "Epoch [214/300], Step [43/112], Loss: 0.6293\n",
      "Epoch [214/300], Step [47/112], Loss: 0.6333\n",
      "Epoch [214/300], Step [51/112], Loss: 0.6346\n",
      "Epoch [214/300], Step [55/112], Loss: 0.6560\n",
      "Epoch [214/300], Step [59/112], Loss: 0.6225\n",
      "Epoch [214/300], Step [63/112], Loss: 0.6294\n",
      "Epoch [214/300], Step [67/112], Loss: 0.6357\n",
      "Epoch [214/300], Step [71/112], Loss: 0.6384\n",
      "Epoch [214/300], Step [75/112], Loss: 0.6547\n",
      "Epoch [214/300], Step [79/112], Loss: 0.6278\n",
      "Epoch [214/300], Step [83/112], Loss: 0.6587\n",
      "Epoch [214/300], Step [87/112], Loss: 0.6281\n",
      "Epoch [214/300], Step [91/112], Loss: 0.6625\n",
      "Epoch [214/300], Step [95/112], Loss: 0.6412\n",
      "Epoch [214/300], Step [99/112], Loss: 0.6471\n",
      "Epoch [214/300], Step [103/112], Loss: 0.6482\n",
      "Epoch [214/300], Step [107/112], Loss: 0.6414\n",
      "Epoch [214/300], Step [111/112], Loss: 0.6000\n",
      "Epoch [215/300], Step [3/112], Loss: 0.6623\n",
      "Epoch [215/300], Step [7/112], Loss: 0.6422\n",
      "Epoch [215/300], Step [11/112], Loss: 0.6398\n",
      "Epoch [215/300], Step [15/112], Loss: 0.6345\n",
      "Epoch [215/300], Step [19/112], Loss: 0.6339\n",
      "Epoch [215/300], Step [23/112], Loss: 0.6375\n",
      "Epoch [215/300], Step [27/112], Loss: 0.6121\n",
      "Epoch [215/300], Step [31/112], Loss: 0.6541\n",
      "Epoch [215/300], Step [35/112], Loss: 0.6197\n",
      "Epoch [215/300], Step [39/112], Loss: 0.6221\n",
      "Epoch [215/300], Step [43/112], Loss: 0.6345\n",
      "Epoch [215/300], Step [47/112], Loss: 0.6253\n",
      "Epoch [215/300], Step [51/112], Loss: 0.6273\n",
      "Epoch [215/300], Step [55/112], Loss: 0.6429\n",
      "Epoch [215/300], Step [59/112], Loss: 0.6224\n",
      "Epoch [215/300], Step [63/112], Loss: 0.6287\n",
      "Epoch [215/300], Step [67/112], Loss: 0.6214\n",
      "Epoch [215/300], Step [71/112], Loss: 0.6387\n",
      "Epoch [215/300], Step [75/112], Loss: 0.6457\n",
      "Epoch [215/300], Step [79/112], Loss: 0.6264\n",
      "Epoch [215/300], Step [83/112], Loss: 0.6562\n",
      "Epoch [215/300], Step [87/112], Loss: 0.6229\n",
      "Epoch [215/300], Step [91/112], Loss: 0.6571\n",
      "Epoch [215/300], Step [95/112], Loss: 0.6404\n",
      "Epoch [215/300], Step [99/112], Loss: 0.6549\n",
      "Epoch [215/300], Step [103/112], Loss: 0.6489\n",
      "Epoch [215/300], Step [107/112], Loss: 0.6418\n",
      "Epoch [215/300], Step [111/112], Loss: 0.6052\n",
      "Epoch [216/300], Step [3/112], Loss: 0.6478\n",
      "Epoch [216/300], Step [7/112], Loss: 0.6396\n",
      "Epoch [216/300], Step [11/112], Loss: 0.6402\n",
      "Epoch [216/300], Step [15/112], Loss: 0.6311\n",
      "Epoch [216/300], Step [19/112], Loss: 0.6329\n",
      "Epoch [216/300], Step [23/112], Loss: 0.6199\n",
      "Epoch [216/300], Step [27/112], Loss: 0.6084\n",
      "Epoch [216/300], Step [31/112], Loss: 0.6508\n",
      "Epoch [216/300], Step [35/112], Loss: 0.6183\n",
      "Epoch [216/300], Step [39/112], Loss: 0.6140\n",
      "Epoch [216/300], Step [43/112], Loss: 0.6317\n",
      "Epoch [216/300], Step [47/112], Loss: 0.6257\n",
      "Epoch [216/300], Step [51/112], Loss: 0.6221\n",
      "Epoch [216/300], Step [55/112], Loss: 0.6328\n",
      "Epoch [216/300], Step [59/112], Loss: 0.6214\n",
      "Epoch [216/300], Step [63/112], Loss: 0.6240\n",
      "Epoch [216/300], Step [67/112], Loss: 0.6272\n",
      "Epoch [216/300], Step [71/112], Loss: 0.6326\n",
      "Epoch [216/300], Step [75/112], Loss: 0.6460\n",
      "Epoch [216/300], Step [79/112], Loss: 0.6231\n",
      "Epoch [216/300], Step [83/112], Loss: 0.6568\n",
      "Epoch [216/300], Step [87/112], Loss: 0.6257\n",
      "Epoch [216/300], Step [91/112], Loss: 0.6582\n",
      "Epoch [216/300], Step [95/112], Loss: 0.6345\n",
      "Epoch [216/300], Step [99/112], Loss: 0.6445\n",
      "Epoch [216/300], Step [103/112], Loss: 0.6482\n",
      "Epoch [216/300], Step [107/112], Loss: 0.6411\n",
      "Epoch [216/300], Step [111/112], Loss: 0.5963\n",
      "Epoch [217/300], Step [3/112], Loss: 0.6532\n",
      "Epoch [217/300], Step [7/112], Loss: 0.6430\n",
      "Epoch [217/300], Step [11/112], Loss: 0.6336\n",
      "Epoch [217/300], Step [15/112], Loss: 0.6337\n",
      "Epoch [217/300], Step [19/112], Loss: 0.6280\n",
      "Epoch [217/300], Step [23/112], Loss: 0.6474\n",
      "Epoch [217/300], Step [27/112], Loss: 0.6095\n",
      "Epoch [217/300], Step [31/112], Loss: 0.6602\n",
      "Epoch [217/300], Step [35/112], Loss: 0.6244\n",
      "Epoch [217/300], Step [39/112], Loss: 0.6197\n",
      "Epoch [217/300], Step [43/112], Loss: 0.6254\n",
      "Epoch [217/300], Step [47/112], Loss: 0.6217\n",
      "Epoch [217/300], Step [51/112], Loss: 0.6203\n",
      "Epoch [217/300], Step [55/112], Loss: 0.6375\n",
      "Epoch [217/300], Step [59/112], Loss: 0.6298\n",
      "Epoch [217/300], Step [63/112], Loss: 0.6307\n",
      "Epoch [217/300], Step [67/112], Loss: 0.6299\n",
      "Epoch [217/300], Step [71/112], Loss: 0.6385\n",
      "Epoch [217/300], Step [75/112], Loss: 0.6487\n",
      "Epoch [217/300], Step [79/112], Loss: 0.6307\n",
      "Epoch [217/300], Step [83/112], Loss: 0.6578\n",
      "Epoch [217/300], Step [87/112], Loss: 0.6204\n",
      "Epoch [217/300], Step [91/112], Loss: 0.6585\n",
      "Epoch [217/300], Step [95/112], Loss: 0.6462\n",
      "Epoch [217/300], Step [99/112], Loss: 0.6513\n",
      "Epoch [217/300], Step [103/112], Loss: 0.6426\n",
      "Epoch [217/300], Step [107/112], Loss: 0.6374\n",
      "Epoch [217/300], Step [111/112], Loss: 0.5957\n",
      "Epoch [218/300], Step [3/112], Loss: 0.6499\n",
      "Epoch [218/300], Step [7/112], Loss: 0.6486\n",
      "Epoch [218/300], Step [11/112], Loss: 0.6396\n",
      "Epoch [218/300], Step [15/112], Loss: 0.6346\n",
      "Epoch [218/300], Step [19/112], Loss: 0.6249\n",
      "Epoch [218/300], Step [23/112], Loss: 0.6349\n",
      "Epoch [218/300], Step [27/112], Loss: 0.6031\n",
      "Epoch [218/300], Step [31/112], Loss: 0.6567\n",
      "Epoch [218/300], Step [35/112], Loss: 0.6202\n",
      "Epoch [218/300], Step [39/112], Loss: 0.6185\n",
      "Epoch [218/300], Step [43/112], Loss: 0.6301\n",
      "Epoch [218/300], Step [47/112], Loss: 0.6282\n",
      "Epoch [218/300], Step [51/112], Loss: 0.6217\n",
      "Epoch [218/300], Step [55/112], Loss: 0.6374\n",
      "Epoch [218/300], Step [59/112], Loss: 0.6211\n",
      "Epoch [218/300], Step [63/112], Loss: 0.6278\n",
      "Epoch [218/300], Step [67/112], Loss: 0.6249\n",
      "Epoch [218/300], Step [71/112], Loss: 0.6358\n",
      "Epoch [218/300], Step [75/112], Loss: 0.6523\n",
      "Epoch [218/300], Step [79/112], Loss: 0.6270\n",
      "Epoch [218/300], Step [83/112], Loss: 0.6562\n",
      "Epoch [218/300], Step [87/112], Loss: 0.6262\n",
      "Epoch [218/300], Step [91/112], Loss: 0.6570\n",
      "Epoch [218/300], Step [95/112], Loss: 0.6407\n",
      "Epoch [218/300], Step [99/112], Loss: 0.6478\n",
      "Epoch [218/300], Step [103/112], Loss: 0.6506\n",
      "Epoch [218/300], Step [107/112], Loss: 0.6365\n",
      "Epoch [218/300], Step [111/112], Loss: 0.5976\n",
      "Epoch [219/300], Step [3/112], Loss: 0.6416\n",
      "Epoch [219/300], Step [7/112], Loss: 0.6390\n",
      "Epoch [219/300], Step [11/112], Loss: 0.6308\n",
      "Epoch [219/300], Step [15/112], Loss: 0.6340\n",
      "Epoch [219/300], Step [19/112], Loss: 0.6303\n",
      "Epoch [219/300], Step [23/112], Loss: 0.6269\n",
      "Epoch [219/300], Step [27/112], Loss: 0.6066\n",
      "Epoch [219/300], Step [31/112], Loss: 0.6449\n",
      "Epoch [219/300], Step [35/112], Loss: 0.6164\n",
      "Epoch [219/300], Step [39/112], Loss: 0.6159\n",
      "Epoch [219/300], Step [43/112], Loss: 0.6251\n",
      "Epoch [219/300], Step [47/112], Loss: 0.6266\n",
      "Epoch [219/300], Step [51/112], Loss: 0.6265\n",
      "Epoch [219/300], Step [55/112], Loss: 0.6352\n",
      "Epoch [219/300], Step [59/112], Loss: 0.6221\n",
      "Epoch [219/300], Step [63/112], Loss: 0.6214\n",
      "Epoch [219/300], Step [67/112], Loss: 0.6388\n",
      "Epoch [219/300], Step [71/112], Loss: 0.6482\n",
      "Epoch [219/300], Step [75/112], Loss: 0.6463\n",
      "Epoch [219/300], Step [79/112], Loss: 0.6241\n",
      "Epoch [219/300], Step [83/112], Loss: 0.6547\n",
      "Epoch [219/300], Step [87/112], Loss: 0.6258\n",
      "Epoch [219/300], Step [91/112], Loss: 0.6541\n",
      "Epoch [219/300], Step [95/112], Loss: 0.6380\n",
      "Epoch [219/300], Step [99/112], Loss: 0.6421\n",
      "Epoch [219/300], Step [103/112], Loss: 0.6541\n",
      "Epoch [219/300], Step [107/112], Loss: 0.6317\n",
      "Epoch [219/300], Step [111/112], Loss: 0.6020\n",
      "Epoch [220/300], Step [3/112], Loss: 0.6560\n",
      "Epoch [220/300], Step [7/112], Loss: 0.6533\n",
      "Epoch [220/300], Step [11/112], Loss: 0.6396\n",
      "Epoch [220/300], Step [15/112], Loss: 0.6362\n",
      "Epoch [220/300], Step [19/112], Loss: 0.6288\n",
      "Epoch [220/300], Step [23/112], Loss: 0.6315\n",
      "Epoch [220/300], Step [27/112], Loss: 0.6022\n",
      "Epoch [220/300], Step [31/112], Loss: 0.6598\n",
      "Epoch [220/300], Step [35/112], Loss: 0.6195\n",
      "Epoch [220/300], Step [39/112], Loss: 0.6249\n",
      "Epoch [220/300], Step [43/112], Loss: 0.6325\n",
      "Epoch [220/300], Step [47/112], Loss: 0.6317\n",
      "Epoch [220/300], Step [51/112], Loss: 0.6210\n",
      "Epoch [220/300], Step [55/112], Loss: 0.6427\n",
      "Epoch [220/300], Step [59/112], Loss: 0.6262\n",
      "Epoch [220/300], Step [63/112], Loss: 0.6227\n",
      "Epoch [220/300], Step [67/112], Loss: 0.6238\n",
      "Epoch [220/300], Step [71/112], Loss: 0.6343\n",
      "Epoch [220/300], Step [75/112], Loss: 0.6430\n",
      "Epoch [220/300], Step [79/112], Loss: 0.6250\n",
      "Epoch [220/300], Step [83/112], Loss: 0.6605\n",
      "Epoch [220/300], Step [87/112], Loss: 0.6204\n",
      "Epoch [220/300], Step [91/112], Loss: 0.6556\n",
      "Epoch [220/300], Step [95/112], Loss: 0.6370\n",
      "Epoch [220/300], Step [99/112], Loss: 0.6467\n",
      "Epoch [220/300], Step [103/112], Loss: 0.6448\n",
      "Epoch [220/300], Step [107/112], Loss: 0.6346\n",
      "Epoch [220/300], Step [111/112], Loss: 0.5943\n",
      "Epoch [221/300], Step [3/112], Loss: 0.6530\n",
      "Epoch [221/300], Step [7/112], Loss: 0.6366\n",
      "Epoch [221/300], Step [11/112], Loss: 0.6314\n",
      "Epoch [221/300], Step [15/112], Loss: 0.6356\n",
      "Epoch [221/300], Step [19/112], Loss: 0.6223\n",
      "Epoch [221/300], Step [23/112], Loss: 0.6245\n",
      "Epoch [221/300], Step [27/112], Loss: 0.6006\n",
      "Epoch [221/300], Step [31/112], Loss: 0.6455\n",
      "Epoch [221/300], Step [35/112], Loss: 0.6203\n",
      "Epoch [221/300], Step [39/112], Loss: 0.6190\n",
      "Epoch [221/300], Step [43/112], Loss: 0.6347\n",
      "Epoch [221/300], Step [47/112], Loss: 0.6329\n",
      "Epoch [221/300], Step [51/112], Loss: 0.6236\n",
      "Epoch [221/300], Step [55/112], Loss: 0.6364\n",
      "Epoch [221/300], Step [59/112], Loss: 0.6262\n",
      "Epoch [221/300], Step [63/112], Loss: 0.6279\n",
      "Epoch [221/300], Step [67/112], Loss: 0.6204\n",
      "Epoch [221/300], Step [71/112], Loss: 0.6349\n",
      "Epoch [221/300], Step [75/112], Loss: 0.6485\n",
      "Epoch [221/300], Step [79/112], Loss: 0.6276\n",
      "Epoch [221/300], Step [83/112], Loss: 0.6598\n",
      "Epoch [221/300], Step [87/112], Loss: 0.6279\n",
      "Epoch [221/300], Step [91/112], Loss: 0.6518\n",
      "Epoch [221/300], Step [95/112], Loss: 0.6427\n",
      "Epoch [221/300], Step [99/112], Loss: 0.6475\n",
      "Epoch [221/300], Step [103/112], Loss: 0.6423\n",
      "Epoch [221/300], Step [107/112], Loss: 0.6425\n",
      "Epoch [221/300], Step [111/112], Loss: 0.6010\n",
      "Epoch [222/300], Step [3/112], Loss: 0.6496\n",
      "Epoch [222/300], Step [7/112], Loss: 0.6403\n",
      "Epoch [222/300], Step [11/112], Loss: 0.6326\n",
      "Epoch [222/300], Step [15/112], Loss: 0.6254\n",
      "Epoch [222/300], Step [19/112], Loss: 0.6220\n",
      "Epoch [222/300], Step [23/112], Loss: 0.6282\n",
      "Epoch [222/300], Step [27/112], Loss: 0.6110\n",
      "Epoch [222/300], Step [31/112], Loss: 0.6447\n",
      "Epoch [222/300], Step [35/112], Loss: 0.6149\n",
      "Epoch [222/300], Step [39/112], Loss: 0.6214\n",
      "Epoch [222/300], Step [43/112], Loss: 0.6273\n",
      "Epoch [222/300], Step [47/112], Loss: 0.6303\n",
      "Epoch [222/300], Step [51/112], Loss: 0.6217\n",
      "Epoch [222/300], Step [55/112], Loss: 0.6357\n",
      "Epoch [222/300], Step [59/112], Loss: 0.6180\n",
      "Epoch [222/300], Step [63/112], Loss: 0.6276\n",
      "Epoch [222/300], Step [67/112], Loss: 0.6191\n",
      "Epoch [222/300], Step [71/112], Loss: 0.6361\n",
      "Epoch [222/300], Step [75/112], Loss: 0.6451\n",
      "Epoch [222/300], Step [79/112], Loss: 0.6305\n",
      "Epoch [222/300], Step [83/112], Loss: 0.6563\n",
      "Epoch [222/300], Step [87/112], Loss: 0.6211\n",
      "Epoch [222/300], Step [91/112], Loss: 0.6498\n",
      "Epoch [222/300], Step [95/112], Loss: 0.6375\n",
      "Epoch [222/300], Step [99/112], Loss: 0.6441\n",
      "Epoch [222/300], Step [103/112], Loss: 0.6463\n",
      "Epoch [222/300], Step [107/112], Loss: 0.6325\n",
      "Epoch [222/300], Step [111/112], Loss: 0.5966\n",
      "Epoch [223/300], Step [3/112], Loss: 0.6541\n",
      "Epoch [223/300], Step [7/112], Loss: 0.6384\n",
      "Epoch [223/300], Step [11/112], Loss: 0.6387\n",
      "Epoch [223/300], Step [15/112], Loss: 0.6266\n",
      "Epoch [223/300], Step [19/112], Loss: 0.6233\n",
      "Epoch [223/300], Step [23/112], Loss: 0.6239\n",
      "Epoch [223/300], Step [27/112], Loss: 0.6019\n",
      "Epoch [223/300], Step [31/112], Loss: 0.6518\n",
      "Epoch [223/300], Step [35/112], Loss: 0.6137\n",
      "Epoch [223/300], Step [39/112], Loss: 0.6159\n",
      "Epoch [223/300], Step [43/112], Loss: 0.6243\n",
      "Epoch [223/300], Step [47/112], Loss: 0.6311\n",
      "Epoch [223/300], Step [51/112], Loss: 0.6261\n",
      "Epoch [223/300], Step [55/112], Loss: 0.6318\n",
      "Epoch [223/300], Step [59/112], Loss: 0.6166\n",
      "Epoch [223/300], Step [63/112], Loss: 0.6295\n",
      "Epoch [223/300], Step [67/112], Loss: 0.6278\n",
      "Epoch [223/300], Step [71/112], Loss: 0.6289\n",
      "Epoch [223/300], Step [75/112], Loss: 0.6409\n",
      "Epoch [223/300], Step [79/112], Loss: 0.6229\n",
      "Epoch [223/300], Step [83/112], Loss: 0.6502\n",
      "Epoch [223/300], Step [87/112], Loss: 0.6243\n",
      "Epoch [223/300], Step [91/112], Loss: 0.6641\n",
      "Epoch [223/300], Step [95/112], Loss: 0.6419\n",
      "Epoch [223/300], Step [99/112], Loss: 0.6457\n",
      "Epoch [223/300], Step [103/112], Loss: 0.6457\n",
      "Epoch [223/300], Step [107/112], Loss: 0.6367\n",
      "Epoch [223/300], Step [111/112], Loss: 0.5965\n",
      "Epoch [224/300], Step [3/112], Loss: 0.6517\n",
      "Epoch [224/300], Step [7/112], Loss: 0.6408\n",
      "Epoch [224/300], Step [11/112], Loss: 0.6320\n",
      "Epoch [224/300], Step [15/112], Loss: 0.6332\n",
      "Epoch [224/300], Step [19/112], Loss: 0.6299\n",
      "Epoch [224/300], Step [23/112], Loss: 0.6304\n",
      "Epoch [224/300], Step [27/112], Loss: 0.6045\n",
      "Epoch [224/300], Step [31/112], Loss: 0.6463\n",
      "Epoch [224/300], Step [35/112], Loss: 0.6217\n",
      "Epoch [224/300], Step [39/112], Loss: 0.6220\n",
      "Epoch [224/300], Step [43/112], Loss: 0.6229\n",
      "Epoch [224/300], Step [47/112], Loss: 0.6229\n",
      "Epoch [224/300], Step [51/112], Loss: 0.6244\n",
      "Epoch [224/300], Step [55/112], Loss: 0.6371\n",
      "Epoch [224/300], Step [59/112], Loss: 0.6169\n",
      "Epoch [224/300], Step [63/112], Loss: 0.6193\n",
      "Epoch [224/300], Step [67/112], Loss: 0.6207\n",
      "Epoch [224/300], Step [71/112], Loss: 0.6356\n",
      "Epoch [224/300], Step [75/112], Loss: 0.6430\n",
      "Epoch [224/300], Step [79/112], Loss: 0.6128\n",
      "Epoch [224/300], Step [83/112], Loss: 0.6606\n",
      "Epoch [224/300], Step [87/112], Loss: 0.6235\n",
      "Epoch [224/300], Step [91/112], Loss: 0.6558\n",
      "Epoch [224/300], Step [95/112], Loss: 0.6404\n",
      "Epoch [224/300], Step [99/112], Loss: 0.6435\n",
      "Epoch [224/300], Step [103/112], Loss: 0.6411\n",
      "Epoch [224/300], Step [107/112], Loss: 0.6374\n",
      "Epoch [224/300], Step [111/112], Loss: 0.5982\n",
      "Epoch [225/300], Step [3/112], Loss: 0.6478\n",
      "Epoch [225/300], Step [7/112], Loss: 0.6413\n",
      "Epoch [225/300], Step [11/112], Loss: 0.6289\n",
      "Epoch [225/300], Step [15/112], Loss: 0.6274\n",
      "Epoch [225/300], Step [19/112], Loss: 0.6243\n",
      "Epoch [225/300], Step [23/112], Loss: 0.6226\n",
      "Epoch [225/300], Step [27/112], Loss: 0.6010\n",
      "Epoch [225/300], Step [31/112], Loss: 0.6499\n",
      "Epoch [225/300], Step [35/112], Loss: 0.6155\n",
      "Epoch [225/300], Step [39/112], Loss: 0.6156\n",
      "Epoch [225/300], Step [43/112], Loss: 0.6240\n",
      "Epoch [225/300], Step [47/112], Loss: 0.6358\n",
      "Epoch [225/300], Step [51/112], Loss: 0.6198\n",
      "Epoch [225/300], Step [55/112], Loss: 0.6354\n",
      "Epoch [225/300], Step [59/112], Loss: 0.6184\n",
      "Epoch [225/300], Step [63/112], Loss: 0.6281\n",
      "Epoch [225/300], Step [67/112], Loss: 0.6238\n",
      "Epoch [225/300], Step [71/112], Loss: 0.6316\n",
      "Epoch [225/300], Step [75/112], Loss: 0.6497\n",
      "Epoch [225/300], Step [79/112], Loss: 0.6276\n",
      "Epoch [225/300], Step [83/112], Loss: 0.6595\n",
      "Epoch [225/300], Step [87/112], Loss: 0.6216\n",
      "Epoch [225/300], Step [91/112], Loss: 0.6590\n",
      "Epoch [225/300], Step [95/112], Loss: 0.6441\n",
      "Epoch [225/300], Step [99/112], Loss: 0.6438\n",
      "Epoch [225/300], Step [103/112], Loss: 0.6437\n",
      "Epoch [225/300], Step [107/112], Loss: 0.6383\n",
      "Epoch [225/300], Step [111/112], Loss: 0.5980\n",
      "Epoch [226/300], Step [3/112], Loss: 0.6494\n",
      "Epoch [226/300], Step [7/112], Loss: 0.6379\n",
      "Epoch [226/300], Step [11/112], Loss: 0.6315\n",
      "Epoch [226/300], Step [15/112], Loss: 0.6308\n",
      "Epoch [226/300], Step [19/112], Loss: 0.6190\n",
      "Epoch [226/300], Step [23/112], Loss: 0.6219\n",
      "Epoch [226/300], Step [27/112], Loss: 0.6044\n",
      "Epoch [226/300], Step [31/112], Loss: 0.6479\n",
      "Epoch [226/300], Step [35/112], Loss: 0.6174\n",
      "Epoch [226/300], Step [39/112], Loss: 0.6147\n",
      "Epoch [226/300], Step [43/112], Loss: 0.6281\n",
      "Epoch [226/300], Step [47/112], Loss: 0.6304\n",
      "Epoch [226/300], Step [51/112], Loss: 0.6234\n",
      "Epoch [226/300], Step [55/112], Loss: 0.6327\n",
      "Epoch [226/300], Step [59/112], Loss: 0.6138\n",
      "Epoch [226/300], Step [63/112], Loss: 0.6344\n",
      "Epoch [226/300], Step [67/112], Loss: 0.6305\n",
      "Epoch [226/300], Step [71/112], Loss: 0.6424\n",
      "Epoch [226/300], Step [75/112], Loss: 0.6420\n",
      "Epoch [226/300], Step [79/112], Loss: 0.6212\n",
      "Epoch [226/300], Step [83/112], Loss: 0.6602\n",
      "Epoch [226/300], Step [87/112], Loss: 0.6254\n",
      "Epoch [226/300], Step [91/112], Loss: 0.6548\n",
      "Epoch [226/300], Step [95/112], Loss: 0.6353\n",
      "Epoch [226/300], Step [99/112], Loss: 0.6426\n",
      "Epoch [226/300], Step [103/112], Loss: 0.6414\n",
      "Epoch [226/300], Step [107/112], Loss: 0.6337\n",
      "Epoch [226/300], Step [111/112], Loss: 0.5966\n",
      "Epoch [227/300], Step [3/112], Loss: 0.6518\n",
      "Epoch [227/300], Step [7/112], Loss: 0.6378\n",
      "Epoch [227/300], Step [11/112], Loss: 0.6378\n",
      "Epoch [227/300], Step [15/112], Loss: 0.6366\n",
      "Epoch [227/300], Step [19/112], Loss: 0.6242\n",
      "Epoch [227/300], Step [23/112], Loss: 0.6247\n",
      "Epoch [227/300], Step [27/112], Loss: 0.6091\n",
      "Epoch [227/300], Step [31/112], Loss: 0.6456\n",
      "Epoch [227/300], Step [35/112], Loss: 0.6176\n",
      "Epoch [227/300], Step [39/112], Loss: 0.6209\n",
      "Epoch [227/300], Step [43/112], Loss: 0.6292\n",
      "Epoch [227/300], Step [47/112], Loss: 0.6281\n",
      "Epoch [227/300], Step [51/112], Loss: 0.6179\n",
      "Epoch [227/300], Step [55/112], Loss: 0.6316\n",
      "Epoch [227/300], Step [59/112], Loss: 0.6243\n",
      "Epoch [227/300], Step [63/112], Loss: 0.6301\n",
      "Epoch [227/300], Step [67/112], Loss: 0.6230\n",
      "Epoch [227/300], Step [71/112], Loss: 0.6362\n",
      "Epoch [227/300], Step [75/112], Loss: 0.6498\n",
      "Epoch [227/300], Step [79/112], Loss: 0.6323\n",
      "Epoch [227/300], Step [83/112], Loss: 0.6644\n",
      "Epoch [227/300], Step [87/112], Loss: 0.6261\n",
      "Epoch [227/300], Step [91/112], Loss: 0.6601\n",
      "Epoch [227/300], Step [95/112], Loss: 0.6485\n",
      "Epoch [227/300], Step [99/112], Loss: 0.6488\n",
      "Epoch [227/300], Step [103/112], Loss: 0.6449\n",
      "Epoch [227/300], Step [107/112], Loss: 0.6345\n",
      "Epoch [227/300], Step [111/112], Loss: 0.5960\n",
      "Epoch [228/300], Step [3/112], Loss: 0.6449\n",
      "Epoch [228/300], Step [7/112], Loss: 0.6345\n",
      "Epoch [228/300], Step [11/112], Loss: 0.6318\n",
      "Epoch [228/300], Step [15/112], Loss: 0.6391\n",
      "Epoch [228/300], Step [19/112], Loss: 0.6346\n",
      "Epoch [228/300], Step [23/112], Loss: 0.6284\n",
      "Epoch [228/300], Step [27/112], Loss: 0.6140\n",
      "Epoch [228/300], Step [31/112], Loss: 0.6500\n",
      "Epoch [228/300], Step [35/112], Loss: 0.6135\n",
      "Epoch [228/300], Step [39/112], Loss: 0.6260\n",
      "Epoch [228/300], Step [43/112], Loss: 0.6249\n",
      "Epoch [228/300], Step [47/112], Loss: 0.6420\n",
      "Epoch [228/300], Step [51/112], Loss: 0.6276\n",
      "Epoch [228/300], Step [55/112], Loss: 0.6333\n",
      "Epoch [228/300], Step [59/112], Loss: 0.6160\n",
      "Epoch [228/300], Step [63/112], Loss: 0.6259\n",
      "Epoch [228/300], Step [67/112], Loss: 0.6288\n",
      "Epoch [228/300], Step [71/112], Loss: 0.6416\n",
      "Epoch [228/300], Step [75/112], Loss: 0.6517\n",
      "Epoch [228/300], Step [79/112], Loss: 0.6216\n",
      "Epoch [228/300], Step [83/112], Loss: 0.6597\n",
      "Epoch [228/300], Step [87/112], Loss: 0.6232\n",
      "Epoch [228/300], Step [91/112], Loss: 0.6602\n",
      "Epoch [228/300], Step [95/112], Loss: 0.6476\n",
      "Epoch [228/300], Step [99/112], Loss: 0.6484\n",
      "Epoch [228/300], Step [103/112], Loss: 0.6412\n",
      "Epoch [228/300], Step [107/112], Loss: 0.6379\n",
      "Epoch [228/300], Step [111/112], Loss: 0.6005\n",
      "Epoch [229/300], Step [3/112], Loss: 0.6455\n",
      "Epoch [229/300], Step [7/112], Loss: 0.6377\n",
      "Epoch [229/300], Step [11/112], Loss: 0.6272\n",
      "Epoch [229/300], Step [15/112], Loss: 0.6295\n",
      "Epoch [229/300], Step [19/112], Loss: 0.6338\n",
      "Epoch [229/300], Step [23/112], Loss: 0.6382\n",
      "Epoch [229/300], Step [27/112], Loss: 0.6074\n",
      "Epoch [229/300], Step [31/112], Loss: 0.6492\n",
      "Epoch [229/300], Step [35/112], Loss: 0.6144\n",
      "Epoch [229/300], Step [39/112], Loss: 0.6196\n",
      "Epoch [229/300], Step [43/112], Loss: 0.6302\n",
      "Epoch [229/300], Step [47/112], Loss: 0.6303\n",
      "Epoch [229/300], Step [51/112], Loss: 0.6290\n",
      "Epoch [229/300], Step [55/112], Loss: 0.6321\n",
      "Epoch [229/300], Step [59/112], Loss: 0.6238\n",
      "Epoch [229/300], Step [63/112], Loss: 0.6228\n",
      "Epoch [229/300], Step [67/112], Loss: 0.6299\n",
      "Epoch [229/300], Step [71/112], Loss: 0.6403\n",
      "Epoch [229/300], Step [75/112], Loss: 0.6438\n",
      "Epoch [229/300], Step [79/112], Loss: 0.6228\n",
      "Epoch [229/300], Step [83/112], Loss: 0.6572\n",
      "Epoch [229/300], Step [87/112], Loss: 0.6200\n",
      "Epoch [229/300], Step [91/112], Loss: 0.6609\n",
      "Epoch [229/300], Step [95/112], Loss: 0.6473\n",
      "Epoch [229/300], Step [99/112], Loss: 0.6408\n",
      "Epoch [229/300], Step [103/112], Loss: 0.6456\n",
      "Epoch [229/300], Step [107/112], Loss: 0.6273\n",
      "Epoch [229/300], Step [111/112], Loss: 0.5959\n",
      "Epoch [230/300], Step [3/112], Loss: 0.6489\n",
      "Epoch [230/300], Step [7/112], Loss: 0.6324\n",
      "Epoch [230/300], Step [11/112], Loss: 0.6347\n",
      "Epoch [230/300], Step [15/112], Loss: 0.6283\n",
      "Epoch [230/300], Step [19/112], Loss: 0.6234\n",
      "Epoch [230/300], Step [23/112], Loss: 0.6262\n",
      "Epoch [230/300], Step [27/112], Loss: 0.6045\n",
      "Epoch [230/300], Step [31/112], Loss: 0.6465\n",
      "Epoch [230/300], Step [35/112], Loss: 0.6202\n",
      "Epoch [230/300], Step [39/112], Loss: 0.6159\n",
      "Epoch [230/300], Step [43/112], Loss: 0.6266\n",
      "Epoch [230/300], Step [47/112], Loss: 0.6328\n",
      "Epoch [230/300], Step [51/112], Loss: 0.6281\n",
      "Epoch [230/300], Step [55/112], Loss: 0.6369\n",
      "Epoch [230/300], Step [59/112], Loss: 0.6199\n",
      "Epoch [230/300], Step [63/112], Loss: 0.6232\n",
      "Epoch [230/300], Step [67/112], Loss: 0.6241\n",
      "Epoch [230/300], Step [71/112], Loss: 0.6343\n",
      "Epoch [230/300], Step [75/112], Loss: 0.6433\n",
      "Epoch [230/300], Step [79/112], Loss: 0.6211\n",
      "Epoch [230/300], Step [83/112], Loss: 0.6619\n",
      "Epoch [230/300], Step [87/112], Loss: 0.6250\n",
      "Epoch [230/300], Step [91/112], Loss: 0.6503\n",
      "Epoch [230/300], Step [95/112], Loss: 0.6373\n",
      "Epoch [230/300], Step [99/112], Loss: 0.6449\n",
      "Epoch [230/300], Step [103/112], Loss: 0.6394\n",
      "Epoch [230/300], Step [107/112], Loss: 0.6308\n",
      "Epoch [230/300], Step [111/112], Loss: 0.5922\n",
      "Epoch [231/300], Step [3/112], Loss: 0.6495\n",
      "Epoch [231/300], Step [7/112], Loss: 0.6402\n",
      "Epoch [231/300], Step [11/112], Loss: 0.6404\n",
      "Epoch [231/300], Step [15/112], Loss: 0.6306\n",
      "Epoch [231/300], Step [19/112], Loss: 0.6188\n",
      "Epoch [231/300], Step [23/112], Loss: 0.6237\n",
      "Epoch [231/300], Step [27/112], Loss: 0.5964\n",
      "Epoch [231/300], Step [31/112], Loss: 0.6424\n",
      "Epoch [231/300], Step [35/112], Loss: 0.6122\n",
      "Epoch [231/300], Step [39/112], Loss: 0.6141\n",
      "Epoch [231/300], Step [43/112], Loss: 0.6258\n",
      "Epoch [231/300], Step [47/112], Loss: 0.6317\n",
      "Epoch [231/300], Step [51/112], Loss: 0.6229\n",
      "Epoch [231/300], Step [55/112], Loss: 0.6342\n",
      "Epoch [231/300], Step [59/112], Loss: 0.6180\n",
      "Epoch [231/300], Step [63/112], Loss: 0.6232\n",
      "Epoch [231/300], Step [67/112], Loss: 0.6226\n",
      "Epoch [231/300], Step [71/112], Loss: 0.6330\n",
      "Epoch [231/300], Step [75/112], Loss: 0.6454\n",
      "Epoch [231/300], Step [79/112], Loss: 0.6209\n",
      "Epoch [231/300], Step [83/112], Loss: 0.6518\n",
      "Epoch [231/300], Step [87/112], Loss: 0.6112\n",
      "Epoch [231/300], Step [91/112], Loss: 0.6587\n",
      "Epoch [231/300], Step [95/112], Loss: 0.6440\n",
      "Epoch [231/300], Step [99/112], Loss: 0.6392\n",
      "Epoch [231/300], Step [103/112], Loss: 0.6427\n",
      "Epoch [231/300], Step [107/112], Loss: 0.6352\n",
      "Epoch [231/300], Step [111/112], Loss: 0.5885\n",
      "Epoch [232/300], Step [3/112], Loss: 0.6441\n",
      "Epoch [232/300], Step [7/112], Loss: 0.6362\n",
      "Epoch [232/300], Step [11/112], Loss: 0.6335\n",
      "Epoch [232/300], Step [15/112], Loss: 0.6248\n",
      "Epoch [232/300], Step [19/112], Loss: 0.6209\n",
      "Epoch [232/300], Step [23/112], Loss: 0.6324\n",
      "Epoch [232/300], Step [27/112], Loss: 0.6067\n",
      "Epoch [232/300], Step [31/112], Loss: 0.6469\n",
      "Epoch [232/300], Step [35/112], Loss: 0.6121\n",
      "Epoch [232/300], Step [39/112], Loss: 0.6285\n",
      "Epoch [232/300], Step [43/112], Loss: 0.6221\n",
      "Epoch [232/300], Step [47/112], Loss: 0.6282\n",
      "Epoch [232/300], Step [51/112], Loss: 0.6218\n",
      "Epoch [232/300], Step [55/112], Loss: 0.6393\n",
      "Epoch [232/300], Step [59/112], Loss: 0.6271\n",
      "Epoch [232/300], Step [63/112], Loss: 0.6304\n",
      "Epoch [232/300], Step [67/112], Loss: 0.6238\n",
      "Epoch [232/300], Step [71/112], Loss: 0.6428\n",
      "Epoch [232/300], Step [75/112], Loss: 0.6369\n",
      "Epoch [232/300], Step [79/112], Loss: 0.6272\n",
      "Epoch [232/300], Step [83/112], Loss: 0.6609\n",
      "Epoch [232/300], Step [87/112], Loss: 0.6195\n",
      "Epoch [232/300], Step [91/112], Loss: 0.6537\n",
      "Epoch [232/300], Step [95/112], Loss: 0.6324\n",
      "Epoch [232/300], Step [99/112], Loss: 0.6467\n",
      "Epoch [232/300], Step [103/112], Loss: 0.6434\n",
      "Epoch [232/300], Step [107/112], Loss: 0.6360\n",
      "Epoch [232/300], Step [111/112], Loss: 0.6015\n",
      "Epoch [233/300], Step [3/112], Loss: 0.6581\n",
      "Epoch [233/300], Step [7/112], Loss: 0.6382\n",
      "Epoch [233/300], Step [11/112], Loss: 0.6313\n",
      "Epoch [233/300], Step [15/112], Loss: 0.6324\n",
      "Epoch [233/300], Step [19/112], Loss: 0.6171\n",
      "Epoch [233/300], Step [23/112], Loss: 0.6294\n",
      "Epoch [233/300], Step [27/112], Loss: 0.5996\n",
      "Epoch [233/300], Step [31/112], Loss: 0.6497\n",
      "Epoch [233/300], Step [35/112], Loss: 0.6108\n",
      "Epoch [233/300], Step [39/112], Loss: 0.6158\n",
      "Epoch [233/300], Step [43/112], Loss: 0.6203\n",
      "Epoch [233/300], Step [47/112], Loss: 0.6258\n",
      "Epoch [233/300], Step [51/112], Loss: 0.6224\n",
      "Epoch [233/300], Step [55/112], Loss: 0.6345\n",
      "Epoch [233/300], Step [59/112], Loss: 0.6268\n",
      "Epoch [233/300], Step [63/112], Loss: 0.6193\n",
      "Epoch [233/300], Step [67/112], Loss: 0.6192\n",
      "Epoch [233/300], Step [71/112], Loss: 0.6475\n",
      "Epoch [233/300], Step [75/112], Loss: 0.6389\n",
      "Epoch [233/300], Step [79/112], Loss: 0.6166\n",
      "Epoch [233/300], Step [83/112], Loss: 0.6568\n",
      "Epoch [233/300], Step [87/112], Loss: 0.6226\n",
      "Epoch [233/300], Step [91/112], Loss: 0.6585\n",
      "Epoch [233/300], Step [95/112], Loss: 0.6373\n",
      "Epoch [233/300], Step [99/112], Loss: 0.6375\n",
      "Epoch [233/300], Step [103/112], Loss: 0.6434\n",
      "Epoch [233/300], Step [107/112], Loss: 0.6344\n",
      "Epoch [233/300], Step [111/112], Loss: 0.6029\n",
      "Epoch [234/300], Step [3/112], Loss: 0.6558\n",
      "Epoch [234/300], Step [7/112], Loss: 0.6379\n",
      "Epoch [234/300], Step [11/112], Loss: 0.6387\n",
      "Epoch [234/300], Step [15/112], Loss: 0.6387\n",
      "Epoch [234/300], Step [19/112], Loss: 0.6307\n",
      "Epoch [234/300], Step [23/112], Loss: 0.6251\n",
      "Epoch [234/300], Step [27/112], Loss: 0.6101\n",
      "Epoch [234/300], Step [31/112], Loss: 0.6549\n",
      "Epoch [234/300], Step [35/112], Loss: 0.6220\n",
      "Epoch [234/300], Step [39/112], Loss: 0.6248\n",
      "Epoch [234/300], Step [43/112], Loss: 0.6206\n",
      "Epoch [234/300], Step [47/112], Loss: 0.6316\n",
      "Epoch [234/300], Step [51/112], Loss: 0.6185\n",
      "Epoch [234/300], Step [55/112], Loss: 0.6410\n",
      "Epoch [234/300], Step [59/112], Loss: 0.6205\n",
      "Epoch [234/300], Step [63/112], Loss: 0.6246\n",
      "Epoch [234/300], Step [67/112], Loss: 0.6279\n",
      "Epoch [234/300], Step [71/112], Loss: 0.6414\n",
      "Epoch [234/300], Step [75/112], Loss: 0.6448\n",
      "Epoch [234/300], Step [79/112], Loss: 0.6261\n",
      "Epoch [234/300], Step [83/112], Loss: 0.6552\n",
      "Epoch [234/300], Step [87/112], Loss: 0.6231\n",
      "Epoch [234/300], Step [91/112], Loss: 0.6528\n",
      "Epoch [234/300], Step [95/112], Loss: 0.6387\n",
      "Epoch [234/300], Step [99/112], Loss: 0.6428\n",
      "Epoch [234/300], Step [103/112], Loss: 0.6378\n",
      "Epoch [234/300], Step [107/112], Loss: 0.6380\n",
      "Epoch [234/300], Step [111/112], Loss: 0.5972\n",
      "Epoch [235/300], Step [3/112], Loss: 0.6610\n",
      "Epoch [235/300], Step [7/112], Loss: 0.6412\n",
      "Epoch [235/300], Step [11/112], Loss: 0.6417\n",
      "Epoch [235/300], Step [15/112], Loss: 0.6357\n",
      "Epoch [235/300], Step [19/112], Loss: 0.6275\n",
      "Epoch [235/300], Step [23/112], Loss: 0.6299\n",
      "Epoch [235/300], Step [27/112], Loss: 0.6045\n",
      "Epoch [235/300], Step [31/112], Loss: 0.6530\n",
      "Epoch [235/300], Step [35/112], Loss: 0.6132\n",
      "Epoch [235/300], Step [39/112], Loss: 0.6238\n",
      "Epoch [235/300], Step [43/112], Loss: 0.6274\n",
      "Epoch [235/300], Step [47/112], Loss: 0.6329\n",
      "Epoch [235/300], Step [51/112], Loss: 0.6222\n",
      "Epoch [235/300], Step [55/112], Loss: 0.6407\n",
      "Epoch [235/300], Step [59/112], Loss: 0.6319\n",
      "Epoch [235/300], Step [63/112], Loss: 0.6344\n",
      "Epoch [235/300], Step [67/112], Loss: 0.6318\n",
      "Epoch [235/300], Step [71/112], Loss: 0.6360\n",
      "Epoch [235/300], Step [75/112], Loss: 0.6510\n",
      "Epoch [235/300], Step [79/112], Loss: 0.6290\n",
      "Epoch [235/300], Step [83/112], Loss: 0.6619\n",
      "Epoch [235/300], Step [87/112], Loss: 0.6179\n",
      "Epoch [235/300], Step [91/112], Loss: 0.6577\n",
      "Epoch [235/300], Step [95/112], Loss: 0.6375\n",
      "Epoch [235/300], Step [99/112], Loss: 0.6454\n",
      "Epoch [235/300], Step [103/112], Loss: 0.6374\n",
      "Epoch [235/300], Step [107/112], Loss: 0.6354\n",
      "Epoch [235/300], Step [111/112], Loss: 0.6046\n",
      "Epoch [236/300], Step [3/112], Loss: 0.6469\n",
      "Epoch [236/300], Step [7/112], Loss: 0.6391\n",
      "Epoch [236/300], Step [11/112], Loss: 0.6324\n",
      "Epoch [236/300], Step [15/112], Loss: 0.6318\n",
      "Epoch [236/300], Step [19/112], Loss: 0.6227\n",
      "Epoch [236/300], Step [23/112], Loss: 0.6280\n",
      "Epoch [236/300], Step [27/112], Loss: 0.5982\n",
      "Epoch [236/300], Step [31/112], Loss: 0.6434\n",
      "Epoch [236/300], Step [35/112], Loss: 0.6144\n",
      "Epoch [236/300], Step [39/112], Loss: 0.6217\n",
      "Epoch [236/300], Step [43/112], Loss: 0.6276\n",
      "Epoch [236/300], Step [47/112], Loss: 0.6293\n",
      "Epoch [236/300], Step [51/112], Loss: 0.6277\n",
      "Epoch [236/300], Step [55/112], Loss: 0.6296\n",
      "Epoch [236/300], Step [59/112], Loss: 0.6215\n",
      "Epoch [236/300], Step [63/112], Loss: 0.6180\n",
      "Epoch [236/300], Step [67/112], Loss: 0.6267\n",
      "Epoch [236/300], Step [71/112], Loss: 0.6341\n",
      "Epoch [236/300], Step [75/112], Loss: 0.6492\n",
      "Epoch [236/300], Step [79/112], Loss: 0.6224\n",
      "Epoch [236/300], Step [83/112], Loss: 0.6521\n",
      "Epoch [236/300], Step [87/112], Loss: 0.6235\n",
      "Epoch [236/300], Step [91/112], Loss: 0.6641\n",
      "Epoch [236/300], Step [95/112], Loss: 0.6430\n",
      "Epoch [236/300], Step [99/112], Loss: 0.6501\n",
      "Epoch [236/300], Step [103/112], Loss: 0.6421\n",
      "Epoch [236/300], Step [107/112], Loss: 0.6363\n",
      "Epoch [236/300], Step [111/112], Loss: 0.5998\n",
      "Epoch [237/300], Step [3/112], Loss: 0.6447\n",
      "Epoch [237/300], Step [7/112], Loss: 0.6376\n",
      "Epoch [237/300], Step [11/112], Loss: 0.6414\n",
      "Epoch [237/300], Step [15/112], Loss: 0.6298\n",
      "Epoch [237/300], Step [19/112], Loss: 0.6302\n",
      "Epoch [237/300], Step [23/112], Loss: 0.6430\n",
      "Epoch [237/300], Step [27/112], Loss: 0.6044\n",
      "Epoch [237/300], Step [31/112], Loss: 0.6523\n",
      "Epoch [237/300], Step [35/112], Loss: 0.6318\n",
      "Epoch [237/300], Step [39/112], Loss: 0.6230\n",
      "Epoch [237/300], Step [43/112], Loss: 0.6336\n",
      "Epoch [237/300], Step [47/112], Loss: 0.6285\n",
      "Epoch [237/300], Step [51/112], Loss: 0.6228\n",
      "Epoch [237/300], Step [55/112], Loss: 0.6389\n",
      "Epoch [237/300], Step [59/112], Loss: 0.6274\n",
      "Epoch [237/300], Step [63/112], Loss: 0.6338\n",
      "Epoch [237/300], Step [67/112], Loss: 0.6292\n",
      "Epoch [237/300], Step [71/112], Loss: 0.6427\n",
      "Epoch [237/300], Step [75/112], Loss: 0.6436\n",
      "Epoch [237/300], Step [79/112], Loss: 0.6320\n",
      "Epoch [237/300], Step [83/112], Loss: 0.6607\n",
      "Epoch [237/300], Step [87/112], Loss: 0.6199\n",
      "Epoch [237/300], Step [91/112], Loss: 0.6488\n",
      "Epoch [237/300], Step [95/112], Loss: 0.6422\n",
      "Epoch [237/300], Step [99/112], Loss: 0.6426\n",
      "Epoch [237/300], Step [103/112], Loss: 0.6434\n",
      "Epoch [237/300], Step [107/112], Loss: 0.6331\n",
      "Epoch [237/300], Step [111/112], Loss: 0.5952\n",
      "Epoch [238/300], Step [3/112], Loss: 0.6551\n",
      "Epoch [238/300], Step [7/112], Loss: 0.6355\n",
      "Epoch [238/300], Step [11/112], Loss: 0.6338\n",
      "Epoch [238/300], Step [15/112], Loss: 0.6349\n",
      "Epoch [238/300], Step [19/112], Loss: 0.6264\n",
      "Epoch [238/300], Step [23/112], Loss: 0.6336\n",
      "Epoch [238/300], Step [27/112], Loss: 0.6079\n",
      "Epoch [238/300], Step [31/112], Loss: 0.6507\n",
      "Epoch [238/300], Step [35/112], Loss: 0.6221\n",
      "Epoch [238/300], Step [39/112], Loss: 0.6204\n",
      "Epoch [238/300], Step [43/112], Loss: 0.6315\n",
      "Epoch [238/300], Step [47/112], Loss: 0.6328\n",
      "Epoch [238/300], Step [51/112], Loss: 0.6217\n",
      "Epoch [238/300], Step [55/112], Loss: 0.6475\n",
      "Epoch [238/300], Step [59/112], Loss: 0.6295\n",
      "Epoch [238/300], Step [63/112], Loss: 0.6236\n",
      "Epoch [238/300], Step [67/112], Loss: 0.6268\n",
      "Epoch [238/300], Step [71/112], Loss: 0.6310\n",
      "Epoch [238/300], Step [75/112], Loss: 0.6364\n",
      "Epoch [238/300], Step [79/112], Loss: 0.6310\n",
      "Epoch [238/300], Step [83/112], Loss: 0.6621\n",
      "Epoch [238/300], Step [87/112], Loss: 0.6183\n",
      "Epoch [238/300], Step [91/112], Loss: 0.6522\n",
      "Epoch [238/300], Step [95/112], Loss: 0.6411\n",
      "Epoch [238/300], Step [99/112], Loss: 0.6462\n",
      "Epoch [238/300], Step [103/112], Loss: 0.6404\n",
      "Epoch [238/300], Step [107/112], Loss: 0.6352\n",
      "Epoch [238/300], Step [111/112], Loss: 0.5906\n",
      "Epoch [239/300], Step [3/112], Loss: 0.6455\n",
      "Epoch [239/300], Step [7/112], Loss: 0.6397\n",
      "Epoch [239/300], Step [11/112], Loss: 0.6305\n",
      "Epoch [239/300], Step [15/112], Loss: 0.6286\n",
      "Epoch [239/300], Step [19/112], Loss: 0.6201\n",
      "Epoch [239/300], Step [23/112], Loss: 0.6319\n",
      "Epoch [239/300], Step [27/112], Loss: 0.6043\n",
      "Epoch [239/300], Step [31/112], Loss: 0.6517\n",
      "Epoch [239/300], Step [35/112], Loss: 0.6143\n",
      "Epoch [239/300], Step [39/112], Loss: 0.6176\n",
      "Epoch [239/300], Step [43/112], Loss: 0.6329\n",
      "Epoch [239/300], Step [47/112], Loss: 0.6263\n",
      "Epoch [239/300], Step [51/112], Loss: 0.6175\n",
      "Epoch [239/300], Step [55/112], Loss: 0.6378\n",
      "Epoch [239/300], Step [59/112], Loss: 0.6192\n",
      "Epoch [239/300], Step [63/112], Loss: 0.6273\n",
      "Epoch [239/300], Step [67/112], Loss: 0.6330\n",
      "Epoch [239/300], Step [71/112], Loss: 0.6378\n",
      "Epoch [239/300], Step [75/112], Loss: 0.6343\n",
      "Epoch [239/300], Step [79/112], Loss: 0.6294\n",
      "Epoch [239/300], Step [83/112], Loss: 0.6649\n",
      "Epoch [239/300], Step [87/112], Loss: 0.6224\n",
      "Epoch [239/300], Step [91/112], Loss: 0.6578\n",
      "Epoch [239/300], Step [95/112], Loss: 0.6475\n",
      "Epoch [239/300], Step [99/112], Loss: 0.6520\n",
      "Epoch [239/300], Step [103/112], Loss: 0.6331\n",
      "Epoch [239/300], Step [107/112], Loss: 0.6372\n",
      "Epoch [239/300], Step [111/112], Loss: 0.5921\n",
      "Epoch [240/300], Step [3/112], Loss: 0.6477\n",
      "Epoch [240/300], Step [7/112], Loss: 0.6385\n",
      "Epoch [240/300], Step [11/112], Loss: 0.6274\n",
      "Epoch [240/300], Step [15/112], Loss: 0.6320\n",
      "Epoch [240/300], Step [19/112], Loss: 0.6186\n",
      "Epoch [240/300], Step [23/112], Loss: 0.6304\n",
      "Epoch [240/300], Step [27/112], Loss: 0.6037\n",
      "Epoch [240/300], Step [31/112], Loss: 0.6451\n",
      "Epoch [240/300], Step [35/112], Loss: 0.6113\n",
      "Epoch [240/300], Step [39/112], Loss: 0.6221\n",
      "Epoch [240/300], Step [43/112], Loss: 0.6212\n",
      "Epoch [240/300], Step [47/112], Loss: 0.6305\n",
      "Epoch [240/300], Step [51/112], Loss: 0.6218\n",
      "Epoch [240/300], Step [55/112], Loss: 0.6287\n",
      "Epoch [240/300], Step [59/112], Loss: 0.6148\n",
      "Epoch [240/300], Step [63/112], Loss: 0.6216\n",
      "Epoch [240/300], Step [67/112], Loss: 0.6187\n",
      "Epoch [240/300], Step [71/112], Loss: 0.6278\n",
      "Epoch [240/300], Step [75/112], Loss: 0.6361\n",
      "Epoch [240/300], Step [79/112], Loss: 0.6231\n",
      "Epoch [240/300], Step [83/112], Loss: 0.6500\n",
      "Epoch [240/300], Step [87/112], Loss: 0.6195\n",
      "Epoch [240/300], Step [91/112], Loss: 0.6553\n",
      "Epoch [240/300], Step [95/112], Loss: 0.6473\n",
      "Epoch [240/300], Step [99/112], Loss: 0.6455\n",
      "Epoch [240/300], Step [103/112], Loss: 0.6383\n",
      "Epoch [240/300], Step [107/112], Loss: 0.6341\n",
      "Epoch [240/300], Step [111/112], Loss: 0.5962\n",
      "Epoch [241/300], Step [3/112], Loss: 0.6523\n",
      "Epoch [241/300], Step [7/112], Loss: 0.6370\n",
      "Epoch [241/300], Step [11/112], Loss: 0.6261\n",
      "Epoch [241/300], Step [15/112], Loss: 0.6353\n",
      "Epoch [241/300], Step [19/112], Loss: 0.6161\n",
      "Epoch [241/300], Step [23/112], Loss: 0.6220\n",
      "Epoch [241/300], Step [27/112], Loss: 0.6052\n",
      "Epoch [241/300], Step [31/112], Loss: 0.6457\n",
      "Epoch [241/300], Step [35/112], Loss: 0.6150\n",
      "Epoch [241/300], Step [39/112], Loss: 0.6174\n",
      "Epoch [241/300], Step [43/112], Loss: 0.6292\n",
      "Epoch [241/300], Step [47/112], Loss: 0.6244\n",
      "Epoch [241/300], Step [51/112], Loss: 0.6140\n",
      "Epoch [241/300], Step [55/112], Loss: 0.6293\n",
      "Epoch [241/300], Step [59/112], Loss: 0.6272\n",
      "Epoch [241/300], Step [63/112], Loss: 0.6193\n",
      "Epoch [241/300], Step [67/112], Loss: 0.6270\n",
      "Epoch [241/300], Step [71/112], Loss: 0.6305\n",
      "Epoch [241/300], Step [75/112], Loss: 0.6422\n",
      "Epoch [241/300], Step [79/112], Loss: 0.6219\n",
      "Epoch [241/300], Step [83/112], Loss: 0.6548\n",
      "Epoch [241/300], Step [87/112], Loss: 0.6178\n",
      "Epoch [241/300], Step [91/112], Loss: 0.6603\n",
      "Epoch [241/300], Step [95/112], Loss: 0.6389\n",
      "Epoch [241/300], Step [99/112], Loss: 0.6433\n",
      "Epoch [241/300], Step [103/112], Loss: 0.6400\n",
      "Epoch [241/300], Step [107/112], Loss: 0.6339\n",
      "Epoch [241/300], Step [111/112], Loss: 0.5905\n",
      "Epoch [242/300], Step [3/112], Loss: 0.6566\n",
      "Epoch [242/300], Step [7/112], Loss: 0.6379\n",
      "Epoch [242/300], Step [11/112], Loss: 0.6234\n",
      "Epoch [242/300], Step [15/112], Loss: 0.6312\n",
      "Epoch [242/300], Step [19/112], Loss: 0.6157\n",
      "Epoch [242/300], Step [23/112], Loss: 0.6243\n",
      "Epoch [242/300], Step [27/112], Loss: 0.6051\n",
      "Epoch [242/300], Step [31/112], Loss: 0.6413\n",
      "Epoch [242/300], Step [35/112], Loss: 0.6189\n",
      "Epoch [242/300], Step [39/112], Loss: 0.6239\n",
      "Epoch [242/300], Step [43/112], Loss: 0.6325\n",
      "Epoch [242/300], Step [47/112], Loss: 0.6226\n",
      "Epoch [242/300], Step [51/112], Loss: 0.6135\n",
      "Epoch [242/300], Step [55/112], Loss: 0.6270\n",
      "Epoch [242/300], Step [59/112], Loss: 0.6210\n",
      "Epoch [242/300], Step [63/112], Loss: 0.6208\n",
      "Epoch [242/300], Step [67/112], Loss: 0.6238\n",
      "Epoch [242/300], Step [71/112], Loss: 0.6292\n",
      "Epoch [242/300], Step [75/112], Loss: 0.6391\n",
      "Epoch [242/300], Step [79/112], Loss: 0.6254\n",
      "Epoch [242/300], Step [83/112], Loss: 0.6576\n",
      "Epoch [242/300], Step [87/112], Loss: 0.6204\n",
      "Epoch [242/300], Step [91/112], Loss: 0.6590\n",
      "Epoch [242/300], Step [95/112], Loss: 0.6418\n",
      "Epoch [242/300], Step [99/112], Loss: 0.6493\n",
      "Epoch [242/300], Step [103/112], Loss: 0.6372\n",
      "Epoch [242/300], Step [107/112], Loss: 0.6319\n",
      "Epoch [242/300], Step [111/112], Loss: 0.5966\n",
      "Epoch [243/300], Step [3/112], Loss: 0.6442\n",
      "Epoch [243/300], Step [7/112], Loss: 0.6381\n",
      "Epoch [243/300], Step [11/112], Loss: 0.6323\n",
      "Epoch [243/300], Step [15/112], Loss: 0.6330\n",
      "Epoch [243/300], Step [19/112], Loss: 0.6233\n",
      "Epoch [243/300], Step [23/112], Loss: 0.6347\n",
      "Epoch [243/300], Step [27/112], Loss: 0.6040\n",
      "Epoch [243/300], Step [31/112], Loss: 0.6534\n",
      "Epoch [243/300], Step [35/112], Loss: 0.6248\n",
      "Epoch [243/300], Step [39/112], Loss: 0.6253\n",
      "Epoch [243/300], Step [43/112], Loss: 0.6255\n",
      "Epoch [243/300], Step [47/112], Loss: 0.6244\n",
      "Epoch [243/300], Step [51/112], Loss: 0.6362\n",
      "Epoch [243/300], Step [55/112], Loss: 0.6328\n",
      "Epoch [243/300], Step [59/112], Loss: 0.6315\n",
      "Epoch [243/300], Step [63/112], Loss: 0.6253\n",
      "Epoch [243/300], Step [67/112], Loss: 0.6314\n",
      "Epoch [243/300], Step [71/112], Loss: 0.6286\n",
      "Epoch [243/300], Step [75/112], Loss: 0.6360\n",
      "Epoch [243/300], Step [79/112], Loss: 0.6262\n",
      "Epoch [243/300], Step [83/112], Loss: 0.6602\n",
      "Epoch [243/300], Step [87/112], Loss: 0.6180\n",
      "Epoch [243/300], Step [91/112], Loss: 0.6619\n",
      "Epoch [243/300], Step [95/112], Loss: 0.6453\n",
      "Epoch [243/300], Step [99/112], Loss: 0.6546\n",
      "Epoch [243/300], Step [103/112], Loss: 0.6402\n",
      "Epoch [243/300], Step [107/112], Loss: 0.6337\n",
      "Epoch [243/300], Step [111/112], Loss: 0.6045\n",
      "Epoch [244/300], Step [3/112], Loss: 0.6558\n",
      "Epoch [244/300], Step [7/112], Loss: 0.6382\n",
      "Epoch [244/300], Step [11/112], Loss: 0.6314\n",
      "Epoch [244/300], Step [15/112], Loss: 0.6338\n",
      "Epoch [244/300], Step [19/112], Loss: 0.6206\n",
      "Epoch [244/300], Step [23/112], Loss: 0.6324\n",
      "Epoch [244/300], Step [27/112], Loss: 0.6024\n",
      "Epoch [244/300], Step [31/112], Loss: 0.6518\n",
      "Epoch [244/300], Step [35/112], Loss: 0.6165\n",
      "Epoch [244/300], Step [39/112], Loss: 0.6234\n",
      "Epoch [244/300], Step [43/112], Loss: 0.6351\n",
      "Epoch [244/300], Step [47/112], Loss: 0.6269\n",
      "Epoch [244/300], Step [51/112], Loss: 0.6188\n",
      "Epoch [244/300], Step [55/112], Loss: 0.6298\n",
      "Epoch [244/300], Step [59/112], Loss: 0.6163\n",
      "Epoch [244/300], Step [63/112], Loss: 0.6151\n",
      "Epoch [244/300], Step [67/112], Loss: 0.6300\n",
      "Epoch [244/300], Step [71/112], Loss: 0.6280\n",
      "Epoch [244/300], Step [75/112], Loss: 0.6397\n",
      "Epoch [244/300], Step [79/112], Loss: 0.6280\n",
      "Epoch [244/300], Step [83/112], Loss: 0.6532\n",
      "Epoch [244/300], Step [87/112], Loss: 0.6211\n",
      "Epoch [244/300], Step [91/112], Loss: 0.6658\n",
      "Epoch [244/300], Step [95/112], Loss: 0.6535\n",
      "Epoch [244/300], Step [99/112], Loss: 0.6523\n",
      "Epoch [244/300], Step [103/112], Loss: 0.6371\n",
      "Epoch [244/300], Step [107/112], Loss: 0.6314\n",
      "Epoch [244/300], Step [111/112], Loss: 0.5966\n",
      "Epoch [245/300], Step [3/112], Loss: 0.6570\n",
      "Epoch [245/300], Step [7/112], Loss: 0.6415\n",
      "Epoch [245/300], Step [11/112], Loss: 0.6289\n",
      "Epoch [245/300], Step [15/112], Loss: 0.6369\n",
      "Epoch [245/300], Step [19/112], Loss: 0.6295\n",
      "Epoch [245/300], Step [23/112], Loss: 0.6358\n",
      "Epoch [245/300], Step [27/112], Loss: 0.6093\n",
      "Epoch [245/300], Step [31/112], Loss: 0.6469\n",
      "Epoch [245/300], Step [35/112], Loss: 0.6237\n",
      "Epoch [245/300], Step [39/112], Loss: 0.6231\n",
      "Epoch [245/300], Step [43/112], Loss: 0.6342\n",
      "Epoch [245/300], Step [47/112], Loss: 0.6317\n",
      "Epoch [245/300], Step [51/112], Loss: 0.6171\n",
      "Epoch [245/300], Step [55/112], Loss: 0.6498\n",
      "Epoch [245/300], Step [59/112], Loss: 0.6247\n",
      "Epoch [245/300], Step [63/112], Loss: 0.6283\n",
      "Epoch [245/300], Step [67/112], Loss: 0.6266\n",
      "Epoch [245/300], Step [71/112], Loss: 0.6379\n",
      "Epoch [245/300], Step [75/112], Loss: 0.6403\n",
      "Epoch [245/300], Step [79/112], Loss: 0.6341\n",
      "Epoch [245/300], Step [83/112], Loss: 0.6591\n",
      "Epoch [245/300], Step [87/112], Loss: 0.6211\n",
      "Epoch [245/300], Step [91/112], Loss: 0.6637\n",
      "Epoch [245/300], Step [95/112], Loss: 0.6427\n",
      "Epoch [245/300], Step [99/112], Loss: 0.6514\n",
      "Epoch [245/300], Step [103/112], Loss: 0.6397\n",
      "Epoch [245/300], Step [107/112], Loss: 0.6347\n",
      "Epoch [245/300], Step [111/112], Loss: 0.6014\n",
      "Epoch [246/300], Step [3/112], Loss: 0.6425\n",
      "Epoch [246/300], Step [7/112], Loss: 0.6486\n",
      "Epoch [246/300], Step [11/112], Loss: 0.6340\n",
      "Epoch [246/300], Step [15/112], Loss: 0.6479\n",
      "Epoch [246/300], Step [19/112], Loss: 0.6274\n",
      "Epoch [246/300], Step [23/112], Loss: 0.6315\n",
      "Epoch [246/300], Step [27/112], Loss: 0.6187\n",
      "Epoch [246/300], Step [31/112], Loss: 0.6529\n",
      "Epoch [246/300], Step [35/112], Loss: 0.6281\n",
      "Epoch [246/300], Step [39/112], Loss: 0.6252\n",
      "Epoch [246/300], Step [43/112], Loss: 0.6341\n",
      "Epoch [246/300], Step [47/112], Loss: 0.6332\n",
      "Epoch [246/300], Step [51/112], Loss: 0.6254\n",
      "Epoch [246/300], Step [55/112], Loss: 0.6358\n",
      "Epoch [246/300], Step [59/112], Loss: 0.6176\n",
      "Epoch [246/300], Step [63/112], Loss: 0.6188\n",
      "Epoch [246/300], Step [67/112], Loss: 0.6307\n",
      "Epoch [246/300], Step [71/112], Loss: 0.6367\n",
      "Epoch [246/300], Step [75/112], Loss: 0.6449\n",
      "Epoch [246/300], Step [79/112], Loss: 0.6330\n",
      "Epoch [246/300], Step [83/112], Loss: 0.6581\n",
      "Epoch [246/300], Step [87/112], Loss: 0.6210\n",
      "Epoch [246/300], Step [91/112], Loss: 0.6629\n",
      "Epoch [246/300], Step [95/112], Loss: 0.6453\n",
      "Epoch [246/300], Step [99/112], Loss: 0.6433\n",
      "Epoch [246/300], Step [103/112], Loss: 0.6499\n",
      "Epoch [246/300], Step [107/112], Loss: 0.6397\n",
      "Epoch [246/300], Step [111/112], Loss: 0.5970\n",
      "Epoch [247/300], Step [3/112], Loss: 0.6432\n",
      "Epoch [247/300], Step [7/112], Loss: 0.6514\n",
      "Epoch [247/300], Step [11/112], Loss: 0.6399\n",
      "Epoch [247/300], Step [15/112], Loss: 0.6357\n",
      "Epoch [247/300], Step [19/112], Loss: 0.6289\n",
      "Epoch [247/300], Step [23/112], Loss: 0.6327\n",
      "Epoch [247/300], Step [27/112], Loss: 0.6082\n",
      "Epoch [247/300], Step [31/112], Loss: 0.6505\n",
      "Epoch [247/300], Step [35/112], Loss: 0.6244\n",
      "Epoch [247/300], Step [39/112], Loss: 0.6165\n",
      "Epoch [247/300], Step [43/112], Loss: 0.6298\n",
      "Epoch [247/300], Step [47/112], Loss: 0.6302\n",
      "Epoch [247/300], Step [51/112], Loss: 0.6238\n",
      "Epoch [247/300], Step [55/112], Loss: 0.6356\n",
      "Epoch [247/300], Step [59/112], Loss: 0.6335\n",
      "Epoch [247/300], Step [63/112], Loss: 0.6264\n",
      "Epoch [247/300], Step [67/112], Loss: 0.6254\n",
      "Epoch [247/300], Step [71/112], Loss: 0.6387\n",
      "Epoch [247/300], Step [75/112], Loss: 0.6383\n",
      "Epoch [247/300], Step [79/112], Loss: 0.6316\n",
      "Epoch [247/300], Step [83/112], Loss: 0.6523\n",
      "Epoch [247/300], Step [87/112], Loss: 0.6197\n",
      "Epoch [247/300], Step [91/112], Loss: 0.6593\n",
      "Epoch [247/300], Step [95/112], Loss: 0.6407\n",
      "Epoch [247/300], Step [99/112], Loss: 0.6491\n",
      "Epoch [247/300], Step [103/112], Loss: 0.6462\n",
      "Epoch [247/300], Step [107/112], Loss: 0.6318\n",
      "Epoch [247/300], Step [111/112], Loss: 0.6013\n",
      "Epoch [248/300], Step [3/112], Loss: 0.6484\n",
      "Epoch [248/300], Step [7/112], Loss: 0.6359\n",
      "Epoch [248/300], Step [11/112], Loss: 0.6304\n",
      "Epoch [248/300], Step [15/112], Loss: 0.6336\n",
      "Epoch [248/300], Step [19/112], Loss: 0.6191\n",
      "Epoch [248/300], Step [23/112], Loss: 0.6176\n",
      "Epoch [248/300], Step [27/112], Loss: 0.6042\n",
      "Epoch [248/300], Step [31/112], Loss: 0.6497\n",
      "Epoch [248/300], Step [35/112], Loss: 0.6238\n",
      "Epoch [248/300], Step [39/112], Loss: 0.6122\n",
      "Epoch [248/300], Step [43/112], Loss: 0.6406\n",
      "Epoch [248/300], Step [47/112], Loss: 0.6265\n",
      "Epoch [248/300], Step [51/112], Loss: 0.6243\n",
      "Epoch [248/300], Step [55/112], Loss: 0.6251\n",
      "Epoch [248/300], Step [59/112], Loss: 0.6259\n",
      "Epoch [248/300], Step [63/112], Loss: 0.6375\n",
      "Epoch [248/300], Step [67/112], Loss: 0.6346\n",
      "Epoch [248/300], Step [71/112], Loss: 0.6320\n",
      "Epoch [248/300], Step [75/112], Loss: 0.6392\n",
      "Epoch [248/300], Step [79/112], Loss: 0.6228\n",
      "Epoch [248/300], Step [83/112], Loss: 0.6593\n",
      "Epoch [248/300], Step [87/112], Loss: 0.6117\n",
      "Epoch [248/300], Step [91/112], Loss: 0.6566\n",
      "Epoch [248/300], Step [95/112], Loss: 0.6415\n",
      "Epoch [248/300], Step [99/112], Loss: 0.6404\n",
      "Epoch [248/300], Step [103/112], Loss: 0.6369\n",
      "Epoch [248/300], Step [107/112], Loss: 0.6305\n",
      "Epoch [248/300], Step [111/112], Loss: 0.6008\n",
      "Epoch [249/300], Step [3/112], Loss: 0.6481\n",
      "Epoch [249/300], Step [7/112], Loss: 0.6407\n",
      "Epoch [249/300], Step [11/112], Loss: 0.6373\n",
      "Epoch [249/300], Step [15/112], Loss: 0.6340\n",
      "Epoch [249/300], Step [19/112], Loss: 0.6193\n",
      "Epoch [249/300], Step [23/112], Loss: 0.6290\n",
      "Epoch [249/300], Step [27/112], Loss: 0.6051\n",
      "Epoch [249/300], Step [31/112], Loss: 0.6487\n",
      "Epoch [249/300], Step [35/112], Loss: 0.6252\n",
      "Epoch [249/300], Step [39/112], Loss: 0.6166\n",
      "Epoch [249/300], Step [43/112], Loss: 0.6408\n",
      "Epoch [249/300], Step [47/112], Loss: 0.6259\n",
      "Epoch [249/300], Step [51/112], Loss: 0.6201\n",
      "Epoch [249/300], Step [55/112], Loss: 0.6317\n",
      "Epoch [249/300], Step [59/112], Loss: 0.6228\n",
      "Epoch [249/300], Step [63/112], Loss: 0.6200\n",
      "Epoch [249/300], Step [67/112], Loss: 0.6296\n",
      "Epoch [249/300], Step [71/112], Loss: 0.6251\n",
      "Epoch [249/300], Step [75/112], Loss: 0.6370\n",
      "Epoch [249/300], Step [79/112], Loss: 0.6219\n",
      "Epoch [249/300], Step [83/112], Loss: 0.6576\n",
      "Epoch [249/300], Step [87/112], Loss: 0.6181\n",
      "Epoch [249/300], Step [91/112], Loss: 0.6579\n",
      "Epoch [249/300], Step [95/112], Loss: 0.6395\n",
      "Epoch [249/300], Step [99/112], Loss: 0.6440\n",
      "Epoch [249/300], Step [103/112], Loss: 0.6376\n",
      "Epoch [249/300], Step [107/112], Loss: 0.6383\n",
      "Epoch [249/300], Step [111/112], Loss: 0.5901\n",
      "Epoch [250/300], Step [3/112], Loss: 0.6420\n",
      "Epoch [250/300], Step [7/112], Loss: 0.6431\n",
      "Epoch [250/300], Step [11/112], Loss: 0.6363\n",
      "Epoch [250/300], Step [15/112], Loss: 0.6260\n",
      "Epoch [250/300], Step [19/112], Loss: 0.6250\n",
      "Epoch [250/300], Step [23/112], Loss: 0.6245\n",
      "Epoch [250/300], Step [27/112], Loss: 0.6035\n",
      "Epoch [250/300], Step [31/112], Loss: 0.6495\n",
      "Epoch [250/300], Step [35/112], Loss: 0.6268\n",
      "Epoch [250/300], Step [39/112], Loss: 0.6156\n",
      "Epoch [250/300], Step [43/112], Loss: 0.6211\n",
      "Epoch [250/300], Step [47/112], Loss: 0.6252\n",
      "Epoch [250/300], Step [51/112], Loss: 0.6194\n",
      "Epoch [250/300], Step [55/112], Loss: 0.6291\n",
      "Epoch [250/300], Step [59/112], Loss: 0.6178\n",
      "Epoch [250/300], Step [63/112], Loss: 0.6190\n",
      "Epoch [250/300], Step [67/112], Loss: 0.6276\n",
      "Epoch [250/300], Step [71/112], Loss: 0.6309\n",
      "Epoch [250/300], Step [75/112], Loss: 0.6408\n",
      "Epoch [250/300], Step [79/112], Loss: 0.6256\n",
      "Epoch [250/300], Step [83/112], Loss: 0.6576\n",
      "Epoch [250/300], Step [87/112], Loss: 0.6122\n",
      "Epoch [250/300], Step [91/112], Loss: 0.6538\n",
      "Epoch [250/300], Step [95/112], Loss: 0.6311\n",
      "Epoch [250/300], Step [99/112], Loss: 0.6464\n",
      "Epoch [250/300], Step [103/112], Loss: 0.6352\n",
      "Epoch [250/300], Step [107/112], Loss: 0.6338\n",
      "Epoch [250/300], Step [111/112], Loss: 0.5900\n",
      "Epoch [251/300], Step [3/112], Loss: 0.6436\n",
      "Epoch [251/300], Step [7/112], Loss: 0.6329\n",
      "Epoch [251/300], Step [11/112], Loss: 0.6250\n",
      "Epoch [251/300], Step [15/112], Loss: 0.6339\n",
      "Epoch [251/300], Step [19/112], Loss: 0.6226\n",
      "Epoch [251/300], Step [23/112], Loss: 0.6333\n",
      "Epoch [251/300], Step [27/112], Loss: 0.6073\n",
      "Epoch [251/300], Step [31/112], Loss: 0.6459\n",
      "Epoch [251/300], Step [35/112], Loss: 0.6233\n",
      "Epoch [251/300], Step [39/112], Loss: 0.6242\n",
      "Epoch [251/300], Step [43/112], Loss: 0.6305\n",
      "Epoch [251/300], Step [47/112], Loss: 0.6215\n",
      "Epoch [251/300], Step [51/112], Loss: 0.6188\n",
      "Epoch [251/300], Step [55/112], Loss: 0.6265\n",
      "Epoch [251/300], Step [59/112], Loss: 0.6246\n",
      "Epoch [251/300], Step [63/112], Loss: 0.6221\n",
      "Epoch [251/300], Step [67/112], Loss: 0.6306\n",
      "Epoch [251/300], Step [71/112], Loss: 0.6281\n",
      "Epoch [251/300], Step [75/112], Loss: 0.6460\n",
      "Epoch [251/300], Step [79/112], Loss: 0.6261\n",
      "Epoch [251/300], Step [83/112], Loss: 0.6570\n",
      "Epoch [251/300], Step [87/112], Loss: 0.6137\n",
      "Epoch [251/300], Step [91/112], Loss: 0.6549\n",
      "Epoch [251/300], Step [95/112], Loss: 0.6426\n",
      "Epoch [251/300], Step [99/112], Loss: 0.6403\n",
      "Epoch [251/300], Step [103/112], Loss: 0.6419\n",
      "Epoch [251/300], Step [107/112], Loss: 0.6332\n",
      "Epoch [251/300], Step [111/112], Loss: 0.5931\n",
      "Epoch [252/300], Step [3/112], Loss: 0.6500\n",
      "Epoch [252/300], Step [7/112], Loss: 0.6386\n",
      "Epoch [252/300], Step [11/112], Loss: 0.6272\n",
      "Epoch [252/300], Step [15/112], Loss: 0.6318\n",
      "Epoch [252/300], Step [19/112], Loss: 0.6247\n",
      "Epoch [252/300], Step [23/112], Loss: 0.6297\n",
      "Epoch [252/300], Step [27/112], Loss: 0.6076\n",
      "Epoch [252/300], Step [31/112], Loss: 0.6479\n",
      "Epoch [252/300], Step [35/112], Loss: 0.6167\n",
      "Epoch [252/300], Step [39/112], Loss: 0.6278\n",
      "Epoch [252/300], Step [43/112], Loss: 0.6316\n",
      "Epoch [252/300], Step [47/112], Loss: 0.6262\n",
      "Epoch [252/300], Step [51/112], Loss: 0.6157\n",
      "Epoch [252/300], Step [55/112], Loss: 0.6354\n",
      "Epoch [252/300], Step [59/112], Loss: 0.6258\n",
      "Epoch [252/300], Step [63/112], Loss: 0.6301\n",
      "Epoch [252/300], Step [67/112], Loss: 0.6184\n",
      "Epoch [252/300], Step [71/112], Loss: 0.6350\n",
      "Epoch [252/300], Step [75/112], Loss: 0.6310\n",
      "Epoch [252/300], Step [79/112], Loss: 0.6373\n",
      "Epoch [252/300], Step [83/112], Loss: 0.6601\n",
      "Epoch [252/300], Step [87/112], Loss: 0.6267\n",
      "Epoch [252/300], Step [91/112], Loss: 0.6693\n",
      "Epoch [252/300], Step [95/112], Loss: 0.6557\n",
      "Epoch [252/300], Step [99/112], Loss: 0.6516\n",
      "Epoch [252/300], Step [103/112], Loss: 0.6531\n",
      "Epoch [252/300], Step [107/112], Loss: 0.6411\n",
      "Epoch [252/300], Step [111/112], Loss: 0.5960\n",
      "Epoch [253/300], Step [3/112], Loss: 0.6429\n",
      "Epoch [253/300], Step [7/112], Loss: 0.6479\n",
      "Epoch [253/300], Step [11/112], Loss: 0.6365\n",
      "Epoch [253/300], Step [15/112], Loss: 0.6388\n",
      "Epoch [253/300], Step [19/112], Loss: 0.6243\n",
      "Epoch [253/300], Step [23/112], Loss: 0.6426\n",
      "Epoch [253/300], Step [27/112], Loss: 0.6060\n",
      "Epoch [253/300], Step [31/112], Loss: 0.6582\n",
      "Epoch [253/300], Step [35/112], Loss: 0.6254\n",
      "Epoch [253/300], Step [39/112], Loss: 0.6361\n",
      "Epoch [253/300], Step [43/112], Loss: 0.6314\n",
      "Epoch [253/300], Step [47/112], Loss: 0.6281\n",
      "Epoch [253/300], Step [51/112], Loss: 0.6243\n",
      "Epoch [253/300], Step [55/112], Loss: 0.6372\n",
      "Epoch [253/300], Step [59/112], Loss: 0.6181\n",
      "Epoch [253/300], Step [63/112], Loss: 0.6290\n",
      "Epoch [253/300], Step [67/112], Loss: 0.6252\n",
      "Epoch [253/300], Step [71/112], Loss: 0.6291\n",
      "Epoch [253/300], Step [75/112], Loss: 0.6352\n",
      "Epoch [253/300], Step [79/112], Loss: 0.6290\n",
      "Epoch [253/300], Step [83/112], Loss: 0.6553\n",
      "Epoch [253/300], Step [87/112], Loss: 0.6173\n",
      "Epoch [253/300], Step [91/112], Loss: 0.6568\n",
      "Epoch [253/300], Step [95/112], Loss: 0.6496\n",
      "Epoch [253/300], Step [99/112], Loss: 0.6405\n",
      "Epoch [253/300], Step [103/112], Loss: 0.6364\n",
      "Epoch [253/300], Step [107/112], Loss: 0.6364\n",
      "Epoch [253/300], Step [111/112], Loss: 0.5894\n",
      "Epoch [254/300], Step [3/112], Loss: 0.6359\n",
      "Epoch [254/300], Step [7/112], Loss: 0.6319\n",
      "Epoch [254/300], Step [11/112], Loss: 0.6266\n",
      "Epoch [254/300], Step [15/112], Loss: 0.6365\n",
      "Epoch [254/300], Step [19/112], Loss: 0.6215\n",
      "Epoch [254/300], Step [23/112], Loss: 0.6367\n",
      "Epoch [254/300], Step [27/112], Loss: 0.6036\n",
      "Epoch [254/300], Step [31/112], Loss: 0.6436\n",
      "Epoch [254/300], Step [35/112], Loss: 0.6223\n",
      "Epoch [254/300], Step [39/112], Loss: 0.6207\n",
      "Epoch [254/300], Step [43/112], Loss: 0.6311\n",
      "Epoch [254/300], Step [47/112], Loss: 0.6301\n",
      "Epoch [254/300], Step [51/112], Loss: 0.6281\n",
      "Epoch [254/300], Step [55/112], Loss: 0.6420\n",
      "Epoch [254/300], Step [59/112], Loss: 0.6206\n",
      "Epoch [254/300], Step [63/112], Loss: 0.6259\n",
      "Epoch [254/300], Step [67/112], Loss: 0.6274\n",
      "Epoch [254/300], Step [71/112], Loss: 0.6309\n",
      "Epoch [254/300], Step [75/112], Loss: 0.6432\n",
      "Epoch [254/300], Step [79/112], Loss: 0.6224\n",
      "Epoch [254/300], Step [83/112], Loss: 0.6526\n",
      "Epoch [254/300], Step [87/112], Loss: 0.6174\n",
      "Epoch [254/300], Step [91/112], Loss: 0.6531\n",
      "Epoch [254/300], Step [95/112], Loss: 0.6399\n",
      "Epoch [254/300], Step [99/112], Loss: 0.6469\n",
      "Epoch [254/300], Step [103/112], Loss: 0.6328\n",
      "Epoch [254/300], Step [107/112], Loss: 0.6381\n",
      "Epoch [254/300], Step [111/112], Loss: 0.6011\n",
      "Epoch [255/300], Step [3/112], Loss: 0.6402\n",
      "Epoch [255/300], Step [7/112], Loss: 0.6372\n",
      "Epoch [255/300], Step [11/112], Loss: 0.6226\n",
      "Epoch [255/300], Step [15/112], Loss: 0.6362\n",
      "Epoch [255/300], Step [19/112], Loss: 0.6362\n",
      "Epoch [255/300], Step [23/112], Loss: 0.6349\n",
      "Epoch [255/300], Step [27/112], Loss: 0.6032\n",
      "Epoch [255/300], Step [31/112], Loss: 0.6476\n",
      "Epoch [255/300], Step [35/112], Loss: 0.6186\n",
      "Epoch [255/300], Step [39/112], Loss: 0.6169\n",
      "Epoch [255/300], Step [43/112], Loss: 0.6282\n",
      "Epoch [255/300], Step [47/112], Loss: 0.6220\n",
      "Epoch [255/300], Step [51/112], Loss: 0.6136\n",
      "Epoch [255/300], Step [55/112], Loss: 0.6369\n",
      "Epoch [255/300], Step [59/112], Loss: 0.6274\n",
      "Epoch [255/300], Step [63/112], Loss: 0.6248\n",
      "Epoch [255/300], Step [67/112], Loss: 0.6269\n",
      "Epoch [255/300], Step [71/112], Loss: 0.6321\n",
      "Epoch [255/300], Step [75/112], Loss: 0.6415\n",
      "Epoch [255/300], Step [79/112], Loss: 0.6255\n",
      "Epoch [255/300], Step [83/112], Loss: 0.6469\n",
      "Epoch [255/300], Step [87/112], Loss: 0.6197\n",
      "Epoch [255/300], Step [91/112], Loss: 0.6539\n",
      "Epoch [255/300], Step [95/112], Loss: 0.6374\n",
      "Epoch [255/300], Step [99/112], Loss: 0.6384\n",
      "Epoch [255/300], Step [103/112], Loss: 0.6451\n",
      "Epoch [255/300], Step [107/112], Loss: 0.6330\n",
      "Epoch [255/300], Step [111/112], Loss: 0.5853\n",
      "Epoch [256/300], Step [3/112], Loss: 0.6417\n",
      "Epoch [256/300], Step [7/112], Loss: 0.6343\n",
      "Epoch [256/300], Step [11/112], Loss: 0.6268\n",
      "Epoch [256/300], Step [15/112], Loss: 0.6320\n",
      "Epoch [256/300], Step [19/112], Loss: 0.6279\n",
      "Epoch [256/300], Step [23/112], Loss: 0.6314\n",
      "Epoch [256/300], Step [27/112], Loss: 0.6028\n",
      "Epoch [256/300], Step [31/112], Loss: 0.6487\n",
      "Epoch [256/300], Step [35/112], Loss: 0.6167\n",
      "Epoch [256/300], Step [39/112], Loss: 0.6168\n",
      "Epoch [256/300], Step [43/112], Loss: 0.6198\n",
      "Epoch [256/300], Step [47/112], Loss: 0.6254\n",
      "Epoch [256/300], Step [51/112], Loss: 0.6156\n",
      "Epoch [256/300], Step [55/112], Loss: 0.6333\n",
      "Epoch [256/300], Step [59/112], Loss: 0.6242\n",
      "Epoch [256/300], Step [63/112], Loss: 0.6198\n",
      "Epoch [256/300], Step [67/112], Loss: 0.6153\n",
      "Epoch [256/300], Step [71/112], Loss: 0.6243\n",
      "Epoch [256/300], Step [75/112], Loss: 0.6336\n",
      "Epoch [256/300], Step [79/112], Loss: 0.6277\n",
      "Epoch [256/300], Step [83/112], Loss: 0.6467\n",
      "Epoch [256/300], Step [87/112], Loss: 0.6128\n",
      "Epoch [256/300], Step [91/112], Loss: 0.6542\n",
      "Epoch [256/300], Step [95/112], Loss: 0.6495\n",
      "Epoch [256/300], Step [99/112], Loss: 0.6400\n",
      "Epoch [256/300], Step [103/112], Loss: 0.6401\n",
      "Epoch [256/300], Step [107/112], Loss: 0.6364\n",
      "Epoch [256/300], Step [111/112], Loss: 0.5903\n",
      "Epoch [257/300], Step [3/112], Loss: 0.6341\n",
      "Epoch [257/300], Step [7/112], Loss: 0.6316\n",
      "Epoch [257/300], Step [11/112], Loss: 0.6208\n",
      "Epoch [257/300], Step [15/112], Loss: 0.6250\n",
      "Epoch [257/300], Step [19/112], Loss: 0.6184\n",
      "Epoch [257/300], Step [23/112], Loss: 0.6249\n",
      "Epoch [257/300], Step [27/112], Loss: 0.6074\n",
      "Epoch [257/300], Step [31/112], Loss: 0.6431\n",
      "Epoch [257/300], Step [35/112], Loss: 0.6218\n",
      "Epoch [257/300], Step [39/112], Loss: 0.6160\n",
      "Epoch [257/300], Step [43/112], Loss: 0.6349\n",
      "Epoch [257/300], Step [47/112], Loss: 0.6187\n",
      "Epoch [257/300], Step [51/112], Loss: 0.6205\n",
      "Epoch [257/300], Step [55/112], Loss: 0.6338\n",
      "Epoch [257/300], Step [59/112], Loss: 0.6274\n",
      "Epoch [257/300], Step [63/112], Loss: 0.6292\n",
      "Epoch [257/300], Step [67/112], Loss: 0.6235\n",
      "Epoch [257/300], Step [71/112], Loss: 0.6254\n",
      "Epoch [257/300], Step [75/112], Loss: 0.6352\n",
      "Epoch [257/300], Step [79/112], Loss: 0.6259\n",
      "Epoch [257/300], Step [83/112], Loss: 0.6528\n",
      "Epoch [257/300], Step [87/112], Loss: 0.6170\n",
      "Epoch [257/300], Step [91/112], Loss: 0.6584\n",
      "Epoch [257/300], Step [95/112], Loss: 0.6401\n",
      "Epoch [257/300], Step [99/112], Loss: 0.6425\n",
      "Epoch [257/300], Step [103/112], Loss: 0.6448\n",
      "Epoch [257/300], Step [107/112], Loss: 0.6295\n",
      "Epoch [257/300], Step [111/112], Loss: 0.5916\n",
      "Epoch [258/300], Step [3/112], Loss: 0.6450\n",
      "Epoch [258/300], Step [7/112], Loss: 0.6382\n",
      "Epoch [258/300], Step [11/112], Loss: 0.6325\n",
      "Epoch [258/300], Step [15/112], Loss: 0.6344\n",
      "Epoch [258/300], Step [19/112], Loss: 0.6251\n",
      "Epoch [258/300], Step [23/112], Loss: 0.6297\n",
      "Epoch [258/300], Step [27/112], Loss: 0.6071\n",
      "Epoch [258/300], Step [31/112], Loss: 0.6459\n",
      "Epoch [258/300], Step [35/112], Loss: 0.6147\n",
      "Epoch [258/300], Step [39/112], Loss: 0.6214\n",
      "Epoch [258/300], Step [43/112], Loss: 0.6196\n",
      "Epoch [258/300], Step [47/112], Loss: 0.6241\n",
      "Epoch [258/300], Step [51/112], Loss: 0.6210\n",
      "Epoch [258/300], Step [55/112], Loss: 0.6288\n",
      "Epoch [258/300], Step [59/112], Loss: 0.6247\n",
      "Epoch [258/300], Step [63/112], Loss: 0.6188\n",
      "Epoch [258/300], Step [67/112], Loss: 0.6254\n",
      "Epoch [258/300], Step [71/112], Loss: 0.6241\n",
      "Epoch [258/300], Step [75/112], Loss: 0.6388\n",
      "Epoch [258/300], Step [79/112], Loss: 0.6200\n",
      "Epoch [258/300], Step [83/112], Loss: 0.6534\n",
      "Epoch [258/300], Step [87/112], Loss: 0.6076\n",
      "Epoch [258/300], Step [91/112], Loss: 0.6577\n",
      "Epoch [258/300], Step [95/112], Loss: 0.6446\n",
      "Epoch [258/300], Step [99/112], Loss: 0.6407\n",
      "Epoch [258/300], Step [103/112], Loss: 0.6382\n",
      "Epoch [258/300], Step [107/112], Loss: 0.6260\n",
      "Epoch [258/300], Step [111/112], Loss: 0.5960\n",
      "Epoch [259/300], Step [3/112], Loss: 0.6437\n",
      "Epoch [259/300], Step [7/112], Loss: 0.6357\n",
      "Epoch [259/300], Step [11/112], Loss: 0.6247\n",
      "Epoch [259/300], Step [15/112], Loss: 0.6260\n",
      "Epoch [259/300], Step [19/112], Loss: 0.6189\n",
      "Epoch [259/300], Step [23/112], Loss: 0.6296\n",
      "Epoch [259/300], Step [27/112], Loss: 0.6064\n",
      "Epoch [259/300], Step [31/112], Loss: 0.6432\n",
      "Epoch [259/300], Step [35/112], Loss: 0.6160\n",
      "Epoch [259/300], Step [39/112], Loss: 0.6204\n",
      "Epoch [259/300], Step [43/112], Loss: 0.6263\n",
      "Epoch [259/300], Step [47/112], Loss: 0.6172\n",
      "Epoch [259/300], Step [51/112], Loss: 0.6177\n",
      "Epoch [259/300], Step [55/112], Loss: 0.6324\n",
      "Epoch [259/300], Step [59/112], Loss: 0.6238\n",
      "Epoch [259/300], Step [63/112], Loss: 0.6387\n",
      "Epoch [259/300], Step [67/112], Loss: 0.6299\n",
      "Epoch [259/300], Step [71/112], Loss: 0.6258\n",
      "Epoch [259/300], Step [75/112], Loss: 0.6282\n",
      "Epoch [259/300], Step [79/112], Loss: 0.6285\n",
      "Epoch [259/300], Step [83/112], Loss: 0.6541\n",
      "Epoch [259/300], Step [87/112], Loss: 0.6115\n",
      "Epoch [259/300], Step [91/112], Loss: 0.6584\n",
      "Epoch [259/300], Step [95/112], Loss: 0.6369\n",
      "Epoch [259/300], Step [99/112], Loss: 0.6485\n",
      "Epoch [259/300], Step [103/112], Loss: 0.6423\n",
      "Epoch [259/300], Step [107/112], Loss: 0.6327\n",
      "Epoch [259/300], Step [111/112], Loss: 0.6000\n",
      "Epoch [260/300], Step [3/112], Loss: 0.6489\n",
      "Epoch [260/300], Step [7/112], Loss: 0.6426\n",
      "Epoch [260/300], Step [11/112], Loss: 0.6293\n",
      "Epoch [260/300], Step [15/112], Loss: 0.6331\n",
      "Epoch [260/300], Step [19/112], Loss: 0.6213\n",
      "Epoch [260/300], Step [23/112], Loss: 0.6314\n",
      "Epoch [260/300], Step [27/112], Loss: 0.6136\n",
      "Epoch [260/300], Step [31/112], Loss: 0.6555\n",
      "Epoch [260/300], Step [35/112], Loss: 0.6134\n",
      "Epoch [260/300], Step [39/112], Loss: 0.6172\n",
      "Epoch [260/300], Step [43/112], Loss: 0.6305\n",
      "Epoch [260/300], Step [47/112], Loss: 0.6221\n",
      "Epoch [260/300], Step [51/112], Loss: 0.6144\n",
      "Epoch [260/300], Step [55/112], Loss: 0.6339\n",
      "Epoch [260/300], Step [59/112], Loss: 0.6239\n",
      "Epoch [260/300], Step [63/112], Loss: 0.6253\n",
      "Epoch [260/300], Step [67/112], Loss: 0.6273\n",
      "Epoch [260/300], Step [71/112], Loss: 0.6227\n",
      "Epoch [260/300], Step [75/112], Loss: 0.6405\n",
      "Epoch [260/300], Step [79/112], Loss: 0.6303\n",
      "Epoch [260/300], Step [83/112], Loss: 0.6472\n",
      "Epoch [260/300], Step [87/112], Loss: 0.6192\n",
      "Epoch [260/300], Step [91/112], Loss: 0.6603\n",
      "Epoch [260/300], Step [95/112], Loss: 0.6403\n",
      "Epoch [260/300], Step [99/112], Loss: 0.6404\n",
      "Epoch [260/300], Step [103/112], Loss: 0.6406\n",
      "Epoch [260/300], Step [107/112], Loss: 0.6303\n",
      "Epoch [260/300], Step [111/112], Loss: 0.5948\n",
      "Epoch [261/300], Step [3/112], Loss: 0.6443\n",
      "Epoch [261/300], Step [7/112], Loss: 0.6422\n",
      "Epoch [261/300], Step [11/112], Loss: 0.6329\n",
      "Epoch [261/300], Step [15/112], Loss: 0.6346\n",
      "Epoch [261/300], Step [19/112], Loss: 0.6166\n",
      "Epoch [261/300], Step [23/112], Loss: 0.6290\n",
      "Epoch [261/300], Step [27/112], Loss: 0.6080\n",
      "Epoch [261/300], Step [31/112], Loss: 0.6467\n",
      "Epoch [261/300], Step [35/112], Loss: 0.6227\n",
      "Epoch [261/300], Step [39/112], Loss: 0.6140\n",
      "Epoch [261/300], Step [43/112], Loss: 0.6242\n",
      "Epoch [261/300], Step [47/112], Loss: 0.6223\n",
      "Epoch [261/300], Step [51/112], Loss: 0.6123\n",
      "Epoch [261/300], Step [55/112], Loss: 0.6285\n",
      "Epoch [261/300], Step [59/112], Loss: 0.6150\n",
      "Epoch [261/300], Step [63/112], Loss: 0.6268\n",
      "Epoch [261/300], Step [67/112], Loss: 0.6174\n",
      "Epoch [261/300], Step [71/112], Loss: 0.6257\n",
      "Epoch [261/300], Step [75/112], Loss: 0.6355\n",
      "Epoch [261/300], Step [79/112], Loss: 0.6236\n",
      "Epoch [261/300], Step [83/112], Loss: 0.6526\n",
      "Epoch [261/300], Step [87/112], Loss: 0.6137\n",
      "Epoch [261/300], Step [91/112], Loss: 0.6565\n",
      "Epoch [261/300], Step [95/112], Loss: 0.6434\n",
      "Epoch [261/300], Step [99/112], Loss: 0.6390\n",
      "Epoch [261/300], Step [103/112], Loss: 0.6355\n",
      "Epoch [261/300], Step [107/112], Loss: 0.6202\n",
      "Epoch [261/300], Step [111/112], Loss: 0.5842\n",
      "Epoch [262/300], Step [3/112], Loss: 0.6420\n",
      "Epoch [262/300], Step [7/112], Loss: 0.6313\n",
      "Epoch [262/300], Step [11/112], Loss: 0.6237\n",
      "Epoch [262/300], Step [15/112], Loss: 0.6223\n",
      "Epoch [262/300], Step [19/112], Loss: 0.6107\n",
      "Epoch [262/300], Step [23/112], Loss: 0.6194\n",
      "Epoch [262/300], Step [27/112], Loss: 0.6000\n",
      "Epoch [262/300], Step [31/112], Loss: 0.6502\n",
      "Epoch [262/300], Step [35/112], Loss: 0.6173\n",
      "Epoch [262/300], Step [39/112], Loss: 0.6190\n",
      "Epoch [262/300], Step [43/112], Loss: 0.6262\n",
      "Epoch [262/300], Step [47/112], Loss: 0.6215\n",
      "Epoch [262/300], Step [51/112], Loss: 0.6115\n",
      "Epoch [262/300], Step [55/112], Loss: 0.6261\n",
      "Epoch [262/300], Step [59/112], Loss: 0.6288\n",
      "Epoch [262/300], Step [63/112], Loss: 0.6204\n",
      "Epoch [262/300], Step [67/112], Loss: 0.6203\n",
      "Epoch [262/300], Step [71/112], Loss: 0.6281\n",
      "Epoch [262/300], Step [75/112], Loss: 0.6339\n",
      "Epoch [262/300], Step [79/112], Loss: 0.6184\n",
      "Epoch [262/300], Step [83/112], Loss: 0.6472\n",
      "Epoch [262/300], Step [87/112], Loss: 0.6131\n",
      "Epoch [262/300], Step [91/112], Loss: 0.6521\n",
      "Epoch [262/300], Step [95/112], Loss: 0.6352\n",
      "Epoch [262/300], Step [99/112], Loss: 0.6378\n",
      "Epoch [262/300], Step [103/112], Loss: 0.6395\n",
      "Epoch [262/300], Step [107/112], Loss: 0.6340\n",
      "Epoch [262/300], Step [111/112], Loss: 0.5912\n",
      "Epoch [263/300], Step [3/112], Loss: 0.6510\n",
      "Epoch [263/300], Step [7/112], Loss: 0.6332\n",
      "Epoch [263/300], Step [11/112], Loss: 0.6221\n",
      "Epoch [263/300], Step [15/112], Loss: 0.6243\n",
      "Epoch [263/300], Step [19/112], Loss: 0.6129\n",
      "Epoch [263/300], Step [23/112], Loss: 0.6197\n",
      "Epoch [263/300], Step [27/112], Loss: 0.6037\n",
      "Epoch [263/300], Step [31/112], Loss: 0.6439\n",
      "Epoch [263/300], Step [35/112], Loss: 0.6141\n",
      "Epoch [263/300], Step [39/112], Loss: 0.6140\n",
      "Epoch [263/300], Step [43/112], Loss: 0.6303\n",
      "Epoch [263/300], Step [47/112], Loss: 0.6200\n",
      "Epoch [263/300], Step [51/112], Loss: 0.6209\n",
      "Epoch [263/300], Step [55/112], Loss: 0.6254\n",
      "Epoch [263/300], Step [59/112], Loss: 0.6263\n",
      "Epoch [263/300], Step [63/112], Loss: 0.6212\n",
      "Epoch [263/300], Step [67/112], Loss: 0.6212\n",
      "Epoch [263/300], Step [71/112], Loss: 0.6220\n",
      "Epoch [263/300], Step [75/112], Loss: 0.6360\n",
      "Epoch [263/300], Step [79/112], Loss: 0.6200\n",
      "Epoch [263/300], Step [83/112], Loss: 0.6429\n",
      "Epoch [263/300], Step [87/112], Loss: 0.6133\n",
      "Epoch [263/300], Step [91/112], Loss: 0.6567\n",
      "Epoch [263/300], Step [95/112], Loss: 0.6383\n",
      "Epoch [263/300], Step [99/112], Loss: 0.6409\n",
      "Epoch [263/300], Step [103/112], Loss: 0.6356\n",
      "Epoch [263/300], Step [107/112], Loss: 0.6331\n",
      "Epoch [263/300], Step [111/112], Loss: 0.5917\n",
      "Epoch [264/300], Step [3/112], Loss: 0.6364\n",
      "Epoch [264/300], Step [7/112], Loss: 0.6371\n",
      "Epoch [264/300], Step [11/112], Loss: 0.6239\n",
      "Epoch [264/300], Step [15/112], Loss: 0.6252\n",
      "Epoch [264/300], Step [19/112], Loss: 0.6134\n",
      "Epoch [264/300], Step [23/112], Loss: 0.6145\n",
      "Epoch [264/300], Step [27/112], Loss: 0.6027\n",
      "Epoch [264/300], Step [31/112], Loss: 0.6507\n",
      "Epoch [264/300], Step [35/112], Loss: 0.6192\n",
      "Epoch [264/300], Step [39/112], Loss: 0.6118\n",
      "Epoch [264/300], Step [43/112], Loss: 0.6250\n",
      "Epoch [264/300], Step [47/112], Loss: 0.6153\n",
      "Epoch [264/300], Step [51/112], Loss: 0.6164\n",
      "Epoch [264/300], Step [55/112], Loss: 0.6304\n",
      "Epoch [264/300], Step [59/112], Loss: 0.6200\n",
      "Epoch [264/300], Step [63/112], Loss: 0.6290\n",
      "Epoch [264/300], Step [67/112], Loss: 0.6199\n",
      "Epoch [264/300], Step [71/112], Loss: 0.6203\n",
      "Epoch [264/300], Step [75/112], Loss: 0.6392\n",
      "Epoch [264/300], Step [79/112], Loss: 0.6183\n",
      "Epoch [264/300], Step [83/112], Loss: 0.6422\n",
      "Epoch [264/300], Step [87/112], Loss: 0.6187\n",
      "Epoch [264/300], Step [91/112], Loss: 0.6493\n",
      "Epoch [264/300], Step [95/112], Loss: 0.6380\n",
      "Epoch [264/300], Step [99/112], Loss: 0.6381\n",
      "Epoch [264/300], Step [103/112], Loss: 0.6368\n",
      "Epoch [264/300], Step [107/112], Loss: 0.6316\n",
      "Epoch [264/300], Step [111/112], Loss: 0.5933\n",
      "Epoch [265/300], Step [3/112], Loss: 0.6368\n",
      "Epoch [265/300], Step [7/112], Loss: 0.6369\n",
      "Epoch [265/300], Step [11/112], Loss: 0.6199\n",
      "Epoch [265/300], Step [15/112], Loss: 0.6367\n",
      "Epoch [265/300], Step [19/112], Loss: 0.6131\n",
      "Epoch [265/300], Step [23/112], Loss: 0.6174\n",
      "Epoch [265/300], Step [27/112], Loss: 0.5980\n",
      "Epoch [265/300], Step [31/112], Loss: 0.6407\n",
      "Epoch [265/300], Step [35/112], Loss: 0.6314\n",
      "Epoch [265/300], Step [39/112], Loss: 0.6101\n",
      "Epoch [265/300], Step [43/112], Loss: 0.6323\n",
      "Epoch [265/300], Step [47/112], Loss: 0.6237\n",
      "Epoch [265/300], Step [51/112], Loss: 0.6292\n",
      "Epoch [265/300], Step [55/112], Loss: 0.6363\n",
      "Epoch [265/300], Step [59/112], Loss: 0.6277\n",
      "Epoch [265/300], Step [63/112], Loss: 0.6284\n",
      "Epoch [265/300], Step [67/112], Loss: 0.6211\n",
      "Epoch [265/300], Step [71/112], Loss: 0.6273\n",
      "Epoch [265/300], Step [75/112], Loss: 0.6359\n",
      "Epoch [265/300], Step [79/112], Loss: 0.6208\n",
      "Epoch [265/300], Step [83/112], Loss: 0.6468\n",
      "Epoch [265/300], Step [87/112], Loss: 0.6188\n",
      "Epoch [265/300], Step [91/112], Loss: 0.6576\n",
      "Epoch [265/300], Step [95/112], Loss: 0.6475\n",
      "Epoch [265/300], Step [99/112], Loss: 0.6487\n",
      "Epoch [265/300], Step [103/112], Loss: 0.6457\n",
      "Epoch [265/300], Step [107/112], Loss: 0.6311\n",
      "Epoch [265/300], Step [111/112], Loss: 0.5956\n",
      "Epoch [266/300], Step [3/112], Loss: 0.6407\n",
      "Epoch [266/300], Step [7/112], Loss: 0.6420\n",
      "Epoch [266/300], Step [11/112], Loss: 0.6202\n",
      "Epoch [266/300], Step [15/112], Loss: 0.6295\n",
      "Epoch [266/300], Step [19/112], Loss: 0.6228\n",
      "Epoch [266/300], Step [23/112], Loss: 0.6267\n",
      "Epoch [266/300], Step [27/112], Loss: 0.5988\n",
      "Epoch [266/300], Step [31/112], Loss: 0.6506\n",
      "Epoch [266/300], Step [35/112], Loss: 0.6178\n",
      "Epoch [266/300], Step [39/112], Loss: 0.6031\n",
      "Epoch [266/300], Step [43/112], Loss: 0.6287\n",
      "Epoch [266/300], Step [47/112], Loss: 0.6227\n",
      "Epoch [266/300], Step [51/112], Loss: 0.6222\n",
      "Epoch [266/300], Step [55/112], Loss: 0.6241\n",
      "Epoch [266/300], Step [59/112], Loss: 0.6228\n",
      "Epoch [266/300], Step [63/112], Loss: 0.6176\n",
      "Epoch [266/300], Step [67/112], Loss: 0.6254\n",
      "Epoch [266/300], Step [71/112], Loss: 0.6292\n",
      "Epoch [266/300], Step [75/112], Loss: 0.6444\n",
      "Epoch [266/300], Step [79/112], Loss: 0.6328\n",
      "Epoch [266/300], Step [83/112], Loss: 0.6537\n",
      "Epoch [266/300], Step [87/112], Loss: 0.6153\n",
      "Epoch [266/300], Step [91/112], Loss: 0.6587\n",
      "Epoch [266/300], Step [95/112], Loss: 0.6382\n",
      "Epoch [266/300], Step [99/112], Loss: 0.6491\n",
      "Epoch [266/300], Step [103/112], Loss: 0.6407\n",
      "Epoch [266/300], Step [107/112], Loss: 0.6324\n",
      "Epoch [266/300], Step [111/112], Loss: 0.5886\n",
      "Epoch [267/300], Step [3/112], Loss: 0.6418\n",
      "Epoch [267/300], Step [7/112], Loss: 0.6489\n",
      "Epoch [267/300], Step [11/112], Loss: 0.6203\n",
      "Epoch [267/300], Step [15/112], Loss: 0.6404\n",
      "Epoch [267/300], Step [19/112], Loss: 0.6285\n",
      "Epoch [267/300], Step [23/112], Loss: 0.6225\n",
      "Epoch [267/300], Step [27/112], Loss: 0.6049\n",
      "Epoch [267/300], Step [31/112], Loss: 0.6559\n",
      "Epoch [267/300], Step [35/112], Loss: 0.6319\n",
      "Epoch [267/300], Step [39/112], Loss: 0.6187\n",
      "Epoch [267/300], Step [43/112], Loss: 0.6319\n",
      "Epoch [267/300], Step [47/112], Loss: 0.6360\n",
      "Epoch [267/300], Step [51/112], Loss: 0.6297\n",
      "Epoch [267/300], Step [55/112], Loss: 0.6362\n",
      "Epoch [267/300], Step [59/112], Loss: 0.6200\n",
      "Epoch [267/300], Step [63/112], Loss: 0.6191\n",
      "Epoch [267/300], Step [67/112], Loss: 0.6240\n",
      "Epoch [267/300], Step [71/112], Loss: 0.6268\n",
      "Epoch [267/300], Step [75/112], Loss: 0.6563\n",
      "Epoch [267/300], Step [79/112], Loss: 0.6273\n",
      "Epoch [267/300], Step [83/112], Loss: 0.6594\n",
      "Epoch [267/300], Step [87/112], Loss: 0.6259\n",
      "Epoch [267/300], Step [91/112], Loss: 0.6580\n",
      "Epoch [267/300], Step [95/112], Loss: 0.6370\n",
      "Epoch [267/300], Step [99/112], Loss: 0.6462\n",
      "Epoch [267/300], Step [103/112], Loss: 0.6409\n",
      "Epoch [267/300], Step [107/112], Loss: 0.6267\n",
      "Epoch [267/300], Step [111/112], Loss: 0.5981\n",
      "Epoch [268/300], Step [3/112], Loss: 0.6447\n",
      "Epoch [268/300], Step [7/112], Loss: 0.6378\n",
      "Epoch [268/300], Step [11/112], Loss: 0.6249\n",
      "Epoch [268/300], Step [15/112], Loss: 0.6303\n",
      "Epoch [268/300], Step [19/112], Loss: 0.6198\n",
      "Epoch [268/300], Step [23/112], Loss: 0.6263\n",
      "Epoch [268/300], Step [27/112], Loss: 0.5985\n",
      "Epoch [268/300], Step [31/112], Loss: 0.6536\n",
      "Epoch [268/300], Step [35/112], Loss: 0.6160\n",
      "Epoch [268/300], Step [39/112], Loss: 0.6160\n",
      "Epoch [268/300], Step [43/112], Loss: 0.6311\n",
      "Epoch [268/300], Step [47/112], Loss: 0.6256\n",
      "Epoch [268/300], Step [51/112], Loss: 0.6260\n",
      "Epoch [268/300], Step [55/112], Loss: 0.6279\n",
      "Epoch [268/300], Step [59/112], Loss: 0.6217\n",
      "Epoch [268/300], Step [63/112], Loss: 0.6225\n",
      "Epoch [268/300], Step [67/112], Loss: 0.6259\n",
      "Epoch [268/300], Step [71/112], Loss: 0.6222\n",
      "Epoch [268/300], Step [75/112], Loss: 0.6348\n",
      "Epoch [268/300], Step [79/112], Loss: 0.6213\n",
      "Epoch [268/300], Step [83/112], Loss: 0.6457\n",
      "Epoch [268/300], Step [87/112], Loss: 0.6137\n",
      "Epoch [268/300], Step [91/112], Loss: 0.6534\n",
      "Epoch [268/300], Step [95/112], Loss: 0.6442\n",
      "Epoch [268/300], Step [99/112], Loss: 0.6425\n",
      "Epoch [268/300], Step [103/112], Loss: 0.6374\n",
      "Epoch [268/300], Step [107/112], Loss: 0.6371\n",
      "Epoch [268/300], Step [111/112], Loss: 0.5887\n",
      "Epoch [269/300], Step [3/112], Loss: 0.6424\n",
      "Epoch [269/300], Step [7/112], Loss: 0.6350\n",
      "Epoch [269/300], Step [11/112], Loss: 0.6248\n",
      "Epoch [269/300], Step [15/112], Loss: 0.6253\n",
      "Epoch [269/300], Step [19/112], Loss: 0.6218\n",
      "Epoch [269/300], Step [23/112], Loss: 0.6236\n",
      "Epoch [269/300], Step [27/112], Loss: 0.5997\n",
      "Epoch [269/300], Step [31/112], Loss: 0.6460\n",
      "Epoch [269/300], Step [35/112], Loss: 0.6255\n",
      "Epoch [269/300], Step [39/112], Loss: 0.6152\n",
      "Epoch [269/300], Step [43/112], Loss: 0.6315\n",
      "Epoch [269/300], Step [47/112], Loss: 0.6271\n",
      "Epoch [269/300], Step [51/112], Loss: 0.6166\n",
      "Epoch [269/300], Step [55/112], Loss: 0.6306\n",
      "Epoch [269/300], Step [59/112], Loss: 0.6280\n",
      "Epoch [269/300], Step [63/112], Loss: 0.6242\n",
      "Epoch [269/300], Step [67/112], Loss: 0.6191\n",
      "Epoch [269/300], Step [71/112], Loss: 0.6231\n",
      "Epoch [269/300], Step [75/112], Loss: 0.6303\n",
      "Epoch [269/300], Step [79/112], Loss: 0.6268\n",
      "Epoch [269/300], Step [83/112], Loss: 0.6491\n",
      "Epoch [269/300], Step [87/112], Loss: 0.6139\n",
      "Epoch [269/300], Step [91/112], Loss: 0.6556\n",
      "Epoch [269/300], Step [95/112], Loss: 0.6319\n",
      "Epoch [269/300], Step [99/112], Loss: 0.6385\n",
      "Epoch [269/300], Step [103/112], Loss: 0.6320\n",
      "Epoch [269/300], Step [107/112], Loss: 0.6274\n",
      "Epoch [269/300], Step [111/112], Loss: 0.5869\n",
      "Epoch [270/300], Step [3/112], Loss: 0.6359\n",
      "Epoch [270/300], Step [7/112], Loss: 0.6398\n",
      "Epoch [270/300], Step [11/112], Loss: 0.6239\n",
      "Epoch [270/300], Step [15/112], Loss: 0.6251\n",
      "Epoch [270/300], Step [19/112], Loss: 0.6133\n",
      "Epoch [270/300], Step [23/112], Loss: 0.6265\n",
      "Epoch [270/300], Step [27/112], Loss: 0.5993\n",
      "Epoch [270/300], Step [31/112], Loss: 0.6426\n",
      "Epoch [270/300], Step [35/112], Loss: 0.6206\n",
      "Epoch [270/300], Step [39/112], Loss: 0.6113\n",
      "Epoch [270/300], Step [43/112], Loss: 0.6317\n",
      "Epoch [270/300], Step [47/112], Loss: 0.6293\n",
      "Epoch [270/300], Step [51/112], Loss: 0.6194\n",
      "Epoch [270/300], Step [55/112], Loss: 0.6245\n",
      "Epoch [270/300], Step [59/112], Loss: 0.6197\n",
      "Epoch [270/300], Step [63/112], Loss: 0.6274\n",
      "Epoch [270/300], Step [67/112], Loss: 0.6230\n",
      "Epoch [270/300], Step [71/112], Loss: 0.6184\n",
      "Epoch [270/300], Step [75/112], Loss: 0.6282\n",
      "Epoch [270/300], Step [79/112], Loss: 0.6188\n",
      "Epoch [270/300], Step [83/112], Loss: 0.6461\n",
      "Epoch [270/300], Step [87/112], Loss: 0.6152\n",
      "Epoch [270/300], Step [91/112], Loss: 0.6588\n",
      "Epoch [270/300], Step [95/112], Loss: 0.6348\n",
      "Epoch [270/300], Step [99/112], Loss: 0.6460\n",
      "Epoch [270/300], Step [103/112], Loss: 0.6365\n",
      "Epoch [270/300], Step [107/112], Loss: 0.6238\n",
      "Epoch [270/300], Step [111/112], Loss: 0.5848\n",
      "Epoch [271/300], Step [3/112], Loss: 0.6340\n",
      "Epoch [271/300], Step [7/112], Loss: 0.6298\n",
      "Epoch [271/300], Step [11/112], Loss: 0.6201\n",
      "Epoch [271/300], Step [15/112], Loss: 0.6271\n",
      "Epoch [271/300], Step [19/112], Loss: 0.6124\n",
      "Epoch [271/300], Step [23/112], Loss: 0.6171\n",
      "Epoch [271/300], Step [27/112], Loss: 0.5937\n",
      "Epoch [271/300], Step [31/112], Loss: 0.6479\n",
      "Epoch [271/300], Step [35/112], Loss: 0.6182\n",
      "Epoch [271/300], Step [39/112], Loss: 0.6088\n",
      "Epoch [271/300], Step [43/112], Loss: 0.6262\n",
      "Epoch [271/300], Step [47/112], Loss: 0.6222\n",
      "Epoch [271/300], Step [51/112], Loss: 0.6196\n",
      "Epoch [271/300], Step [55/112], Loss: 0.6225\n",
      "Epoch [271/300], Step [59/112], Loss: 0.6199\n",
      "Epoch [271/300], Step [63/112], Loss: 0.6182\n",
      "Epoch [271/300], Step [67/112], Loss: 0.6212\n",
      "Epoch [271/300], Step [71/112], Loss: 0.6266\n",
      "Epoch [271/300], Step [75/112], Loss: 0.6370\n",
      "Epoch [271/300], Step [79/112], Loss: 0.6139\n",
      "Epoch [271/300], Step [83/112], Loss: 0.6405\n",
      "Epoch [271/300], Step [87/112], Loss: 0.6158\n",
      "Epoch [271/300], Step [91/112], Loss: 0.6575\n",
      "Epoch [271/300], Step [95/112], Loss: 0.6337\n",
      "Epoch [271/300], Step [99/112], Loss: 0.6343\n",
      "Epoch [271/300], Step [103/112], Loss: 0.6432\n",
      "Epoch [271/300], Step [107/112], Loss: 0.6317\n",
      "Epoch [271/300], Step [111/112], Loss: 0.5879\n",
      "Epoch [272/300], Step [3/112], Loss: 0.6465\n",
      "Epoch [272/300], Step [7/112], Loss: 0.6338\n",
      "Epoch [272/300], Step [11/112], Loss: 0.6208\n",
      "Epoch [272/300], Step [15/112], Loss: 0.6252\n",
      "Epoch [272/300], Step [19/112], Loss: 0.6172\n",
      "Epoch [272/300], Step [23/112], Loss: 0.6182\n",
      "Epoch [272/300], Step [27/112], Loss: 0.5970\n",
      "Epoch [272/300], Step [31/112], Loss: 0.6462\n",
      "Epoch [272/300], Step [35/112], Loss: 0.6169\n",
      "Epoch [272/300], Step [39/112], Loss: 0.6093\n",
      "Epoch [272/300], Step [43/112], Loss: 0.6302\n",
      "Epoch [272/300], Step [47/112], Loss: 0.6210\n",
      "Epoch [272/300], Step [51/112], Loss: 0.6280\n",
      "Epoch [272/300], Step [55/112], Loss: 0.6279\n",
      "Epoch [272/300], Step [59/112], Loss: 0.6251\n",
      "Epoch [272/300], Step [63/112], Loss: 0.6213\n",
      "Epoch [272/300], Step [67/112], Loss: 0.6208\n",
      "Epoch [272/300], Step [71/112], Loss: 0.6266\n",
      "Epoch [272/300], Step [75/112], Loss: 0.6348\n",
      "Epoch [272/300], Step [79/112], Loss: 0.6241\n",
      "Epoch [272/300], Step [83/112], Loss: 0.6407\n",
      "Epoch [272/300], Step [87/112], Loss: 0.6137\n",
      "Epoch [272/300], Step [91/112], Loss: 0.6562\n",
      "Epoch [272/300], Step [95/112], Loss: 0.6375\n",
      "Epoch [272/300], Step [99/112], Loss: 0.6396\n",
      "Epoch [272/300], Step [103/112], Loss: 0.6400\n",
      "Epoch [272/300], Step [107/112], Loss: 0.6246\n",
      "Epoch [272/300], Step [111/112], Loss: 0.5842\n",
      "Epoch [273/300], Step [3/112], Loss: 0.6366\n",
      "Epoch [273/300], Step [7/112], Loss: 0.6429\n",
      "Epoch [273/300], Step [11/112], Loss: 0.6236\n",
      "Epoch [273/300], Step [15/112], Loss: 0.6300\n",
      "Epoch [273/300], Step [19/112], Loss: 0.6247\n",
      "Epoch [273/300], Step [23/112], Loss: 0.6318\n",
      "Epoch [273/300], Step [27/112], Loss: 0.5976\n",
      "Epoch [273/300], Step [31/112], Loss: 0.6452\n",
      "Epoch [273/300], Step [35/112], Loss: 0.6141\n",
      "Epoch [273/300], Step [39/112], Loss: 0.6139\n",
      "Epoch [273/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [273/300], Step [47/112], Loss: 0.6248\n",
      "Epoch [273/300], Step [51/112], Loss: 0.6236\n",
      "Epoch [273/300], Step [55/112], Loss: 0.6301\n",
      "Epoch [273/300], Step [59/112], Loss: 0.6150\n",
      "Epoch [273/300], Step [63/112], Loss: 0.6268\n",
      "Epoch [273/300], Step [67/112], Loss: 0.6195\n",
      "Epoch [273/300], Step [71/112], Loss: 0.6235\n",
      "Epoch [273/300], Step [75/112], Loss: 0.6331\n",
      "Epoch [273/300], Step [79/112], Loss: 0.6268\n",
      "Epoch [273/300], Step [83/112], Loss: 0.6541\n",
      "Epoch [273/300], Step [87/112], Loss: 0.6186\n",
      "Epoch [273/300], Step [91/112], Loss: 0.6520\n",
      "Epoch [273/300], Step [95/112], Loss: 0.6370\n",
      "Epoch [273/300], Step [99/112], Loss: 0.6467\n",
      "Epoch [273/300], Step [103/112], Loss: 0.6454\n",
      "Epoch [273/300], Step [107/112], Loss: 0.6328\n",
      "Epoch [273/300], Step [111/112], Loss: 0.5871\n",
      "Epoch [274/300], Step [3/112], Loss: 0.6378\n",
      "Epoch [274/300], Step [7/112], Loss: 0.6349\n",
      "Epoch [274/300], Step [11/112], Loss: 0.6211\n",
      "Epoch [274/300], Step [15/112], Loss: 0.6304\n",
      "Epoch [274/300], Step [19/112], Loss: 0.6210\n",
      "Epoch [274/300], Step [23/112], Loss: 0.6203\n",
      "Epoch [274/300], Step [27/112], Loss: 0.5987\n",
      "Epoch [274/300], Step [31/112], Loss: 0.6335\n",
      "Epoch [274/300], Step [35/112], Loss: 0.6147\n",
      "Epoch [274/300], Step [39/112], Loss: 0.6136\n",
      "Epoch [274/300], Step [43/112], Loss: 0.6270\n",
      "Epoch [274/300], Step [47/112], Loss: 0.6214\n",
      "Epoch [274/300], Step [51/112], Loss: 0.6176\n",
      "Epoch [274/300], Step [55/112], Loss: 0.6306\n",
      "Epoch [274/300], Step [59/112], Loss: 0.6202\n",
      "Epoch [274/300], Step [63/112], Loss: 0.6326\n",
      "Epoch [274/300], Step [67/112], Loss: 0.6212\n",
      "Epoch [274/300], Step [71/112], Loss: 0.6388\n",
      "Epoch [274/300], Step [75/112], Loss: 0.6357\n",
      "Epoch [274/300], Step [79/112], Loss: 0.6221\n",
      "Epoch [274/300], Step [83/112], Loss: 0.6414\n",
      "Epoch [274/300], Step [87/112], Loss: 0.6122\n",
      "Epoch [274/300], Step [91/112], Loss: 0.6631\n",
      "Epoch [274/300], Step [95/112], Loss: 0.6343\n",
      "Epoch [274/300], Step [99/112], Loss: 0.6372\n",
      "Epoch [274/300], Step [103/112], Loss: 0.6282\n",
      "Epoch [274/300], Step [107/112], Loss: 0.6193\n",
      "Epoch [274/300], Step [111/112], Loss: 0.5882\n",
      "Epoch [275/300], Step [3/112], Loss: 0.6329\n",
      "Epoch [275/300], Step [7/112], Loss: 0.6340\n",
      "Epoch [275/300], Step [11/112], Loss: 0.6199\n",
      "Epoch [275/300], Step [15/112], Loss: 0.6197\n",
      "Epoch [275/300], Step [19/112], Loss: 0.6116\n",
      "Epoch [275/300], Step [23/112], Loss: 0.6195\n",
      "Epoch [275/300], Step [27/112], Loss: 0.6000\n",
      "Epoch [275/300], Step [31/112], Loss: 0.6450\n",
      "Epoch [275/300], Step [35/112], Loss: 0.6238\n",
      "Epoch [275/300], Step [39/112], Loss: 0.6092\n",
      "Epoch [275/300], Step [43/112], Loss: 0.6236\n",
      "Epoch [275/300], Step [47/112], Loss: 0.6212\n",
      "Epoch [275/300], Step [51/112], Loss: 0.6131\n",
      "Epoch [275/300], Step [55/112], Loss: 0.6269\n",
      "Epoch [275/300], Step [59/112], Loss: 0.6182\n",
      "Epoch [275/300], Step [63/112], Loss: 0.6375\n",
      "Epoch [275/300], Step [67/112], Loss: 0.6217\n",
      "Epoch [275/300], Step [71/112], Loss: 0.6300\n",
      "Epoch [275/300], Step [75/112], Loss: 0.6398\n",
      "Epoch [275/300], Step [79/112], Loss: 0.6236\n",
      "Epoch [275/300], Step [83/112], Loss: 0.6595\n",
      "Epoch [275/300], Step [87/112], Loss: 0.6166\n",
      "Epoch [275/300], Step [91/112], Loss: 0.6676\n",
      "Epoch [275/300], Step [95/112], Loss: 0.6322\n",
      "Epoch [275/300], Step [99/112], Loss: 0.6368\n",
      "Epoch [275/300], Step [103/112], Loss: 0.6354\n",
      "Epoch [275/300], Step [107/112], Loss: 0.6316\n",
      "Epoch [275/300], Step [111/112], Loss: 0.5848\n",
      "Epoch [276/300], Step [3/112], Loss: 0.6450\n",
      "Epoch [276/300], Step [7/112], Loss: 0.6387\n",
      "Epoch [276/300], Step [11/112], Loss: 0.6241\n",
      "Epoch [276/300], Step [15/112], Loss: 0.6278\n",
      "Epoch [276/300], Step [19/112], Loss: 0.6099\n",
      "Epoch [276/300], Step [23/112], Loss: 0.6248\n",
      "Epoch [276/300], Step [27/112], Loss: 0.6009\n",
      "Epoch [276/300], Step [31/112], Loss: 0.6374\n",
      "Epoch [276/300], Step [35/112], Loss: 0.6243\n",
      "Epoch [276/300], Step [39/112], Loss: 0.6126\n",
      "Epoch [276/300], Step [43/112], Loss: 0.6275\n",
      "Epoch [276/300], Step [47/112], Loss: 0.6167\n",
      "Epoch [276/300], Step [51/112], Loss: 0.6201\n",
      "Epoch [276/300], Step [55/112], Loss: 0.6256\n",
      "Epoch [276/300], Step [59/112], Loss: 0.6272\n",
      "Epoch [276/300], Step [63/112], Loss: 0.6321\n",
      "Epoch [276/300], Step [67/112], Loss: 0.6206\n",
      "Epoch [276/300], Step [71/112], Loss: 0.6238\n",
      "Epoch [276/300], Step [75/112], Loss: 0.6357\n",
      "Epoch [276/300], Step [79/112], Loss: 0.6268\n",
      "Epoch [276/300], Step [83/112], Loss: 0.6475\n",
      "Epoch [276/300], Step [87/112], Loss: 0.6187\n",
      "Epoch [276/300], Step [91/112], Loss: 0.6584\n",
      "Epoch [276/300], Step [95/112], Loss: 0.6300\n",
      "Epoch [276/300], Step [99/112], Loss: 0.6383\n",
      "Epoch [276/300], Step [103/112], Loss: 0.6419\n",
      "Epoch [276/300], Step [107/112], Loss: 0.6364\n",
      "Epoch [276/300], Step [111/112], Loss: 0.5816\n",
      "Epoch [277/300], Step [3/112], Loss: 0.6336\n",
      "Epoch [277/300], Step [7/112], Loss: 0.6316\n",
      "Epoch [277/300], Step [11/112], Loss: 0.6158\n",
      "Epoch [277/300], Step [15/112], Loss: 0.6266\n",
      "Epoch [277/300], Step [19/112], Loss: 0.6097\n",
      "Epoch [277/300], Step [23/112], Loss: 0.6304\n",
      "Epoch [277/300], Step [27/112], Loss: 0.6065\n",
      "Epoch [277/300], Step [31/112], Loss: 0.6457\n",
      "Epoch [277/300], Step [35/112], Loss: 0.6162\n",
      "Epoch [277/300], Step [39/112], Loss: 0.6164\n",
      "Epoch [277/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [277/300], Step [47/112], Loss: 0.6173\n",
      "Epoch [277/300], Step [51/112], Loss: 0.6154\n",
      "Epoch [277/300], Step [55/112], Loss: 0.6307\n",
      "Epoch [277/300], Step [59/112], Loss: 0.6234\n",
      "Epoch [277/300], Step [63/112], Loss: 0.6270\n",
      "Epoch [277/300], Step [67/112], Loss: 0.6228\n",
      "Epoch [277/300], Step [71/112], Loss: 0.6238\n",
      "Epoch [277/300], Step [75/112], Loss: 0.6367\n",
      "Epoch [277/300], Step [79/112], Loss: 0.6273\n",
      "Epoch [277/300], Step [83/112], Loss: 0.6465\n",
      "Epoch [277/300], Step [87/112], Loss: 0.6225\n",
      "Epoch [277/300], Step [91/112], Loss: 0.6656\n",
      "Epoch [277/300], Step [95/112], Loss: 0.6447\n",
      "Epoch [277/300], Step [99/112], Loss: 0.6516\n",
      "Epoch [277/300], Step [103/112], Loss: 0.6407\n",
      "Epoch [277/300], Step [107/112], Loss: 0.6273\n",
      "Epoch [277/300], Step [111/112], Loss: 0.5863\n",
      "Epoch [278/300], Step [3/112], Loss: 0.6414\n",
      "Epoch [278/300], Step [7/112], Loss: 0.6365\n",
      "Epoch [278/300], Step [11/112], Loss: 0.6253\n",
      "Epoch [278/300], Step [15/112], Loss: 0.6330\n",
      "Epoch [278/300], Step [19/112], Loss: 0.6166\n",
      "Epoch [278/300], Step [23/112], Loss: 0.6211\n",
      "Epoch [278/300], Step [27/112], Loss: 0.5981\n",
      "Epoch [278/300], Step [31/112], Loss: 0.6432\n",
      "Epoch [278/300], Step [35/112], Loss: 0.6195\n",
      "Epoch [278/300], Step [39/112], Loss: 0.6202\n",
      "Epoch [278/300], Step [43/112], Loss: 0.6165\n",
      "Epoch [278/300], Step [47/112], Loss: 0.6216\n",
      "Epoch [278/300], Step [51/112], Loss: 0.6260\n",
      "Epoch [278/300], Step [55/112], Loss: 0.6329\n",
      "Epoch [278/300], Step [59/112], Loss: 0.6191\n",
      "Epoch [278/300], Step [63/112], Loss: 0.6278\n",
      "Epoch [278/300], Step [67/112], Loss: 0.6291\n",
      "Epoch [278/300], Step [71/112], Loss: 0.6317\n",
      "Epoch [278/300], Step [75/112], Loss: 0.6362\n",
      "Epoch [278/300], Step [79/112], Loss: 0.6233\n",
      "Epoch [278/300], Step [83/112], Loss: 0.6495\n",
      "Epoch [278/300], Step [87/112], Loss: 0.6208\n",
      "Epoch [278/300], Step [91/112], Loss: 0.6587\n",
      "Epoch [278/300], Step [95/112], Loss: 0.6340\n",
      "Epoch [278/300], Step [99/112], Loss: 0.6463\n",
      "Epoch [278/300], Step [103/112], Loss: 0.6376\n",
      "Epoch [278/300], Step [107/112], Loss: 0.6347\n",
      "Epoch [278/300], Step [111/112], Loss: 0.5958\n",
      "Epoch [279/300], Step [3/112], Loss: 0.6422\n",
      "Epoch [279/300], Step [7/112], Loss: 0.6328\n",
      "Epoch [279/300], Step [11/112], Loss: 0.6233\n",
      "Epoch [279/300], Step [15/112], Loss: 0.6249\n",
      "Epoch [279/300], Step [19/112], Loss: 0.6081\n",
      "Epoch [279/300], Step [23/112], Loss: 0.6269\n",
      "Epoch [279/300], Step [27/112], Loss: 0.6005\n",
      "Epoch [279/300], Step [31/112], Loss: 0.6470\n",
      "Epoch [279/300], Step [35/112], Loss: 0.6166\n",
      "Epoch [279/300], Step [39/112], Loss: 0.6132\n",
      "Epoch [279/300], Step [43/112], Loss: 0.6320\n",
      "Epoch [279/300], Step [47/112], Loss: 0.6203\n",
      "Epoch [279/300], Step [51/112], Loss: 0.6199\n",
      "Epoch [279/300], Step [55/112], Loss: 0.6255\n",
      "Epoch [279/300], Step [59/112], Loss: 0.6169\n",
      "Epoch [279/300], Step [63/112], Loss: 0.6240\n",
      "Epoch [279/300], Step [67/112], Loss: 0.6199\n",
      "Epoch [279/300], Step [71/112], Loss: 0.6241\n",
      "Epoch [279/300], Step [75/112], Loss: 0.6330\n",
      "Epoch [279/300], Step [79/112], Loss: 0.6178\n",
      "Epoch [279/300], Step [83/112], Loss: 0.6445\n",
      "Epoch [279/300], Step [87/112], Loss: 0.6196\n",
      "Epoch [279/300], Step [91/112], Loss: 0.6600\n",
      "Epoch [279/300], Step [95/112], Loss: 0.6355\n",
      "Epoch [279/300], Step [99/112], Loss: 0.6391\n",
      "Epoch [279/300], Step [103/112], Loss: 0.6366\n",
      "Epoch [279/300], Step [107/112], Loss: 0.6319\n",
      "Epoch [279/300], Step [111/112], Loss: 0.5856\n",
      "Epoch [280/300], Step [3/112], Loss: 0.6365\n",
      "Epoch [280/300], Step [7/112], Loss: 0.6348\n",
      "Epoch [280/300], Step [11/112], Loss: 0.6188\n",
      "Epoch [280/300], Step [15/112], Loss: 0.6278\n",
      "Epoch [280/300], Step [19/112], Loss: 0.6071\n",
      "Epoch [280/300], Step [23/112], Loss: 0.6161\n",
      "Epoch [280/300], Step [27/112], Loss: 0.6001\n",
      "Epoch [280/300], Step [31/112], Loss: 0.6432\n",
      "Epoch [280/300], Step [35/112], Loss: 0.6137\n",
      "Epoch [280/300], Step [39/112], Loss: 0.6143\n",
      "Epoch [280/300], Step [43/112], Loss: 0.6237\n",
      "Epoch [280/300], Step [47/112], Loss: 0.6174\n",
      "Epoch [280/300], Step [51/112], Loss: 0.6129\n",
      "Epoch [280/300], Step [55/112], Loss: 0.6208\n",
      "Epoch [280/300], Step [59/112], Loss: 0.6137\n",
      "Epoch [280/300], Step [63/112], Loss: 0.6224\n",
      "Epoch [280/300], Step [67/112], Loss: 0.6160\n",
      "Epoch [280/300], Step [71/112], Loss: 0.6227\n",
      "Epoch [280/300], Step [75/112], Loss: 0.6345\n",
      "Epoch [280/300], Step [79/112], Loss: 0.6171\n",
      "Epoch [280/300], Step [83/112], Loss: 0.6446\n",
      "Epoch [280/300], Step [87/112], Loss: 0.6095\n",
      "Epoch [280/300], Step [91/112], Loss: 0.6487\n",
      "Epoch [280/300], Step [95/112], Loss: 0.6326\n",
      "Epoch [280/300], Step [99/112], Loss: 0.6350\n",
      "Epoch [280/300], Step [103/112], Loss: 0.6378\n",
      "Epoch [280/300], Step [107/112], Loss: 0.6268\n",
      "Epoch [280/300], Step [111/112], Loss: 0.5880\n",
      "Epoch [281/300], Step [3/112], Loss: 0.6416\n",
      "Epoch [281/300], Step [7/112], Loss: 0.6374\n",
      "Epoch [281/300], Step [11/112], Loss: 0.6279\n",
      "Epoch [281/300], Step [15/112], Loss: 0.6332\n",
      "Epoch [281/300], Step [19/112], Loss: 0.6146\n",
      "Epoch [281/300], Step [23/112], Loss: 0.6305\n",
      "Epoch [281/300], Step [27/112], Loss: 0.6024\n",
      "Epoch [281/300], Step [31/112], Loss: 0.6540\n",
      "Epoch [281/300], Step [35/112], Loss: 0.6223\n",
      "Epoch [281/300], Step [39/112], Loss: 0.6120\n",
      "Epoch [281/300], Step [43/112], Loss: 0.6312\n",
      "Epoch [281/300], Step [47/112], Loss: 0.6271\n",
      "Epoch [281/300], Step [51/112], Loss: 0.6154\n",
      "Epoch [281/300], Step [55/112], Loss: 0.6270\n",
      "Epoch [281/300], Step [59/112], Loss: 0.6307\n",
      "Epoch [281/300], Step [63/112], Loss: 0.6239\n",
      "Epoch [281/300], Step [67/112], Loss: 0.6156\n",
      "Epoch [281/300], Step [71/112], Loss: 0.6189\n",
      "Epoch [281/300], Step [75/112], Loss: 0.6310\n",
      "Epoch [281/300], Step [79/112], Loss: 0.6279\n",
      "Epoch [281/300], Step [83/112], Loss: 0.6505\n",
      "Epoch [281/300], Step [87/112], Loss: 0.6147\n",
      "Epoch [281/300], Step [91/112], Loss: 0.6511\n",
      "Epoch [281/300], Step [95/112], Loss: 0.6372\n",
      "Epoch [281/300], Step [99/112], Loss: 0.6445\n",
      "Epoch [281/300], Step [103/112], Loss: 0.6344\n",
      "Epoch [281/300], Step [107/112], Loss: 0.6388\n",
      "Epoch [281/300], Step [111/112], Loss: 0.5902\n",
      "Epoch [282/300], Step [3/112], Loss: 0.6444\n",
      "Epoch [282/300], Step [7/112], Loss: 0.6352\n",
      "Epoch [282/300], Step [11/112], Loss: 0.6246\n",
      "Epoch [282/300], Step [15/112], Loss: 0.6282\n",
      "Epoch [282/300], Step [19/112], Loss: 0.6206\n",
      "Epoch [282/300], Step [23/112], Loss: 0.6305\n",
      "Epoch [282/300], Step [27/112], Loss: 0.6005\n",
      "Epoch [282/300], Step [31/112], Loss: 0.6419\n",
      "Epoch [282/300], Step [35/112], Loss: 0.6094\n",
      "Epoch [282/300], Step [39/112], Loss: 0.6115\n",
      "Epoch [282/300], Step [43/112], Loss: 0.6286\n",
      "Epoch [282/300], Step [47/112], Loss: 0.6200\n",
      "Epoch [282/300], Step [51/112], Loss: 0.6196\n",
      "Epoch [282/300], Step [55/112], Loss: 0.6301\n",
      "Epoch [282/300], Step [59/112], Loss: 0.6216\n",
      "Epoch [282/300], Step [63/112], Loss: 0.6349\n",
      "Epoch [282/300], Step [67/112], Loss: 0.6279\n",
      "Epoch [282/300], Step [71/112], Loss: 0.6279\n",
      "Epoch [282/300], Step [75/112], Loss: 0.6374\n",
      "Epoch [282/300], Step [79/112], Loss: 0.6276\n",
      "Epoch [282/300], Step [83/112], Loss: 0.6479\n",
      "Epoch [282/300], Step [87/112], Loss: 0.6165\n",
      "Epoch [282/300], Step [91/112], Loss: 0.6628\n",
      "Epoch [282/300], Step [95/112], Loss: 0.6432\n",
      "Epoch [282/300], Step [99/112], Loss: 0.6438\n",
      "Epoch [282/300], Step [103/112], Loss: 0.6429\n",
      "Epoch [282/300], Step [107/112], Loss: 0.6264\n",
      "Epoch [282/300], Step [111/112], Loss: 0.5924\n",
      "Epoch [283/300], Step [3/112], Loss: 0.6458\n",
      "Epoch [283/300], Step [7/112], Loss: 0.6387\n",
      "Epoch [283/300], Step [11/112], Loss: 0.6191\n",
      "Epoch [283/300], Step [15/112], Loss: 0.6357\n",
      "Epoch [283/300], Step [19/112], Loss: 0.6196\n",
      "Epoch [283/300], Step [23/112], Loss: 0.6233\n",
      "Epoch [283/300], Step [27/112], Loss: 0.5942\n",
      "Epoch [283/300], Step [31/112], Loss: 0.6445\n",
      "Epoch [283/300], Step [35/112], Loss: 0.6170\n",
      "Epoch [283/300], Step [39/112], Loss: 0.6128\n",
      "Epoch [283/300], Step [43/112], Loss: 0.6233\n",
      "Epoch [283/300], Step [47/112], Loss: 0.6274\n",
      "Epoch [283/300], Step [51/112], Loss: 0.6195\n",
      "Epoch [283/300], Step [55/112], Loss: 0.6274\n",
      "Epoch [283/300], Step [59/112], Loss: 0.6250\n",
      "Epoch [283/300], Step [63/112], Loss: 0.6406\n",
      "Epoch [283/300], Step [67/112], Loss: 0.6168\n",
      "Epoch [283/300], Step [71/112], Loss: 0.6303\n",
      "Epoch [283/300], Step [75/112], Loss: 0.6334\n",
      "Epoch [283/300], Step [79/112], Loss: 0.6296\n",
      "Epoch [283/300], Step [83/112], Loss: 0.6413\n",
      "Epoch [283/300], Step [87/112], Loss: 0.6134\n",
      "Epoch [283/300], Step [91/112], Loss: 0.6618\n",
      "Epoch [283/300], Step [95/112], Loss: 0.6347\n",
      "Epoch [283/300], Step [99/112], Loss: 0.6446\n",
      "Epoch [283/300], Step [103/112], Loss: 0.6461\n",
      "Epoch [283/300], Step [107/112], Loss: 0.6342\n",
      "Epoch [283/300], Step [111/112], Loss: 0.5955\n",
      "Epoch [284/300], Step [3/112], Loss: 0.6469\n",
      "Epoch [284/300], Step [7/112], Loss: 0.6402\n",
      "Epoch [284/300], Step [11/112], Loss: 0.6212\n",
      "Epoch [284/300], Step [15/112], Loss: 0.6285\n",
      "Epoch [284/300], Step [19/112], Loss: 0.6176\n",
      "Epoch [284/300], Step [23/112], Loss: 0.6311\n",
      "Epoch [284/300], Step [27/112], Loss: 0.5934\n",
      "Epoch [284/300], Step [31/112], Loss: 0.6405\n",
      "Epoch [284/300], Step [35/112], Loss: 0.6162\n",
      "Epoch [284/300], Step [39/112], Loss: 0.6115\n",
      "Epoch [284/300], Step [43/112], Loss: 0.6214\n",
      "Epoch [284/300], Step [47/112], Loss: 0.6265\n",
      "Epoch [284/300], Step [51/112], Loss: 0.6219\n",
      "Epoch [284/300], Step [55/112], Loss: 0.6288\n",
      "Epoch [284/300], Step [59/112], Loss: 0.6269\n",
      "Epoch [284/300], Step [63/112], Loss: 0.6291\n",
      "Epoch [284/300], Step [67/112], Loss: 0.6204\n",
      "Epoch [284/300], Step [71/112], Loss: 0.6267\n",
      "Epoch [284/300], Step [75/112], Loss: 0.6301\n",
      "Epoch [284/300], Step [79/112], Loss: 0.6254\n",
      "Epoch [284/300], Step [83/112], Loss: 0.6441\n",
      "Epoch [284/300], Step [87/112], Loss: 0.6197\n",
      "Epoch [284/300], Step [91/112], Loss: 0.6562\n",
      "Epoch [284/300], Step [95/112], Loss: 0.6284\n",
      "Epoch [284/300], Step [99/112], Loss: 0.6444\n",
      "Epoch [284/300], Step [103/112], Loss: 0.6442\n",
      "Epoch [284/300], Step [107/112], Loss: 0.6281\n",
      "Epoch [284/300], Step [111/112], Loss: 0.5986\n",
      "Epoch [285/300], Step [3/112], Loss: 0.6355\n",
      "Epoch [285/300], Step [7/112], Loss: 0.6414\n",
      "Epoch [285/300], Step [11/112], Loss: 0.6200\n",
      "Epoch [285/300], Step [15/112], Loss: 0.6209\n",
      "Epoch [285/300], Step [19/112], Loss: 0.6099\n",
      "Epoch [285/300], Step [23/112], Loss: 0.6230\n",
      "Epoch [285/300], Step [27/112], Loss: 0.5927\n",
      "Epoch [285/300], Step [31/112], Loss: 0.6430\n",
      "Epoch [285/300], Step [35/112], Loss: 0.6214\n",
      "Epoch [285/300], Step [39/112], Loss: 0.6073\n",
      "Epoch [285/300], Step [43/112], Loss: 0.6241\n",
      "Epoch [285/300], Step [47/112], Loss: 0.6225\n",
      "Epoch [285/300], Step [51/112], Loss: 0.6195\n",
      "Epoch [285/300], Step [55/112], Loss: 0.6302\n",
      "Epoch [285/300], Step [59/112], Loss: 0.6253\n",
      "Epoch [285/300], Step [63/112], Loss: 0.6302\n",
      "Epoch [285/300], Step [67/112], Loss: 0.6222\n",
      "Epoch [285/300], Step [71/112], Loss: 0.6329\n",
      "Epoch [285/300], Step [75/112], Loss: 0.6428\n",
      "Epoch [285/300], Step [79/112], Loss: 0.6305\n",
      "Epoch [285/300], Step [83/112], Loss: 0.6557\n",
      "Epoch [285/300], Step [87/112], Loss: 0.6174\n",
      "Epoch [285/300], Step [91/112], Loss: 0.6600\n",
      "Epoch [285/300], Step [95/112], Loss: 0.6310\n",
      "Epoch [285/300], Step [99/112], Loss: 0.6351\n",
      "Epoch [285/300], Step [103/112], Loss: 0.6351\n",
      "Epoch [285/300], Step [107/112], Loss: 0.6202\n",
      "Epoch [285/300], Step [111/112], Loss: 0.5872\n",
      "Epoch [286/300], Step [3/112], Loss: 0.6300\n",
      "Epoch [286/300], Step [7/112], Loss: 0.6357\n",
      "Epoch [286/300], Step [11/112], Loss: 0.6204\n",
      "Epoch [286/300], Step [15/112], Loss: 0.6228\n",
      "Epoch [286/300], Step [19/112], Loss: 0.6180\n",
      "Epoch [286/300], Step [23/112], Loss: 0.6176\n",
      "Epoch [286/300], Step [27/112], Loss: 0.5992\n",
      "Epoch [286/300], Step [31/112], Loss: 0.6420\n",
      "Epoch [286/300], Step [35/112], Loss: 0.6114\n",
      "Epoch [286/300], Step [39/112], Loss: 0.6007\n",
      "Epoch [286/300], Step [43/112], Loss: 0.6262\n",
      "Epoch [286/300], Step [47/112], Loss: 0.6215\n",
      "Epoch [286/300], Step [51/112], Loss: 0.6134\n",
      "Epoch [286/300], Step [55/112], Loss: 0.6226\n",
      "Epoch [286/300], Step [59/112], Loss: 0.6251\n",
      "Epoch [286/300], Step [63/112], Loss: 0.6319\n",
      "Epoch [286/300], Step [67/112], Loss: 0.6171\n",
      "Epoch [286/300], Step [71/112], Loss: 0.6279\n",
      "Epoch [286/300], Step [75/112], Loss: 0.6291\n",
      "Epoch [286/300], Step [79/112], Loss: 0.6239\n",
      "Epoch [286/300], Step [83/112], Loss: 0.6427\n",
      "Epoch [286/300], Step [87/112], Loss: 0.6178\n",
      "Epoch [286/300], Step [91/112], Loss: 0.6528\n",
      "Epoch [286/300], Step [95/112], Loss: 0.6303\n",
      "Epoch [286/300], Step [99/112], Loss: 0.6449\n",
      "Epoch [286/300], Step [103/112], Loss: 0.6353\n",
      "Epoch [286/300], Step [107/112], Loss: 0.6275\n",
      "Epoch [286/300], Step [111/112], Loss: 0.5898\n",
      "Epoch [287/300], Step [3/112], Loss: 0.6360\n",
      "Epoch [287/300], Step [7/112], Loss: 0.6375\n",
      "Epoch [287/300], Step [11/112], Loss: 0.6172\n",
      "Epoch [287/300], Step [15/112], Loss: 0.6324\n",
      "Epoch [287/300], Step [19/112], Loss: 0.6084\n",
      "Epoch [287/300], Step [23/112], Loss: 0.6099\n",
      "Epoch [287/300], Step [27/112], Loss: 0.6026\n",
      "Epoch [287/300], Step [31/112], Loss: 0.6341\n",
      "Epoch [287/300], Step [35/112], Loss: 0.6225\n",
      "Epoch [287/300], Step [39/112], Loss: 0.6202\n",
      "Epoch [287/300], Step [43/112], Loss: 0.6233\n",
      "Epoch [287/300], Step [47/112], Loss: 0.6230\n",
      "Epoch [287/300], Step [51/112], Loss: 0.6239\n",
      "Epoch [287/300], Step [55/112], Loss: 0.6414\n",
      "Epoch [287/300], Step [59/112], Loss: 0.6244\n",
      "Epoch [287/300], Step [63/112], Loss: 0.6252\n",
      "Epoch [287/300], Step [67/112], Loss: 0.6156\n",
      "Epoch [287/300], Step [71/112], Loss: 0.6306\n",
      "Epoch [287/300], Step [75/112], Loss: 0.6333\n",
      "Epoch [287/300], Step [79/112], Loss: 0.6296\n",
      "Epoch [287/300], Step [83/112], Loss: 0.6453\n",
      "Epoch [287/300], Step [87/112], Loss: 0.6151\n",
      "Epoch [287/300], Step [91/112], Loss: 0.6636\n",
      "Epoch [287/300], Step [95/112], Loss: 0.6307\n",
      "Epoch [287/300], Step [99/112], Loss: 0.6368\n",
      "Epoch [287/300], Step [103/112], Loss: 0.6440\n",
      "Epoch [287/300], Step [107/112], Loss: 0.6258\n",
      "Epoch [287/300], Step [111/112], Loss: 0.5892\n",
      "Epoch [288/300], Step [3/112], Loss: 0.6430\n",
      "Epoch [288/300], Step [7/112], Loss: 0.6350\n",
      "Epoch [288/300], Step [11/112], Loss: 0.6163\n",
      "Epoch [288/300], Step [15/112], Loss: 0.6277\n",
      "Epoch [288/300], Step [19/112], Loss: 0.6120\n",
      "Epoch [288/300], Step [23/112], Loss: 0.6261\n",
      "Epoch [288/300], Step [27/112], Loss: 0.6079\n",
      "Epoch [288/300], Step [31/112], Loss: 0.6396\n",
      "Epoch [288/300], Step [35/112], Loss: 0.6212\n",
      "Epoch [288/300], Step [39/112], Loss: 0.6118\n",
      "Epoch [288/300], Step [43/112], Loss: 0.6197\n",
      "Epoch [288/300], Step [47/112], Loss: 0.6266\n",
      "Epoch [288/300], Step [51/112], Loss: 0.6199\n",
      "Epoch [288/300], Step [55/112], Loss: 0.6350\n",
      "Epoch [288/300], Step [59/112], Loss: 0.6270\n",
      "Epoch [288/300], Step [63/112], Loss: 0.6314\n",
      "Epoch [288/300], Step [67/112], Loss: 0.6139\n",
      "Epoch [288/300], Step [71/112], Loss: 0.6314\n",
      "Epoch [288/300], Step [75/112], Loss: 0.6297\n",
      "Epoch [288/300], Step [79/112], Loss: 0.6264\n",
      "Epoch [288/300], Step [83/112], Loss: 0.6439\n",
      "Epoch [288/300], Step [87/112], Loss: 0.6103\n",
      "Epoch [288/300], Step [91/112], Loss: 0.6599\n",
      "Epoch [288/300], Step [95/112], Loss: 0.6326\n",
      "Epoch [288/300], Step [99/112], Loss: 0.6346\n",
      "Epoch [288/300], Step [103/112], Loss: 0.6359\n",
      "Epoch [288/300], Step [107/112], Loss: 0.6307\n",
      "Epoch [288/300], Step [111/112], Loss: 0.5868\n",
      "Epoch [289/300], Step [3/112], Loss: 0.6378\n",
      "Epoch [289/300], Step [7/112], Loss: 0.6463\n",
      "Epoch [289/300], Step [11/112], Loss: 0.6259\n",
      "Epoch [289/300], Step [15/112], Loss: 0.6399\n",
      "Epoch [289/300], Step [19/112], Loss: 0.6111\n",
      "Epoch [289/300], Step [23/112], Loss: 0.6175\n",
      "Epoch [289/300], Step [27/112], Loss: 0.5987\n",
      "Epoch [289/300], Step [31/112], Loss: 0.6463\n",
      "Epoch [289/300], Step [35/112], Loss: 0.6203\n",
      "Epoch [289/300], Step [39/112], Loss: 0.6008\n",
      "Epoch [289/300], Step [43/112], Loss: 0.6134\n",
      "Epoch [289/300], Step [47/112], Loss: 0.6161\n",
      "Epoch [289/300], Step [51/112], Loss: 0.6209\n",
      "Epoch [289/300], Step [55/112], Loss: 0.6337\n",
      "Epoch [289/300], Step [59/112], Loss: 0.6258\n",
      "Epoch [289/300], Step [63/112], Loss: 0.6223\n",
      "Epoch [289/300], Step [67/112], Loss: 0.6245\n",
      "Epoch [289/300], Step [71/112], Loss: 0.6302\n",
      "Epoch [289/300], Step [75/112], Loss: 0.6287\n",
      "Epoch [289/300], Step [79/112], Loss: 0.6275\n",
      "Epoch [289/300], Step [83/112], Loss: 0.6419\n",
      "Epoch [289/300], Step [87/112], Loss: 0.6178\n",
      "Epoch [289/300], Step [91/112], Loss: 0.6612\n",
      "Epoch [289/300], Step [95/112], Loss: 0.6296\n",
      "Epoch [289/300], Step [99/112], Loss: 0.6408\n",
      "Epoch [289/300], Step [103/112], Loss: 0.6438\n",
      "Epoch [289/300], Step [107/112], Loss: 0.6270\n",
      "Epoch [289/300], Step [111/112], Loss: 0.5816\n",
      "Epoch [290/300], Step [3/112], Loss: 0.6413\n",
      "Epoch [290/300], Step [7/112], Loss: 0.6336\n",
      "Epoch [290/300], Step [11/112], Loss: 0.6327\n",
      "Epoch [290/300], Step [15/112], Loss: 0.6303\n",
      "Epoch [290/300], Step [19/112], Loss: 0.6142\n",
      "Epoch [290/300], Step [23/112], Loss: 0.6245\n",
      "Epoch [290/300], Step [27/112], Loss: 0.6054\n",
      "Epoch [290/300], Step [31/112], Loss: 0.6435\n",
      "Epoch [290/300], Step [35/112], Loss: 0.6226\n",
      "Epoch [290/300], Step [39/112], Loss: 0.6113\n",
      "Epoch [290/300], Step [43/112], Loss: 0.6269\n",
      "Epoch [290/300], Step [47/112], Loss: 0.6235\n",
      "Epoch [290/300], Step [51/112], Loss: 0.6230\n",
      "Epoch [290/300], Step [55/112], Loss: 0.6442\n",
      "Epoch [290/300], Step [59/112], Loss: 0.6257\n",
      "Epoch [290/300], Step [63/112], Loss: 0.6249\n",
      "Epoch [290/300], Step [67/112], Loss: 0.6306\n",
      "Epoch [290/300], Step [71/112], Loss: 0.6303\n",
      "Epoch [290/300], Step [75/112], Loss: 0.6324\n",
      "Epoch [290/300], Step [79/112], Loss: 0.6273\n",
      "Epoch [290/300], Step [83/112], Loss: 0.6490\n",
      "Epoch [290/300], Step [87/112], Loss: 0.6154\n",
      "Epoch [290/300], Step [91/112], Loss: 0.6502\n",
      "Epoch [290/300], Step [95/112], Loss: 0.6325\n",
      "Epoch [290/300], Step [99/112], Loss: 0.6376\n",
      "Epoch [290/300], Step [103/112], Loss: 0.6420\n",
      "Epoch [290/300], Step [107/112], Loss: 0.6247\n",
      "Epoch [290/300], Step [111/112], Loss: 0.5865\n",
      "Epoch [291/300], Step [3/112], Loss: 0.6447\n",
      "Epoch [291/300], Step [7/112], Loss: 0.6354\n",
      "Epoch [291/300], Step [11/112], Loss: 0.6188\n",
      "Epoch [291/300], Step [15/112], Loss: 0.6337\n",
      "Epoch [291/300], Step [19/112], Loss: 0.6132\n",
      "Epoch [291/300], Step [23/112], Loss: 0.6170\n",
      "Epoch [291/300], Step [27/112], Loss: 0.6028\n",
      "Epoch [291/300], Step [31/112], Loss: 0.6473\n",
      "Epoch [291/300], Step [35/112], Loss: 0.6093\n",
      "Epoch [291/300], Step [39/112], Loss: 0.6189\n",
      "Epoch [291/300], Step [43/112], Loss: 0.6129\n",
      "Epoch [291/300], Step [47/112], Loss: 0.6111\n",
      "Epoch [291/300], Step [51/112], Loss: 0.6158\n",
      "Epoch [291/300], Step [55/112], Loss: 0.6256\n",
      "Epoch [291/300], Step [59/112], Loss: 0.6174\n",
      "Epoch [291/300], Step [63/112], Loss: 0.6216\n",
      "Epoch [291/300], Step [67/112], Loss: 0.6168\n",
      "Epoch [291/300], Step [71/112], Loss: 0.6347\n",
      "Epoch [291/300], Step [75/112], Loss: 0.6231\n",
      "Epoch [291/300], Step [79/112], Loss: 0.6230\n",
      "Epoch [291/300], Step [83/112], Loss: 0.6454\n",
      "Epoch [291/300], Step [87/112], Loss: 0.6119\n",
      "Epoch [291/300], Step [91/112], Loss: 0.6617\n",
      "Epoch [291/300], Step [95/112], Loss: 0.6395\n",
      "Epoch [291/300], Step [99/112], Loss: 0.6432\n",
      "Epoch [291/300], Step [103/112], Loss: 0.6384\n",
      "Epoch [291/300], Step [107/112], Loss: 0.6237\n",
      "Epoch [291/300], Step [111/112], Loss: 0.5937\n",
      "Epoch [292/300], Step [3/112], Loss: 0.6370\n",
      "Epoch [292/300], Step [7/112], Loss: 0.6312\n",
      "Epoch [292/300], Step [11/112], Loss: 0.6258\n",
      "Epoch [292/300], Step [15/112], Loss: 0.6343\n",
      "Epoch [292/300], Step [19/112], Loss: 0.6118\n",
      "Epoch [292/300], Step [23/112], Loss: 0.6133\n",
      "Epoch [292/300], Step [27/112], Loss: 0.5969\n",
      "Epoch [292/300], Step [31/112], Loss: 0.6442\n",
      "Epoch [292/300], Step [35/112], Loss: 0.6133\n",
      "Epoch [292/300], Step [39/112], Loss: 0.6051\n",
      "Epoch [292/300], Step [43/112], Loss: 0.6219\n",
      "Epoch [292/300], Step [47/112], Loss: 0.6274\n",
      "Epoch [292/300], Step [51/112], Loss: 0.6160\n",
      "Epoch [292/300], Step [55/112], Loss: 0.6319\n",
      "Epoch [292/300], Step [59/112], Loss: 0.6293\n",
      "Epoch [292/300], Step [63/112], Loss: 0.6193\n",
      "Epoch [292/300], Step [67/112], Loss: 0.6199\n",
      "Epoch [292/300], Step [71/112], Loss: 0.6243\n",
      "Epoch [292/300], Step [75/112], Loss: 0.6318\n",
      "Epoch [292/300], Step [79/112], Loss: 0.6258\n",
      "Epoch [292/300], Step [83/112], Loss: 0.6408\n",
      "Epoch [292/300], Step [87/112], Loss: 0.6095\n",
      "Epoch [292/300], Step [91/112], Loss: 0.6527\n",
      "Epoch [292/300], Step [95/112], Loss: 0.6383\n",
      "Epoch [292/300], Step [99/112], Loss: 0.6306\n",
      "Epoch [292/300], Step [103/112], Loss: 0.6310\n",
      "Epoch [292/300], Step [107/112], Loss: 0.6342\n",
      "Epoch [292/300], Step [111/112], Loss: 0.5846\n",
      "Epoch [293/300], Step [3/112], Loss: 0.6377\n",
      "Epoch [293/300], Step [7/112], Loss: 0.6332\n",
      "Epoch [293/300], Step [11/112], Loss: 0.6183\n",
      "Epoch [293/300], Step [15/112], Loss: 0.6314\n",
      "Epoch [293/300], Step [19/112], Loss: 0.6109\n",
      "Epoch [293/300], Step [23/112], Loss: 0.6205\n",
      "Epoch [293/300], Step [27/112], Loss: 0.6000\n",
      "Epoch [293/300], Step [31/112], Loss: 0.6437\n",
      "Epoch [293/300], Step [35/112], Loss: 0.6220\n",
      "Epoch [293/300], Step [39/112], Loss: 0.6038\n",
      "Epoch [293/300], Step [43/112], Loss: 0.6169\n",
      "Epoch [293/300], Step [47/112], Loss: 0.6184\n",
      "Epoch [293/300], Step [51/112], Loss: 0.6158\n",
      "Epoch [293/300], Step [55/112], Loss: 0.6241\n",
      "Epoch [293/300], Step [59/112], Loss: 0.6209\n",
      "Epoch [293/300], Step [63/112], Loss: 0.6248\n",
      "Epoch [293/300], Step [67/112], Loss: 0.6302\n",
      "Epoch [293/300], Step [71/112], Loss: 0.6271\n",
      "Epoch [293/300], Step [75/112], Loss: 0.6436\n",
      "Epoch [293/300], Step [79/112], Loss: 0.6233\n",
      "Epoch [293/300], Step [83/112], Loss: 0.6426\n",
      "Epoch [293/300], Step [87/112], Loss: 0.6155\n",
      "Epoch [293/300], Step [91/112], Loss: 0.6582\n",
      "Epoch [293/300], Step [95/112], Loss: 0.6391\n",
      "Epoch [293/300], Step [99/112], Loss: 0.6441\n",
      "Epoch [293/300], Step [103/112], Loss: 0.6369\n",
      "Epoch [293/300], Step [107/112], Loss: 0.6250\n",
      "Epoch [293/300], Step [111/112], Loss: 0.5825\n",
      "Epoch [294/300], Step [3/112], Loss: 0.6379\n",
      "Epoch [294/300], Step [7/112], Loss: 0.6461\n",
      "Epoch [294/300], Step [11/112], Loss: 0.6208\n",
      "Epoch [294/300], Step [15/112], Loss: 0.6336\n",
      "Epoch [294/300], Step [19/112], Loss: 0.6165\n",
      "Epoch [294/300], Step [23/112], Loss: 0.6283\n",
      "Epoch [294/300], Step [27/112], Loss: 0.5955\n",
      "Epoch [294/300], Step [31/112], Loss: 0.6412\n",
      "Epoch [294/300], Step [35/112], Loss: 0.6169\n",
      "Epoch [294/300], Step [39/112], Loss: 0.6071\n",
      "Epoch [294/300], Step [43/112], Loss: 0.6167\n",
      "Epoch [294/300], Step [47/112], Loss: 0.6285\n",
      "Epoch [294/300], Step [51/112], Loss: 0.6175\n",
      "Epoch [294/300], Step [55/112], Loss: 0.6322\n",
      "Epoch [294/300], Step [59/112], Loss: 0.6137\n",
      "Epoch [294/300], Step [63/112], Loss: 0.6214\n",
      "Epoch [294/300], Step [67/112], Loss: 0.6259\n",
      "Epoch [294/300], Step [71/112], Loss: 0.6262\n",
      "Epoch [294/300], Step [75/112], Loss: 0.6342\n",
      "Epoch [294/300], Step [79/112], Loss: 0.6289\n",
      "Epoch [294/300], Step [83/112], Loss: 0.6382\n",
      "Epoch [294/300], Step [87/112], Loss: 0.6148\n",
      "Epoch [294/300], Step [91/112], Loss: 0.6600\n",
      "Epoch [294/300], Step [95/112], Loss: 0.6388\n",
      "Epoch [294/300], Step [99/112], Loss: 0.6334\n",
      "Epoch [294/300], Step [103/112], Loss: 0.6407\n",
      "Epoch [294/300], Step [107/112], Loss: 0.6281\n",
      "Epoch [294/300], Step [111/112], Loss: 0.5868\n",
      "Epoch [295/300], Step [3/112], Loss: 0.6391\n",
      "Epoch [295/300], Step [7/112], Loss: 0.6350\n",
      "Epoch [295/300], Step [11/112], Loss: 0.6174\n",
      "Epoch [295/300], Step [15/112], Loss: 0.6312\n",
      "Epoch [295/300], Step [19/112], Loss: 0.6090\n",
      "Epoch [295/300], Step [23/112], Loss: 0.6164\n",
      "Epoch [295/300], Step [27/112], Loss: 0.5923\n",
      "Epoch [295/300], Step [31/112], Loss: 0.6427\n",
      "Epoch [295/300], Step [35/112], Loss: 0.6152\n",
      "Epoch [295/300], Step [39/112], Loss: 0.6074\n",
      "Epoch [295/300], Step [43/112], Loss: 0.6222\n",
      "Epoch [295/300], Step [47/112], Loss: 0.6137\n",
      "Epoch [295/300], Step [51/112], Loss: 0.6145\n",
      "Epoch [295/300], Step [55/112], Loss: 0.6256\n",
      "Epoch [295/300], Step [59/112], Loss: 0.6158\n",
      "Epoch [295/300], Step [63/112], Loss: 0.6220\n",
      "Epoch [295/300], Step [67/112], Loss: 0.6188\n",
      "Epoch [295/300], Step [71/112], Loss: 0.6230\n",
      "Epoch [295/300], Step [75/112], Loss: 0.6306\n",
      "Epoch [295/300], Step [79/112], Loss: 0.6267\n",
      "Epoch [295/300], Step [83/112], Loss: 0.6423\n",
      "Epoch [295/300], Step [87/112], Loss: 0.6056\n",
      "Epoch [295/300], Step [91/112], Loss: 0.6579\n",
      "Epoch [295/300], Step [95/112], Loss: 0.6278\n",
      "Epoch [295/300], Step [99/112], Loss: 0.6374\n",
      "Epoch [295/300], Step [103/112], Loss: 0.6377\n",
      "Epoch [295/300], Step [107/112], Loss: 0.6273\n",
      "Epoch [295/300], Step [111/112], Loss: 0.5892\n",
      "Epoch [296/300], Step [3/112], Loss: 0.6461\n",
      "Epoch [296/300], Step [7/112], Loss: 0.6353\n",
      "Epoch [296/300], Step [11/112], Loss: 0.6233\n",
      "Epoch [296/300], Step [15/112], Loss: 0.6302\n",
      "Epoch [296/300], Step [19/112], Loss: 0.6099\n",
      "Epoch [296/300], Step [23/112], Loss: 0.6220\n",
      "Epoch [296/300], Step [27/112], Loss: 0.5977\n",
      "Epoch [296/300], Step [31/112], Loss: 0.6426\n",
      "Epoch [296/300], Step [35/112], Loss: 0.6123\n",
      "Epoch [296/300], Step [39/112], Loss: 0.6056\n",
      "Epoch [296/300], Step [43/112], Loss: 0.6124\n",
      "Epoch [296/300], Step [47/112], Loss: 0.6153\n",
      "Epoch [296/300], Step [51/112], Loss: 0.6144\n",
      "Epoch [296/300], Step [55/112], Loss: 0.6299\n",
      "Epoch [296/300], Step [59/112], Loss: 0.6218\n",
      "Epoch [296/300], Step [63/112], Loss: 0.6269\n",
      "Epoch [296/300], Step [67/112], Loss: 0.6257\n",
      "Epoch [296/300], Step [71/112], Loss: 0.6269\n",
      "Epoch [296/300], Step [75/112], Loss: 0.6281\n",
      "Epoch [296/300], Step [79/112], Loss: 0.6282\n",
      "Epoch [296/300], Step [83/112], Loss: 0.6436\n",
      "Epoch [296/300], Step [87/112], Loss: 0.6124\n",
      "Epoch [296/300], Step [91/112], Loss: 0.6504\n",
      "Epoch [296/300], Step [95/112], Loss: 0.6322\n",
      "Epoch [296/300], Step [99/112], Loss: 0.6427\n",
      "Epoch [296/300], Step [103/112], Loss: 0.6299\n",
      "Epoch [296/300], Step [107/112], Loss: 0.6409\n",
      "Epoch [296/300], Step [111/112], Loss: 0.5921\n",
      "Epoch [297/300], Step [3/112], Loss: 0.6419\n",
      "Epoch [297/300], Step [7/112], Loss: 0.6494\n",
      "Epoch [297/300], Step [11/112], Loss: 0.6155\n",
      "Epoch [297/300], Step [15/112], Loss: 0.6304\n",
      "Epoch [297/300], Step [19/112], Loss: 0.6160\n",
      "Epoch [297/300], Step [23/112], Loss: 0.6248\n",
      "Epoch [297/300], Step [27/112], Loss: 0.6072\n",
      "Epoch [297/300], Step [31/112], Loss: 0.6458\n",
      "Epoch [297/300], Step [35/112], Loss: 0.6274\n",
      "Epoch [297/300], Step [39/112], Loss: 0.6135\n",
      "Epoch [297/300], Step [43/112], Loss: 0.6144\n",
      "Epoch [297/300], Step [47/112], Loss: 0.6223\n",
      "Epoch [297/300], Step [51/112], Loss: 0.6210\n",
      "Epoch [297/300], Step [55/112], Loss: 0.6302\n",
      "Epoch [297/300], Step [59/112], Loss: 0.6215\n",
      "Epoch [297/300], Step [63/112], Loss: 0.6299\n",
      "Epoch [297/300], Step [67/112], Loss: 0.6206\n",
      "Epoch [297/300], Step [71/112], Loss: 0.6236\n",
      "Epoch [297/300], Step [75/112], Loss: 0.6319\n",
      "Epoch [297/300], Step [79/112], Loss: 0.6271\n",
      "Epoch [297/300], Step [83/112], Loss: 0.6488\n",
      "Epoch [297/300], Step [87/112], Loss: 0.6143\n",
      "Epoch [297/300], Step [91/112], Loss: 0.6522\n",
      "Epoch [297/300], Step [95/112], Loss: 0.6348\n",
      "Epoch [297/300], Step [99/112], Loss: 0.6438\n",
      "Epoch [297/300], Step [103/112], Loss: 0.6335\n",
      "Epoch [297/300], Step [107/112], Loss: 0.6258\n",
      "Epoch [297/300], Step [111/112], Loss: 0.5793\n",
      "Epoch [298/300], Step [3/112], Loss: 0.6371\n",
      "Epoch [298/300], Step [7/112], Loss: 0.6361\n",
      "Epoch [298/300], Step [11/112], Loss: 0.6168\n",
      "Epoch [298/300], Step [15/112], Loss: 0.6305\n",
      "Epoch [298/300], Step [19/112], Loss: 0.6141\n",
      "Epoch [298/300], Step [23/112], Loss: 0.6159\n",
      "Epoch [298/300], Step [27/112], Loss: 0.6021\n",
      "Epoch [298/300], Step [31/112], Loss: 0.6431\n",
      "Epoch [298/300], Step [35/112], Loss: 0.6230\n",
      "Epoch [298/300], Step [39/112], Loss: 0.6183\n",
      "Epoch [298/300], Step [43/112], Loss: 0.6225\n",
      "Epoch [298/300], Step [47/112], Loss: 0.6171\n",
      "Epoch [298/300], Step [51/112], Loss: 0.6195\n",
      "Epoch [298/300], Step [55/112], Loss: 0.6291\n",
      "Epoch [298/300], Step [59/112], Loss: 0.6276\n",
      "Epoch [298/300], Step [63/112], Loss: 0.6302\n",
      "Epoch [298/300], Step [67/112], Loss: 0.6264\n",
      "Epoch [298/300], Step [71/112], Loss: 0.6247\n",
      "Epoch [298/300], Step [75/112], Loss: 0.6340\n",
      "Epoch [298/300], Step [79/112], Loss: 0.6292\n",
      "Epoch [298/300], Step [83/112], Loss: 0.6484\n",
      "Epoch [298/300], Step [87/112], Loss: 0.6232\n",
      "Epoch [298/300], Step [91/112], Loss: 0.6593\n",
      "Epoch [298/300], Step [95/112], Loss: 0.6335\n",
      "Epoch [298/300], Step [99/112], Loss: 0.6436\n",
      "Epoch [298/300], Step [103/112], Loss: 0.6260\n",
      "Epoch [298/300], Step [107/112], Loss: 0.6236\n",
      "Epoch [298/300], Step [111/112], Loss: 0.5893\n",
      "Epoch [299/300], Step [3/112], Loss: 0.6472\n",
      "Epoch [299/300], Step [7/112], Loss: 0.6356\n",
      "Epoch [299/300], Step [11/112], Loss: 0.6296\n",
      "Epoch [299/300], Step [15/112], Loss: 0.6256\n",
      "Epoch [299/300], Step [19/112], Loss: 0.6177\n",
      "Epoch [299/300], Step [23/112], Loss: 0.6220\n",
      "Epoch [299/300], Step [27/112], Loss: 0.6002\n",
      "Epoch [299/300], Step [31/112], Loss: 0.6485\n",
      "Epoch [299/300], Step [35/112], Loss: 0.6240\n",
      "Epoch [299/300], Step [39/112], Loss: 0.6094\n",
      "Epoch [299/300], Step [43/112], Loss: 0.6157\n",
      "Epoch [299/300], Step [47/112], Loss: 0.6188\n",
      "Epoch [299/300], Step [51/112], Loss: 0.6330\n",
      "Epoch [299/300], Step [55/112], Loss: 0.6333\n",
      "Epoch [299/300], Step [59/112], Loss: 0.6218\n",
      "Epoch [299/300], Step [63/112], Loss: 0.6292\n",
      "Epoch [299/300], Step [67/112], Loss: 0.6288\n",
      "Epoch [299/300], Step [71/112], Loss: 0.6317\n",
      "Epoch [299/300], Step [75/112], Loss: 0.6324\n",
      "Epoch [299/300], Step [79/112], Loss: 0.6287\n",
      "Epoch [299/300], Step [83/112], Loss: 0.6371\n",
      "Epoch [299/300], Step [87/112], Loss: 0.6120\n",
      "Epoch [299/300], Step [91/112], Loss: 0.6498\n",
      "Epoch [299/300], Step [95/112], Loss: 0.6350\n",
      "Epoch [299/300], Step [99/112], Loss: 0.6429\n",
      "Epoch [299/300], Step [103/112], Loss: 0.6335\n",
      "Epoch [299/300], Step [107/112], Loss: 0.6278\n",
      "Epoch [299/300], Step [111/112], Loss: 0.5816\n",
      "Epoch [300/300], Step [3/112], Loss: 0.6419\n",
      "Epoch [300/300], Step [7/112], Loss: 0.6367\n",
      "Epoch [300/300], Step [11/112], Loss: 0.6219\n",
      "Epoch [300/300], Step [15/112], Loss: 0.6227\n",
      "Epoch [300/300], Step [19/112], Loss: 0.6106\n",
      "Epoch [300/300], Step [23/112], Loss: 0.6230\n",
      "Epoch [300/300], Step [27/112], Loss: 0.6030\n",
      "Epoch [300/300], Step [31/112], Loss: 0.6408\n",
      "Epoch [300/300], Step [35/112], Loss: 0.6149\n",
      "Epoch [300/300], Step [39/112], Loss: 0.6053\n",
      "Epoch [300/300], Step [43/112], Loss: 0.6193\n",
      "Epoch [300/300], Step [47/112], Loss: 0.6208\n",
      "Epoch [300/300], Step [51/112], Loss: 0.6158\n",
      "Epoch [300/300], Step [55/112], Loss: 0.6318\n",
      "Epoch [300/300], Step [59/112], Loss: 0.6208\n",
      "Epoch [300/300], Step [63/112], Loss: 0.6279\n",
      "Epoch [300/300], Step [67/112], Loss: 0.6236\n",
      "Epoch [300/300], Step [71/112], Loss: 0.6209\n",
      "Epoch [300/300], Step [75/112], Loss: 0.6298\n",
      "Epoch [300/300], Step [79/112], Loss: 0.6273\n",
      "Epoch [300/300], Step [83/112], Loss: 0.6453\n",
      "Epoch [300/300], Step [87/112], Loss: 0.6175\n",
      "Epoch [300/300], Step [91/112], Loss: 0.6593\n",
      "Epoch [300/300], Step [95/112], Loss: 0.6328\n",
      "Epoch [300/300], Step [99/112], Loss: 0.6438\n",
      "Epoch [300/300], Step [103/112], Loss: 0.6302\n",
      "Epoch [300/300], Step [107/112], Loss: 0.6297\n",
      "Epoch [300/300], Step [111/112], Loss: 0.5829\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           b       0.74      0.62      0.67     23193\n",
      "           t       0.69      0.51      0.59     21669\n",
      "           e       0.63      0.88      0.74     30494\n",
      "           m       0.79      0.53      0.63      9128\n",
      "\n",
      "    accuracy                           0.68     84484\n",
      "   macro avg       0.71      0.64      0.66     84484\n",
      "weighted avg       0.69      0.68      0.67     84484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "input_size = 300  # Assuming 300-dimensional GloVe embeddings\n",
    "output_size = len(label_mapping)\n",
    "hidden_size = 100      # 1st layer and 2nd layer number of features\n",
    "num_epochs = 300\n",
    "batch_size = 3000\n",
    "learning_rate = 0.02\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove = GloVe(name='6B', dim=300)\n",
    "\n",
    "# Tokenization and embeddings using spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "def get_average_embedding(text):\n",
    "    tokens = nlp(text)\n",
    "    embeddings = [glove[token.text].numpy()\n",
    "                  for token in tokens if token.text in glove.stoi]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(input_size)  # Return zeros if no embeddings are found\n",
    "\n",
    "\n",
    "# Apply tokenization and embeddings to the dataset\n",
    "train_df['EMBEDDING_GLOVE'] = train_df['TITLE'].apply(get_average_embedding)\n",
    "test_df['EMBEDDING_GLOVE'] = test_df['TITLE'].apply(get_average_embedding)\n",
    "\n",
    "# Convert embeddings to torch tensors\n",
    "train_embeddings = torch.tensor(\n",
    "    np.vstack(train_df['EMBEDDING_GLOVE'].to_numpy()))\n",
    "test_embeddings = torch.tensor(\n",
    "    np.vstack(test_df['EMBEDDING_GLOVE'].to_numpy()))\n",
    "\n",
    "# Convert labels to torch tensors\n",
    "train_labels = torch.tensor(train_df['CATEGORY'].map(label_mapping).to_numpy())\n",
    "test_labels = torch.tensor(test_df['CATEGORY'].map(label_mapping).to_numpy())\n",
    "\n",
    "# Instantiate the model\n",
    "model = TextClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "if debug:\n",
    "    print(\"Training the model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(train_embeddings), batch_size):\n",
    "        inputs = train_embeddings[i:i+batch_size]\n",
    "        labels = train_labels[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if ((i/batch_size) + 1) % 4 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %\n",
    "                  (epoch + 1, num_epochs, i/batch_size, len(train_embeddings) / batch_size, loss.data))\n",
    "\n",
    "\n",
    "# Evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_embeddings.float())\n",
    "    _, test_predictions = torch.max(test_outputs, 1)\n",
    "\n",
    "test_predictions = test_predictions.numpy()\n",
    "test_labels = test_labels.numpy()\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels,\n",
    "      test_predictions, target_names=label_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See outputs of 2 and 3 above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
